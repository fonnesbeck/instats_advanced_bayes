{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Session 4: The Bayesian Workflow\n",
        "\n",
        "In this session we apply the complete Bayesian workflow to COVID-19 case data. We'll iterate through model building, prior predictive checks, fitting, diagnostics, posterior predictive checks, model comparison, and forecasting.\n",
        "\n",
        "Learning objectives:\n",
        "- Apply prior predictive checks to validate model/priors\n",
        "- Fit models and assess convergence (R-hat, ESS, divergences/energy)\n",
        "- Evaluate fit with posterior predictive checks and residuals\n",
        "- Compare models (LOO/WAIC) and refine iteratively\n",
        "- Use pm.Data for forecasting and scenario analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MCMC Output Processing and Model Checking with ArviZ\n",
        "\n",
        "ArviZ is a library for exploratory analysis of Bayesian models. In the workflow, we use it to:\n",
        "\n",
        "- Inspect sampler behavior (e.g., step size, tree depth, divergences)\n",
        "- Assess convergence (R-hat close to 1, effective sample sizes sufficiently large)\n",
        "- Visualize posterior distributions and dependencies\n",
        "- Evaluate fit using posterior predictive checks (PPCs)\n",
        "- Compare models using LOO/WAIC\n",
        "\n",
        "For workflow credibility, diagnostics must come before interpretation. A model that has not converged or explores the posterior inefficiently cannot be trusted, regardless of apparent fit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import load_covid_data\n",
        "import pymc as pm\n",
        "import arviz as az\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "sns.set_context('talk')\n",
        "\n",
        "sampler_kwargs = {\"chains\": 4, \"cores\": 4, \"tune\": 2000}\n",
        "RANDOM_SEED = 20090425\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load data\n",
        "\n",
        "We will use COVID-19 case counts, aligned to the day each country crosses 100 confirmed cases, to stabilize early reporting noise and enable comparisons.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = load_covid_data.load_data(drop_states=True, filter_n_days_100=2)\n",
        "countries = df.country.unique()\n",
        "n_countries = len(countries)\n",
        "# Align to day since crossing 100 confirmed cases\n",
        "df = df.loc[lambda x: (x.days_since_100 >= 0)]\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bayesian workflow steps\n",
        "\n",
        "1. Plot the data\n",
        "2. Build model\n",
        "3. Run prior predictive check\n",
        "4. Fit model\n",
        "5. Assess convergence\n",
        "6. Run posterior predictive check\n",
        "7. Improve model\n",
        "\n",
        "### 1) Plot the data (Germany, first 30 days)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "country = 'Germany'\n",
        "date = '2020-07-31'\n",
        "df_country = df.query(f'country==\"{country}\"').loc[:date].iloc[:30]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "df_country.confirmed.plot(ax=ax)\n",
        "ax.set(ylabel='Confirmed cases', title=country)\n",
        "sns.despine()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2) Build an initial model (exponential with Normal likelihood)\n",
        "\n",
        "We start with a simple exponential growth model with a Normal likelihood to illustrate prior predictive checks and why this is inadequate for counts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time and observations\n",
        "t = df_country.days_since_100.values\n",
        "confirmed = df_country.confirmed.values\n",
        "\n",
        "with pm.Model() as model_exp1:\n",
        "    a = pm.Normal('a', mu=0, sigma=100)\n",
        "    b = pm.Normal('b', mu=0.3, sigma=0.3)\n",
        "    growth = a * (1 + b) ** t\n",
        "    eps = pm.HalfNormal('eps', 100)\n",
        "    pm.Normal('obs', mu=growth, sigma=eps, observed=confirmed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3) Prior predictive check\n",
        "\n",
        "Generate data from the prior to check that implied data are plausible.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with model_exp1:\n",
        "    prior_pred = pm.sample_prior_predictive(random_seed=RANDOM_SEED)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "ax.plot(prior_pred.prior_predictive['obs'].values.squeeze().T, color=\"0.5\", alpha=.1)\n",
        "ax.set(ylim=(-1000, 1000), xlim=(0, 10), title=\"Prior predictive\", xlabel=\"Days since 100 cases\", ylabel=\"Positive cases\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2b) Improve likelihood: Negative Binomial for counts\n",
        "\n",
        "The Normal likelihood allows negative counts and mismodels dispersion. We switch to a Negative Binomial with an overdispersion parameter `alpha`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t = df_country.days_since_100.values\n",
        "confirmed = df_country.confirmed.values\n",
        "\n",
        "with pm.Model() as model_exp2:\n",
        "    a = pm.Normal('a', mu=100, sigma=25)\n",
        "    b = pm.Normal('b', mu=0.3, sigma=0.1)\n",
        "    growth = a * (1 + b) ** t\n",
        "    alpha = pm.Gamma(\"alpha\", mu=6, sigma=1)\n",
        "    pm.NegativeBinomial('obs', growth, alpha=alpha, observed=confirmed)\n",
        "\n",
        "with model_exp2:\n",
        "    prior_pred2 = pm.sample_prior_predictive(random_seed=RANDOM_SEED)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "ax.plot(prior_pred2.prior_predictive['obs'].values.squeeze().T, color=\"0.5\", alpha=.1)\n",
        "ax.set(ylim=(-100, 1000), xlim=(0, 10), title=\"Prior predictive (NB)\", xlabel=\"Days since 100 cases\", ylabel=\"Positive cases\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4) Fit model and 5) Assess convergence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with model_exp2:\n",
        "    trace_exp2 = pm.sample(**sampler_kwargs, random_seed=RANDOM_SEED)\n",
        "\n",
        "az.plot_trace(trace_exp2, var_names=['a', 'b', 'alpha']);\n",
        "plt.tight_layout();\n",
        "\n",
        "az.summary(trace_exp2, var_names=['a', 'b', 'alpha'])\n",
        "\n",
        "az.plot_energy(trace_exp2);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6) Posterior predictive check and residuals\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with model_exp2:\n",
        "    post_pred = pm.sample_posterior_predictive(trace_exp2.posterior)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "ax.plot(post_pred.posterior_predictive['obs'].sel(chain=0).values.squeeze().T, color='0.5', alpha=.05)\n",
        "ax.plot(confirmed, color='r', label='data')\n",
        "ax.set(xlabel=\"Days since 100 cases\", ylabel=\"Confirmed cases (log scale)\", title=country, yscale=\"log\")\n",
        "ax.legend();\n",
        "\n",
        "# Residuals\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "resid = post_pred.posterior_predictive[\"obs\"].sel(chain=0) - confirmed\n",
        "ax.plot(resid.T, color=\"0.5\", alpha=.01)\n",
        "ax.set(ylim=(-50_000, 200_000), ylabel=\"Residual\", xlabel=\"Days since 100 cases\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7) Improve model with constrained priors (exp3) and compare to exp2\n",
        "\n",
        "Reflect prior knowledge: intercept â‰¥ 100 (because we start at 100 cases) and positive growth. Use a constrained prior for `alpha`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t = df_country.days_since_100.values\n",
        "confirmed = df_country.confirmed.values\n",
        "\n",
        "with pm.Model() as model_exp3:\n",
        "    a0 = pm.HalfNormal('a0', sigma=25)\n",
        "    a = pm.Deterministic('a', a0 + 100)\n",
        "    b = pm.HalfNormal('b', sigma=0.2)\n",
        "    growth = a * (1 + b) ** t\n",
        "    alpha = pm.Gamma(\"alpha\", mu=6, sigma=1)\n",
        "    pm.NegativeBinomial('obs', growth, alpha=alpha, observed=confirmed)\n",
        "\n",
        "with model_exp3:\n",
        "    prior_pred3 = pm.sample_prior_predictive(random_seed=RANDOM_SEED)\n",
        "    trace_exp3 = pm.sample(**sampler_kwargs, random_seed=RANDOM_SEED)\n",
        "\n",
        "az.plot_trace(trace_exp3, var_names=['a', 'b', 'alpha']);\n",
        "plt.tight_layout();\n",
        "\n",
        "# Compute log-likelihoods for comparison\n",
        "with model_exp2:\n",
        "    pm.compute_log_likelihood(trace_exp2)\n",
        "with model_exp3:\n",
        "    pm.compute_log_likelihood(trace_exp3)\n",
        "\n",
        "cmp = az.compare({\"exp2\": trace_exp2, \"exp3\": trace_exp3})\n",
        "az.plot_compare(cmp);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Forecasting with pm.Data\n",
        "\n",
        "Switch to `pm.Data` containers for time and case series to support future-time posterior predictive forecasting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with pm.Model() as model_exp4:\n",
        "    t_data = pm.Data('t', df_country.days_since_100.values)\n",
        "    confirmed_data = pm.Data('confirmed', df_country.confirmed.values)\n",
        "\n",
        "    a0 = pm.HalfNormal('a0', sigma=25)\n",
        "    a = pm.Deterministic('a', a0 + 100)\n",
        "    b = pm.HalfNormal('b', sigma=0.2)\n",
        "    growth = a * (1 + b) ** t_data\n",
        "\n",
        "    pm.NegativeBinomial('obs', growth, alpha=pm.Gamma(\"alpha\", mu=6, sigma=1), observed=confirmed_data)\n",
        "\n",
        "    trace_exp4 = pm.sample(**sampler_kwargs, random_seed=RANDOM_SEED)\n",
        "\n",
        "# Forecast next 60 days\n",
        "forecast_days = 60\n",
        "future_days = np.arange(len(df_country.days_since_100.values), len(df_country.days_since_100.values) + forecast_days)\n",
        "\n",
        "with model_exp4:\n",
        "    pm.set_data({'t': np.concatenate([df_country.days_since_100.values, future_days]),\n",
        "                 'confirmed': np.concatenate([df_country.confirmed.values, np.zeros(forecast_days, dtype='int')])})\n",
        "    forecast = pm.sample_posterior_predictive(trace_exp4.posterior)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "historical_days = len(df_country.days_since_100.values)\n",
        "all_days = np.arange(historical_days + forecast_days)\n",
        "\n",
        "forecast_samples = forecast.posterior_predictive['obs'].values\n",
        "forecast_mean = forecast_samples.mean(axis=(0, 1))\n",
        "forecast_lower = np.percentile(forecast_samples, 2.5, axis=(0, 1))\n",
        "forecast_upper = np.percentile(forecast_samples, 97.5, axis=(0, 1))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "ax.plot(np.arange(historical_days), df_country.confirmed.values, label='Observed', color='black')\n",
        "ax.plot(all_days, forecast_mean, label='Mean forecast', color='blue')\n",
        "ax.fill_between(all_days, forecast_lower, forecast_upper, color='blue', alpha=0.2, label='95% CI')\n",
        "ax.axvline(historical_days-1, color='gray', linestyle='--')\n",
        "ax.set(title=f'COVID-19 Forecast for {country}', xlabel='Days since 100 cases', ylabel='Confirmed cases')\n",
        "ax.legend();\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Logistic growth model and comparison\n",
        "\n",
        "The exponential model cannot capture plateauing behavior. We fit a logistic growth model with a carrying capacity and compare.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_country_full = df.query(f'country==\"{country}\"').loc[:date]\n",
        "\n",
        "with pm.Model() as logistic_model:\n",
        "    t_data = pm.Data('t', df_country_full.days_since_100.values)\n",
        "    confirmed_data = pm.Data('confirmed', df_country_full.confirmed.values)\n",
        "\n",
        "    a0 = pm.HalfNormal('a0', sigma=25)\n",
        "    intercept = pm.Deterministic('intercept', a0 + 100)\n",
        "    b = pm.HalfNormal('b', sigma=0.2)\n",
        "    carrying_capacity = pm.Uniform('carrying_capacity', lower=1_000, upper=80_000_000)\n",
        "    a = carrying_capacity / intercept - 1\n",
        "    growth = carrying_capacity / (1 + a * pm.math.exp(-b * t_data))\n",
        "\n",
        "    pm.NegativeBinomial('obs', growth, alpha=pm.Gamma(\"alpha\", mu=6, sigma=1), observed=confirmed_data)\n",
        "\n",
        "    trace_logistic = pm.sample(**sampler_kwargs, target_accept=0.9, random_seed=RANDOM_SEED)\n",
        "    pm.sample_posterior_predictive(trace_logistic, extend_inferencedata=True)\n",
        "\n",
        "az.plot_trace(trace_logistic);\n",
        "plt.tight_layout();\n",
        "\n",
        "with model_exp4:\n",
        "    pm.set_data({\"t\": df_country_full.days_since_100.values, \"confirmed\": df_country_full.confirmed.values})\n",
        "    trace_exp4_full = pm.sample(**sampler_kwargs, random_seed=RANDOM_SEED)\n",
        "\n",
        "with model_exp4:\n",
        "    pm.compute_log_likelihood(trace_exp4_full)\n",
        "with logistic_model:\n",
        "    pm.compute_log_likelihood(trace_logistic)\n",
        "\n",
        "az.plot_compare(az.compare({\"exp4\": trace_exp4_full, \"logistic\": trace_logistic}))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Validation on another country\n",
        "\n",
        "Fit the logistic model to a different country (e.g., US) to probe assumptions and generalization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "country2 = 'US'\n",
        "df_country2 = df.query(f'country==\"{country2}\"').loc[:date]\n",
        "\n",
        "with pm.Model() as logistic_model2:\n",
        "    t_data = pm.Data('t', df_country2.days_since_100.values)\n",
        "    confirmed_data = pm.Data('confirmed', df_country2.confirmed.values)\n",
        "\n",
        "    a0 = pm.HalfNormal('a0', sigma=25)\n",
        "    intercept = pm.Deterministic('intercept', a0 + 100)\n",
        "    b = pm.HalfNormal('b', sigma=0.2)\n",
        "    carrying_capacity = pm.Uniform('carrying_capacity', lower=1_000, upper=100_000_000)\n",
        "    a = carrying_capacity / intercept - 1\n",
        "    growth = carrying_capacity / (1 + a * pm.math.exp(-b * t_data))\n",
        "\n",
        "    pm.NegativeBinomial('obs', growth, alpha=pm.Gamma(\"alpha\", mu=6, sigma=1), observed=confirmed_data)\n",
        "\n",
        "    trace_logistic_us = pm.sample(**sampler_kwargs, random_seed=RANDOM_SEED)\n",
        "    pm.sample_posterior_predictive(trace_logistic_us, extend_inferencedata=True)\n",
        "\n",
        "az.plot_trace(trace_logistic_us); plt.tight_layout();\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "ax.plot(trace_logistic_us.posterior_predictive['obs'].sel(chain=0).squeeze().values.T, color='0.5', alpha=.05)\n",
        "ax.plot(df_country2.confirmed.values, color='r')\n",
        "ax.set(xlabel='Days since 100 cases', ylabel='Confirmed cases', title=country2);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optional: Calibration and sensitivity\n",
        "\n",
        "- LOO-PIT (leave-one-out probability integral transform) for calibration\n",
        "- Sensitivity to prior choices for `alpha` and slope `b`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loo_exp4 = az.loo(trace_exp4_full)\n",
        "loo_logistic = az.loo(trace_logistic)\n",
        "az.plot_loo_pit(idata=trace_logistic, y='obs');\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n",
        "\n",
        "- Gabry, J., Simpson, D., Vehtari, A., Betancourt, M., & Gelman, A. (2019). Visualization in Bayesian workflow. JRSS-A, 182(2), 389â€“402.\n",
        "- Gelman, A., Hwang, J., & Vehtari, A. (2014). Understanding predictive information criteria for Bayesian models. Statistics and Computing, 24(6), 997â€“1016.\n",
        "- Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., & BÃ¼rkner, P.-C. (2019). Rank-normalization, folding, and localization: An improved R-hat. arXiv:1903.08008.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sampler statistics and tuning behavior\n",
        "\n",
        "Sampler statistics expose what HMC/NUTS is doing under the hood. We examine trees (depth), acceptance rates, and energy behavior to detect pathologies. Consistently hitting maximum tree depth, low acceptance, or poor energy geometry suggest re-tuning or reparameterization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect sampler stats from the latest fit (e.g., NB exp2)\n",
        "trace = trace_exp2\n",
        "trace.sample_stats\n",
        "\n",
        "# Tree depth by chain\n",
        "trace.sample_stats[\"tree_depth\"].plot(col=\"chain\", ls=\"none\", marker=\".\", alpha=0.3);\n",
        "\n",
        "# Acceptance rate histogram\n",
        "trace.sample_stats[\"acceptance_rate\"].plot.hist(bins=20, density=True);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Posterior analysis and interpretation\n",
        "\n",
        "After confirming sampler health, we interpret posteriors: marginal distributions, correlation structure, credible intervals, and effect sizes. Summaries should be coupled with plots to guard against overreliance on point estimates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "az.summary(trace_exp2, var_names=['a', 'b', 'alpha'], round_to=2)\n",
        "az.plot_posterior(trace_exp2, var_names=['a', 'b', 'alpha'], kind='hist'); plt.tight_layout();\n",
        "az.plot_pair(trace_exp2, var_names=['a', 'b'], kind='kde'); plt.tight_layout();\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Enhanced posterior predictive checks\n",
        "\n",
        "Posterior predictive checks evaluate whether replicated data from the model resemble the observed data. We combine multiple perspectives: histograms, cumulative distribution comparisons, and ECDF overlays. Visual agreement indicates good fit, while systematic deviations point to model misspecification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with model_exp2:\n",
        "    post_pred2 = pm.sample_posterior_predictive(trace_exp2.posterior)\n",
        "\n",
        "# Histogram and KDE overlays\n",
        "az.plot_ppc(post_pred2, data_pairs={\"obs\": \"obs\"});\n",
        "# Cumulative PPC to assess tail behavior\n",
        "az.plot_ppc(post_pred2, kind='cumulative', mean=False);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Diagnostics: Divergences, BFMI, and Energy\n",
        "\n",
        "Divergences flag integration failures in HMC that often arise from problematic geometries. BFMI quantifies how well momentum resampling matches the marginal energy distribution. Poor overlap between the marginal energy and energy transitions indicates inefficient exploration.\n",
        "\n",
        "We will:\n",
        "- Check for divergences and their locations\n",
        "- Compute BFMI and visualize energy overlap\n",
        "- Discuss remediation: increase `target_accept`, reparameterize, rescale predictors/responses\n",
        "\n",
        "These diagnostics and their interpretations are adapted from the existing model checking notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Diagnostics: Divergences, BFMI, and Energy\n",
        "\n",
        "As we have seen, Hamiltonian Monte Carlo (and NUTS) performs numerical integration to explore the posterior distribution. When the integration goes wrong, it can go dramatically wrong. Divergences signal numerical integration failures in regions of difficult geometry and must be investigated.\n",
        "\n",
        "![diverging HMC](images/diverging_hmc.png)\n",
        "\n",
        "Two practical remedies:\n",
        "1. Increase the target acceptance rate (e.g., `target_accept=0.9`), which typically reduces the step size and improves integration accuracy.\n",
        "2. Reparameterize the model to improve geometry (e.g., non-centered parameterizations, appropriate scaling of predictors/responses).\n",
        "\n",
        "The Bayesian Fraction of Missing Information (BFMI) quantifies how well momentum resampling matches the marginal energy distribution. Poor overlap between the marginal energy and the distribution of energy transitions indicates inefficient exploration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BFMI and energy plots for recent fits\n",
        "az.bfmi(trace_exp2)\n",
        "az.plot_energy(trace_exp2);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model evaluation and comparison (LOO/WAIC)\n",
        "\n",
        "Information criteria approximate out-of-sample predictive performance by penalizing model complexity. With `arviz.compare`, we obtain LOO/WAIC, standard errors, and model weights. Interpret differences with uncertainty: small deltas relative to SE suggest caution. Use weights as soft evidence, not as decisive selection rules.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cmp2 = az.compare({\"exp4\": trace_exp4_full, \"logistic\": trace_logistic})\n",
        "cmp2\n",
        "az.plot_compare(cmp2);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best practices for model improvement\n",
        "\n",
        "- Prior predictive checks: validate that prior + likelihood can generate plausible data\n",
        "- Center/scale predictors; consider link functions matching the outcome's domain\n",
        "- Use non-centered parameterizations in hierarchical or weakly-informed scales\n",
        "- Diagnose and remediate: increase `target_accept`, revisit priors, reparameterize\n",
        "- Compare candidate models with LOO; prefer simpler models unless predictive gains are clear\n",
        "- Validate on held-out data or out-of-sample regions (e.g., different country/time)\n",
        "- Communicate uncertainty with intervals, posterior predictive envelopes, and scenario analyses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: Hierarchical COVID extension\n",
        "\n",
        "Counts from multiple countries can be modeled with partial pooling to share information while allowing country-specific variation. Start with varying intercepts and consider varying slopes, using a log link to ensure positivity and a Negative Binomial likelihood. Diagnostics and PPCs proceed as before, with special attention to non-centered parameterizations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "subset_countries = ['Germany', 'US', 'Italy', 'Spain']\n",
        "df_sub = df[df.country.isin(subset_countries)].copy()\n",
        "country_idx, countries_unique = pd.factorize(df_sub.country)\n",
        "\n",
        "t = df_sub.days_since_100.values\n",
        "y = df_sub.confirmed.values\n",
        "n = len(y)\n",
        "C = len(countries_unique)\n",
        "\n",
        "with pm.Model(coords={\"country\": countries_unique}) as hier_model:\n",
        "    a_group = pm.Normal('a_group', 0.0, 10.0)\n",
        "    b_group = pm.HalfNormal('b_group', 0.5)\n",
        "\n",
        "    a_raw = pm.Normal('a_raw', 0.0, 1.0, dims=\"country\")\n",
        "    b_raw = pm.Normal('b_raw', 0.0, 1.0, dims=\"country\")\n",
        "\n",
        "    sigma_a = pm.HalfNormal('sigma_a', 10.0)\n",
        "    sigma_b = pm.HalfNormal('sigma_b', 0.5)\n",
        "\n",
        "    a = pm.Deterministic('a', 100 + a_group + sigma_a * a_raw, dims=\"country\")\n",
        "    b = pm.Deterministic('b', pm.math.abs(b_group + sigma_b * b_raw), dims=\"country\")\n",
        "\n",
        "    mu = a[country_idx] * (1 + b[country_idx]) ** t\n",
        "\n",
        "    alpha = pm.Gamma('alpha', mu=6, sigma=1)\n",
        "    pm.NegativeBinomial('obs', mu, alpha=alpha, observed=y)\n",
        "\n",
        "    hier_trace = pm.sample(tune=1500, chains=4, cores=4, random_seed=RANDOM_SEED, target_accept=0.9)\n",
        "\n",
        "az.plot_trace(hier_trace, var_names=['a_group', 'b_group', 'sigma_a', 'sigma_b']); plt.tight_layout();\n",
        "az.summary(hier_trace, var_names=['a_group', 'b_group', 'sigma_a', 'sigma_b'])\n",
        "az.plot_energy(hier_trace);\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext watermark\n",
        "%watermark -n -u -v -iv -w\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
