{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c22206b8",
   "metadata": {},
   "source": [
    "# Session 2: Building Models with PyMC and MCMC Fundamentals\n",
    "\n",
    "In the previous session, we explored the foundations of Bayesian inference, including Bayes' theorem, conjugate priors, and the mechanics of Bayesian updating. While conjugate priors provide elegant closed-form solutions, they are limited to a small set of model families. Real-world problems often require more flexible models that don't have analytical solutions.\n",
    "\n",
    "This session introduces PyMC, a powerful probabilistic programming framework that enables us to build and analyze complex Bayesian models. We'll learn how to specify models using PyMC's intuitive API, understand the theoretical foundations of Markov Chain Monte Carlo (MCMC) methods that make modern Bayesian computation possible, and work through practical examples that demonstrate the complete modeling workflow.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "\n",
    "1. **Build probabilistic models in PyMC**: Understand PyMC's core components including distributions, random variables, and the model context\n",
    "2. **Specify model structure**: Learn how to encode assumptions about data generating processes using priors and likelihoods\n",
    "3. **Understand MCMC fundamentals**: Grasp why we need MCMC, how it works conceptually, and what makes it powerful for Bayesian inference\n",
    "4. **Implement complete Bayesian analyses**: Build, fit, and interpret results from real-world models including linear regression\n",
    "\n",
    "## Why PyMC?\n",
    "\n",
    "PyMC provides several key advantages for Bayesian modeling:\n",
    "\n",
    "- **Expressive model specification**: Write models that look like their mathematical notation\n",
    "- **Automatic differentiation**: No need to derive gradients by hand\n",
    "- **State-of-the-art samplers**: Access to efficient MCMC algorithms like NUTS (No-U-Turn Sampler)\n",
    "- **Comprehensive diagnostics**: Built-in tools for assessing convergence and model quality\n",
    "- **Integration with the PyData ecosystem**: Works seamlessly with NumPy, Pandas, and visualization libraries\n",
    "\n",
    "Let's begin by setting up our environment and exploring PyMC's fundamental concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2599104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import polars as pl\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "from itertools import takewhile\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad612c5",
   "metadata": {},
   "source": [
    "## Part 1: Building Models in PyMC\n",
    "\n",
    "Probabilistic programming represents a paradigm shift in how we approach statistical modeling. Instead of deriving update equations or coding samplers by hand, we declare the structure of our model and let the framework handle the computational details. PyMC exemplifies this approach by providing an intuitive interface that closely mirrors mathematical notation while leveraging sophisticated algorithms under the hood.\n",
    "\n",
    "### The Philosophy of Probabilistic Programming\n",
    "\n",
    "In traditional statistical programming, we often work backwards from the inference algorithm. We might derive the posterior distribution analytically (if we're lucky), implement a specific sampling scheme, or resort to approximations. This approach becomes increasingly difficult as models grow in complexity.\n",
    "\n",
    "Probabilistic programming flips this workflow. We start by specifying what we know:\n",
    "- **Prior knowledge** about parameters before seeing data\n",
    "- **The data generating process** that connects parameters to observations\n",
    "- **The observed data** itself\n",
    "\n",
    "From this specification, PyMC automatically constructs the computational graph needed for inference, applies appropriate transformations for constrained parameters, and selects suitable sampling algorithms.\n",
    "\n",
    "### PyMC's Core Abstractions\n",
    "\n",
    "PyMC organizes probabilistic models around several key concepts:\n",
    "\n",
    "1. **The Model Context**: Every PyMC model exists within a context that tracks relationships between variables. This context manager pattern ensures that all model components are properly registered and connected.\n",
    "\n",
    "2. **Random Variables**: These represent quantities with uncertainty. In Bayesian modeling, parameters are random variables with prior distributions, and data are random variables with likelihood distributions.\n",
    "\n",
    "3. **Distributions**: PyMC provides a comprehensive library of probability distributions. Each distribution can create random variables when used within a model context.\n",
    "\n",
    "4. **Deterministic Transformations**: Often we need to transform parameters or compute derived quantities. PyMC tracks these deterministic relationships to maintain the full model structure.\n",
    "\n",
    "5. **Observed Data**: By marking random variables as observed, we condition the model on actual data, transforming prior distributions into posterior distributions.\n",
    "\n",
    "Let's explore these concepts through hands-on examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e2679c",
   "metadata": {},
   "source": [
    "### The Distribution Class\n",
    "\n",
    "At the heart of PyMC lies the `Distribution` class, which encapsulates probability distributions and their properties. Understanding how distributions work in PyMC is crucial for effective model building.\n",
    "\n",
    "#### Random Variables and Distributions\n",
    "\n",
    "In probability theory, a random variable is a function that assigns numerical values to outcomes of random phenomena. In PyMC, we create random variables by instantiating distributions within a model context. This seemingly simple act triggers a sophisticated chain of events:\n",
    "\n",
    "1. **Registration**: The variable is registered with the model's computational graph\n",
    "2. **Transformation**: If the variable has constrained support (e.g., positive only), PyMC automatically applies bijective transformations to map it to unconstrained space\n",
    "3. **Tracking**: Dependencies between variables are recorded for efficient computation\n",
    "\n",
    "#### Key Properties of Distributions\n",
    "\n",
    "Every PyMC distribution provides several important methods and properties:\n",
    "\n",
    "- **Sampling**: Generate random draws from the distribution\n",
    "- **Log probability**: Evaluate the log probability density/mass at specific values\n",
    "- **Moments**: Access theoretical moments like mean and variance\n",
    "- **Support**: The valid range of values for the distribution\n",
    "\n",
    "#### Common Arguments\n",
    "\n",
    "When creating distributions in PyMC, you'll encounter these standard arguments:\n",
    "\n",
    "- **`name`** (required): A unique string identifier for the variable. This name is used in results, plots, and diagnostics.\n",
    "\n",
    "- **`shape`**: Specifies the dimensions of the variable. For example, `shape=(3,)` creates three independent draws from the distribution.\n",
    "\n",
    "- **`dims`**: Named dimensions that provide semantic meaning and integrate with labeled coordinates. This is particularly useful for hierarchical models.\n",
    "\n",
    "- **`observed`**: Providing data here conditions the distribution, turning it from a prior into a likelihood. This is how we incorporate evidence into our models.\n",
    "\n",
    "- **`transform`**: While usually handled automatically, you can specify custom transformations for specialized needs.\n",
    "\n",
    "- **`initval`**: Starting values for sampling algorithms. PyMC usually chooses sensible defaults, but manual specification can help with convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db3d84e",
   "metadata": {},
   "source": [
    "### Working with Distributions\n",
    "\n",
    "PyMC distributions can be used in two ways:\n",
    "\n",
    "1. **Inside a model context**: Creates a random variable tracked by the model\n",
    "2. **Standalone with `.dist()`**: Creates a distribution object for direct use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe7c44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standalone distribution - useful for exploration\n",
    "x_dist = pm.Normal.dist(mu=0, sigma=1)\n",
    "\n",
    "# Draw samples from the distribution\n",
    "samples = pm.draw(x_dist, draws=1000, random_seed=rng)\n",
    "\n",
    "# Calculate log probability\n",
    "log_prob = pm.logp(x_dist, 0.5).eval()\n",
    "\n",
    "print(f\"Log probability of x=0.5: {log_prob:.4f}\")\n",
    "px.histogram(samples, title=\"Samples from Normal(0, 1)\").update_layout(\n",
    "    xaxis_title=\"Value\", yaxis_title=\"Count\", showlegend=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbd1cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inside a model context - creates tracked random variables\n",
    "with pm.Model() as model:\n",
    "    # Prior distribution\n",
    "    mu = pm.Normal('mu', mu=0, sigma=10)\n",
    "    \n",
    "    # Another prior\n",
    "    sigma = pm.HalfNormal('sigma', sigma=5)\n",
    "    \n",
    "    # Likelihood (with observed data)\n",
    "    y = pm.Normal('y', mu=mu, sigma=sigma, observed=[1.2, 0.8, 1.5, 0.9])\n",
    "\n",
    "# The model now contains these variables\n",
    "print(f\"Model variables: {list(model.values_to_rvs.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13aa21d",
   "metadata": {},
   "source": [
    "## Part 2: Model Specification and Structure\n",
    "\n",
    "PyMC models are built using a context manager pattern. The `Model` context tracks all variables and their relationships, creating a directed acyclic graph (DAG) that represents the probabilistic model.\n",
    "\n",
    "### Core Components\n",
    "\n",
    "1. **Stochastic Random Variables**: Variables with uncertainty, defined by probability distributions\n",
    "   - Unobserved (parameters to estimate)\n",
    "   - Observed (data)\n",
    "\n",
    "2. **Deterministic Variables**: Variables computed from other variables with no additional randomness\n",
    "\n",
    "3. **Factor Potentials**: Terms that modify the joint log-probability\n",
    "\n",
    "4. **Data Containers**: Special variables for data that might change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01f2a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Complete model structure\n",
    "np.random.seed(42)\n",
    "true_mu = 5\n",
    "true_sigma = 2\n",
    "data = np.random.normal(true_mu, true_sigma, size=100)\n",
    "\n",
    "with pm.Model() as structured_model:\n",
    "    # 1. Stochastic variables (priors)\n",
    "    mu = pm.Normal('mu', mu=0, sigma=10)\n",
    "    log_sigma = pm.Normal('log_sigma', mu=0, sigma=1)\n",
    "    \n",
    "    # 2. Deterministic transformation\n",
    "    sigma = pm.Deterministic('sigma', pm.math.exp(log_sigma))\n",
    "    \n",
    "    # 3. Data container (allows updating data later)\n",
    "    x_data = pm.Data('x_data', data)\n",
    "    \n",
    "    # 4. Likelihood (observed stochastic variable)\n",
    "    likelihood = pm.Normal('likelihood', mu=mu, sigma=sigma, observed=x_data)\n",
    "    \n",
    "    # 5. Factor potential (example: constrain mu to be positive)\n",
    "    positive_mu = pm.Potential('positive_mu', \n",
    "                              pm.math.switch(mu > 0, 0, -np.inf))\n",
    "\n",
    "# Visualize the model structure\n",
    "pm.model_to_graphviz(structured_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd83a18d",
   "metadata": {},
   "source": [
    "## Part 3: Likelihood Functions and Prior Distributions\n",
    "\n",
    "In Bayesian modeling, we combine prior beliefs about parameters with data through a likelihood function. PyMC provides a rich set of distributions for both.\n",
    "\n",
    "### Common Prior Distributions\n",
    "\n",
    "**For location parameters (means, intercepts):**\n",
    "- `Normal(mu, sigma)`: When you expect values around a certain point\n",
    "- `StudentT(nu, mu, sigma)`: Robust to outliers, heavier tails than Normal\n",
    "- `Uniform(lower, upper)`: Flat prior over a range (use sparingly)\n",
    "\n",
    "**For scale parameters (standard deviations, variances):**\n",
    "- `HalfNormal(sigma)`: Positive values with mode at 0\n",
    "- `HalfCauchy(beta)`: Heavy-tailed, good for hierarchical models\n",
    "- `Exponential(lam)`: Alternative for positive parameters\n",
    "- `InverseGamma(alpha, beta)`: Traditional but less recommended\n",
    "\n",
    "**For probabilities and proportions:**\n",
    "- `Beta(alpha, beta)`: Flexible distribution on [0, 1]\n",
    "- `Dirichlet(a)`: Multivariate generalization of Beta\n",
    "\n",
    "**For counts:**\n",
    "- `Poisson(mu)`: Count data with equal mean and variance\n",
    "- `NegativeBinomial(mu, alpha)`: Overdispersed count data\n",
    "\n",
    "### Common Likelihood Functions\n",
    "\n",
    "**For continuous data:**\n",
    "- `Normal(mu, sigma)`: Symmetric errors around a mean\n",
    "- `StudentT(nu, mu, sigma)`: Heavy-tailed errors, robust to outliers\n",
    "- `Lognormal(mu, sigma)`: Positive data with right skew\n",
    "- `Gamma(alpha, beta)`: Positive continuous data\n",
    "- `Beta(alpha, beta)`: Data bounded between 0 and 1\n",
    "\n",
    "**For discrete data:**\n",
    "- `Bernoulli(p)`: Binary outcomes (0/1)\n",
    "- `Binomial(n, p)`: Number of successes in n trials\n",
    "- `Poisson(mu)`: Count data\n",
    "- `NegativeBinomial(mu, alpha)`: Overdispersed counts\n",
    "- `Categorical(p)`: One of K categories\n",
    "- `Multinomial(n, p)`: Counts in K categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30f963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Exploring different prior distributions\n",
    "fig = make_subplots(rows=2, cols=2, subplot_titles=[\n",
    "    'Location Priors', 'Scale Priors', \n",
    "    'Probability Priors', 'Count Priors'\n",
    "])\n",
    "\n",
    "x_range = np.linspace(-5, 5, 1000)\n",
    "x_positive = np.linspace(0.01, 5, 1000)\n",
    "x_prob = np.linspace(0, 1, 1000)\n",
    "x_count = np.arange(0, 20)\n",
    "\n",
    "# Location priors\n",
    "fig.add_trace(go.Scatter(x=x_range, y=pm.Normal.dist(0, 1).logp(x_range).eval(), \n",
    "                        name='Normal(0,1)', line=dict(width=2)), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=x_range, y=pm.StudentT.dist(nu=3, mu=0, sigma=1).logp(x_range).eval(), \n",
    "                        name='StudentT(3,0,1)', line=dict(width=2, dash='dash')), row=1, col=1)\n",
    "\n",
    "# Scale priors  \n",
    "fig.add_trace(go.Scatter(x=x_positive, y=pm.HalfNormal.dist(1).logp(x_positive).eval(), \n",
    "                        name='HalfNormal(1)', line=dict(width=2)), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=x_positive, y=pm.HalfCauchy.dist(1).logp(x_positive).eval(), \n",
    "                        name='HalfCauchy(1)', line=dict(width=2, dash='dash')), row=1, col=2)\n",
    "\n",
    "# Probability priors\n",
    "fig.add_trace(go.Scatter(x=x_prob, y=pm.Beta.dist(2, 2).logp(x_prob).eval(), \n",
    "                        name='Beta(2,2)', line=dict(width=2)), row=2, col=1)\n",
    "fig.add_trace(go.Scatter(x=x_prob, y=pm.Beta.dist(0.5, 0.5).logp(x_prob).eval(), \n",
    "                        name='Beta(0.5,0.5)', line=dict(width=2, dash='dash')), row=2, col=1)\n",
    "\n",
    "# Count priors\n",
    "fig.add_trace(go.Scatter(x=x_count, y=pm.Poisson.dist(5).logp(x_count).eval(), \n",
    "                        mode='markers+lines', name='Poisson(5)'), row=2, col=2)\n",
    "fig.add_trace(go.Scatter(x=x_count, y=pm.NegativeBinomial.dist(5, 2).logp(x_count).eval(), \n",
    "                        mode='markers+lines', name='NegBinom(5,2)', \n",
    "                        line=dict(dash='dash')), row=2, col=2)\n",
    "\n",
    "fig.update_layout(height=600, showlegend=True)\n",
    "fig.update_yaxes(title_text=\"Log Probability\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6f434a",
   "metadata": {},
   "source": [
    "## Part 4: Introduction to MCMC Methods\n",
    "\n",
    "Now that we understand how to specify models in PyMC, we need to address a fundamental question: how do we actually compute with these models? For all but the simplest cases, the posterior distribution cannot be derived analytically. This is where Markov Chain Monte Carlo (MCMC) methods become essential.\n",
    "\n",
    "### The Challenge of Bayesian Computation\n",
    "\n",
    "Recall Bayes' theorem:\n",
    "\n",
    "$$P(\\theta|x) = \\frac{P(x|\\theta) P(\\theta)}{P(x)}$$\n",
    "\n",
    "The denominator, $P(x) = \\int P(x|\\theta) P(\\theta) \\, d\\theta$, is called the marginal likelihood or evidence. For most models, this integral is intractable:\n",
    "\n",
    "1. **High dimensionality**: With multiple parameters, we need to integrate over many dimensions\n",
    "2. **Complex dependencies**: Parameters often have intricate relationships\n",
    "3. **Non-standard distributions**: The posterior rarely has a recognizable form\n",
    "\n",
    "### From Integration to Sampling\n",
    "\n",
    "MCMC methods sidestep the integration problem through a clever insight: instead of computing the posterior distribution directly, we can draw samples from it. With enough samples, we can approximate any quantity of interest:\n",
    "\n",
    "- **Posterior means**: $E[\\theta|x] \\approx \\frac{1}{n}\\sum_{i=1}^n \\theta_i$\n",
    "- **Credible intervals**: Use sample quantiles\n",
    "- **Posterior probabilities**: Proportion of samples in a region\n",
    "\n",
    "The key challenge is: how do we draw samples from a distribution when we only know it up to a normalizing constant?\n",
    "\n",
    "### The Markov Chain Monte Carlo Solution\n",
    "\n",
    "MCMC methods construct a Markov chain whose stationary distribution is the posterior distribution we want to sample from. The \"Markov\" property means that each sample depends only on the previous sample, not the entire history. The \"Monte Carlo\" aspect refers to the use of random sampling.\n",
    "\n",
    "The general MCMC algorithm follows this pattern:\n",
    "\n",
    "1. Start at some initial parameter values $\\theta^{(0)}$\n",
    "2. For iteration $t = 1, 2, ...$:\n",
    "   - Propose new parameter values $\\theta^*$ based on current values $\\theta^{(t-1)}$\n",
    "   - Accept or reject the proposal based on the posterior probability\n",
    "   - Set $\\theta^{(t)} = \\theta^*$ if accepted, otherwise $\\theta^{(t)} = \\theta^{(t-1)}$\n",
    "\n",
    "Different MCMC algorithms vary in how they propose new values and decide whether to accept them. The art lies in designing proposals that efficiently explore the posterior distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146d2643",
   "metadata": {},
   "source": [
    "## Part 5: Sampling Algorithms and Monte Carlo Integration\n",
    "\n",
    "To understand how MCMC works in practice, let's explore the fundamental concepts through concrete examples. We'll start with basic Monte Carlo integration and build up to understanding why sophisticated algorithms like those in PyMC are necessary.\n",
    "\n",
    "### Monte Carlo Integration\n",
    "\n",
    "The foundation of all Monte Carlo methods is a simple but powerful idea: we can approximate integrals using random samples. Consider estimating the expected value of a function $h(\\theta)$ under a probability distribution $p(\\theta)$:\n",
    "\n",
    "$$E[h(\\theta)] = \\int h(\\theta) p(\\theta) d\\theta$$\n",
    "\n",
    "If we can draw samples $\\theta_1, \\theta_2, ..., \\theta_n$ from $p(\\theta)$, then by the Law of Large Numbers:\n",
    "\n",
    "$$E[h(\\theta)] \\approx \\frac{1}{n} \\sum_{i=1}^n h(\\theta_i)$$\n",
    "\n",
    "This approximation becomes exact as $n \\to \\infty$, and we can quantify the uncertainty in our estimate using the Central Limit Theorem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4562a872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Monte Carlo estimation of pi\n",
    "# We'll estimate pi by randomly sampling points in a square and checking if they fall in a circle\n",
    "\n",
    "n_samples = 10000\n",
    "x = np.random.uniform(-1, 1, n_samples)\n",
    "y = np.random.uniform(-1, 1, n_samples)\n",
    "\n",
    "# Check if points are inside the unit circle\n",
    "inside_circle = (x**2 + y**2) <= 1\n",
    "\n",
    "# Estimate pi: (area of circle / area of square) = (pi * r^2) / (2r)^2 = pi/4\n",
    "pi_estimate = 4 * np.mean(inside_circle)\n",
    "\n",
    "# Visualize\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add points\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x[inside_circle], y=y[inside_circle],\n",
    "    mode='markers', marker=dict(size=2, color='blue'),\n",
    "    name='Inside circle'\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x[~inside_circle], y=y[~inside_circle],\n",
    "    mode='markers', marker=dict(size=2, color='red'),\n",
    "    name='Outside circle'\n",
    "))\n",
    "\n",
    "# Add circle\n",
    "theta = np.linspace(0, 2*np.pi, 100)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=np.cos(theta), y=np.sin(theta),\n",
    "    mode='lines', line=dict(color='black', width=2),\n",
    "    name='Unit circle'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'Monte Carlo Estimation of π ≈ {pi_estimate:.4f} (true value: {np.pi:.4f})',\n",
    "    xaxis=dict(scaleanchor=\"y\", scaleratio=1),\n",
    "    yaxis=dict(scaleanchor=\"x\", scaleratio=1),\n",
    "    width=600, height=600\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(f\"Estimated π: {pi_estimate:.4f}\")\n",
    "print(f\"True π:      {np.pi:.4f}\")\n",
    "print(f\"Error:       {abs(pi_estimate - np.pi):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c880ec6a",
   "metadata": {},
   "source": [
    "### The Metropolis-Hastings Algorithm\n",
    "\n",
    "While Monte Carlo integration is powerful, it assumes we can directly sample from the distribution of interest. In Bayesian inference, we typically can only evaluate the unnormalized posterior $\\tilde{p}(\\theta|x) = p(x|\\theta)p(\\theta)$. The Metropolis-Hastings algorithm provides an elegant solution to this problem.\n",
    "\n",
    "#### The Algorithm\n",
    "\n",
    "The Metropolis-Hastings algorithm generates a Markov chain whose stationary distribution is the target posterior distribution. The key insight is the **Metropolis acceptance criterion**, which ensures detailed balance:\n",
    "\n",
    "$$A(\\theta^* | \\theta) = \\min\\left\\{1, \\frac{\\tilde{p}(\\theta^*)}{\\tilde{p}(\\theta)} \\cdot \\frac{q(\\theta|\\theta^*)}{q(\\theta^*|\\theta)}\\right\\}$$\n",
    "\n",
    "where:\n",
    "- $\\tilde{p}(\\theta)$ is the unnormalized posterior\n",
    "- $q(\\theta^*|\\theta)$ is the proposal distribution\n",
    "- $A(\\theta^* | \\theta)$ is the acceptance probability\n",
    "\n",
    "This criterion automatically adjusts for asymmetric proposals and ensures the chain converges to the correct distribution.\n",
    "\n",
    "Let's implement the general Metropolis-Hastings algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff3fb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metropolis_hastings(pdf, prop_dist, init=0):\n",
    "    \"\"\"General Metropolis-Hastings sampler.\n",
    "    \n",
    "    Args:\n",
    "        pdf: Target probability density function (unnormalized is ok)\n",
    "        prop_dist: Proposal distribution (scipy.stats distribution)\n",
    "        init: Initial value\n",
    "        \n",
    "    Yields:\n",
    "        (sample, accepted): Current sample and whether it was accepted\n",
    "    \"\"\"\n",
    "    current = init\n",
    "    while True:\n",
    "        # Propose new state from proposal distribution\n",
    "        prop = prop_dist.rvs()\n",
    "        \n",
    "        # Calculate acceptance ratio\n",
    "        p_accept = min(1, pdf(prop) / pdf(current) * \n",
    "                      prop_dist.pdf(current) / prop_dist.pdf(prop))\n",
    "        \n",
    "        # Accept or reject\n",
    "        accept = np.random.rand() < p_accept\n",
    "        if accept:\n",
    "            current = prop\n",
    "        yield current, accept\n",
    "        \n",
    "def gen_samples(draws, sampler):\n",
    "    \"\"\"Generate samples from a sampler.\"\"\"\n",
    "    samples = np.empty(draws)\n",
    "    accepts = 0\n",
    "    for idx, (z, accept) in takewhile(lambda j: j[0] < draws, enumerate(sampler)):\n",
    "        accepts += accept\n",
    "        samples[idx] = z\n",
    "    return samples, accepts\n",
    "\n",
    "# Example: Sample from a mixture of Gaussians\n",
    "def target_pdf(x):\n",
    "    \"\"\"Mixture of two Gaussians\"\"\"\n",
    "    return 0.3 * st.norm.pdf(x, -2, 0.8) + 0.7 * st.norm.pdf(x, 3, 1.2)\n",
    "\n",
    "# Use a wide normal as proposal\n",
    "proposal_dist = st.norm(0, 10)\n",
    "\n",
    "# Generate samples\n",
    "samples, accepts = gen_samples(10_000, metropolis_hastings(target_pdf, proposal_dist))\n",
    "\n",
    "# Visualize results\n",
    "t = np.linspace(-6, 8, 500)\n",
    "pdf_values = [target_pdf(x) for x in t]\n",
    "\n",
    "hist = go.Histogram(\n",
    "    x=samples,\n",
    "    histnorm='probability density',\n",
    "    name='Samples',\n",
    "    marker=dict(color='rgba(0, 0, 255, 0.7)')\n",
    ")\n",
    "\n",
    "pdf_curve = go.Scatter(\n",
    "    x=t, y=pdf_values,\n",
    "    mode='lines',\n",
    "    name='True PDF',\n",
    "    line=dict(color='orange', width=2)\n",
    ")\n",
    "\n",
    "go.Figure(\n",
    "    data=[hist, pdf_curve]\n",
    ").update_layout(\n",
    "    title=f'Metropolis-Hastings: {samples.size:,d} samples with {100 * accepts / samples.size:.1f}% acceptance rate',\n",
    "    xaxis_title='Value',\n",
    "    yaxis_title='Density',\n",
    "    width=750,\n",
    "    height=400\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cd6798",
   "metadata": {},
   "source": [
    "### Random Walk Metropolis-Hastings\n",
    "\n",
    "The general Metropolis-Hastings algorithm can be inefficient if the proposal distribution is poorly chosen. A popular special case is **Random Walk Metropolis**, where the proposal is centered at the current position:\n",
    "\n",
    "$$\\theta^* \\sim \\mathcal{N}(\\theta, \\sigma^2)$$\n",
    "\n",
    "This symmetric proposal simplifies the acceptance ratio because the proposal terms cancel out:\n",
    "\n",
    "$$A(\\theta^* | \\theta) = \\min\\left\\{1, \\frac{\\tilde{p}(\\theta^*)}{\\tilde{p}(\\theta)}\\right\\}$$\n",
    "\n",
    "The key hyperparameter is the step size $\\sigma$:\n",
    "- **Too small**: Chain takes tiny steps and explores slowly (high acceptance, slow mixing)\n",
    "- **Too large**: Many proposals are rejected (low acceptance, slow mixing)\n",
    "- **Just right**: Balance between acceptance rate and step size (typically 20-50% acceptance)\n",
    "\n",
    "Let's implement and compare different step sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0be924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_walk_metropolis(pdf, step_size, init=0):\n",
    "    \"\"\"Random walk Metropolis algorithm.\n",
    "    \n",
    "    Args:\n",
    "        pdf: Target probability density function (unnormalized is ok)\n",
    "        step_size: Standard deviation of proposal distribution\n",
    "        init: Initial value\n",
    "        \n",
    "    Yields:\n",
    "        (sample, accepted): Current sample and whether it was accepted\n",
    "    \"\"\"\n",
    "    current = init\n",
    "    while True:\n",
    "        # Random walk proposal\n",
    "        prop = current + np.random.normal(0, step_size)\n",
    "        \n",
    "        # Simple acceptance ratio (proposal terms cancel)\n",
    "        p_accept = min(1, pdf(prop) / pdf(current))\n",
    "        \n",
    "        # Accept or reject\n",
    "        accept = np.random.rand() < p_accept\n",
    "        if accept:\n",
    "            current = prop\n",
    "        yield current, accept\n",
    "\n",
    "# Compare different step sizes\n",
    "fig = make_subplots(rows=3, cols=1, \n",
    "                    subplot_titles=['Small Step Size (σ=0.1)', \n",
    "                                   'Medium Step Size (σ=8.0)', \n",
    "                                   'Large Step Size (σ=70.0)'])\n",
    "\n",
    "step_sizes = [0.1, 8.0, 70.0]\n",
    "\n",
    "for i, step_size in enumerate(step_sizes, 1):\n",
    "    # Generate samples\n",
    "    samples, accepts = gen_samples(10_000, random_walk_metropolis(target_pdf, step_size))\n",
    "    \n",
    "    # Calculate t and pdf values for the line\n",
    "    t = np.linspace(samples.min(), samples.max(), 500)\n",
    "    pdf_values = [target_pdf(x) for x in t]\n",
    "    \n",
    "    # Add histogram\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=samples,\n",
    "            histnorm='probability density',\n",
    "            marker=dict(color='rgba(0, 0, 255, 0.7)'),\n",
    "            name=f\"Samples (σ={step_size})\",\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=i, col=1\n",
    "    )\n",
    "    \n",
    "    # Add PDF line\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=t,\n",
    "            y=pdf_values,\n",
    "            mode='lines',\n",
    "            line=dict(color='orange', width=2),\n",
    "            name=f\"True PDF\",\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=i, col=1\n",
    "    )\n",
    "    \n",
    "    # Add annotation with acceptance rate\n",
    "    fig.add_annotation(\n",
    "        text=f\"Acceptance rate: {100 * accepts / samples.size:.1f}%\",\n",
    "        xref=f\"x{i}\", yref=f\"y{i}\",\n",
    "        x=0.95, y=0.95,\n",
    "        xanchor=\"right\", yanchor=\"top\",\n",
    "        showarrow=False,\n",
    "        bgcolor=\"white\",\n",
    "        bordercolor=\"black\",\n",
    "        borderwidth=1\n",
    "    )\n",
    "\n",
    "fig.update_layout(height=900, width=700, showlegend=False)\n",
    "fig.update_xaxes(title_text=\"Value\", row=3, col=1)\n",
    "fig.update_yaxes(title_text=\"Density\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8906701e",
   "metadata": {},
   "source": [
    "### Visualizing MCMC Behavior\n",
    "\n",
    "To better understand how different step sizes affect the sampler's behavior, let's look at trace plots that show the evolution of the chain over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8d0f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate shorter chains for visualization\n",
    "n_steps = 1000\n",
    "\n",
    "fig = make_subplots(rows=3, cols=1, \n",
    "                    subplot_titles=['Small Step Size (σ=0.1): Slow Exploration', \n",
    "                                   'Medium Step Size (σ=8.0): Good Mixing', \n",
    "                                   'Large Step Size (σ=70.0): Many Rejections'])\n",
    "\n",
    "for i, step_size in enumerate(step_sizes, 1):\n",
    "    # Generate samples\n",
    "    sampler = random_walk_metropolis(target_pdf, step_size)\n",
    "    trace = []\n",
    "    for _ in range(n_steps):\n",
    "        sample, _ = next(sampler)\n",
    "        trace.append(sample)\n",
    "    \n",
    "    # Add trace plot\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            y=trace,\n",
    "            mode='lines',\n",
    "            line=dict(width=1),\n",
    "            name=f\"σ={step_size}\",\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=i, col=1\n",
    "    )\n",
    "    \n",
    "    # Add horizontal lines at the modes\n",
    "    fig.add_hline(y=-2, line_dash=\"dash\", line_color=\"red\", \n",
    "                  row=i, col=1, annotation_text=\"Mode 1\")\n",
    "    fig.add_hline(y=3, line_dash=\"dash\", line_color=\"red\", \n",
    "                  row=i, col=1, annotation_text=\"Mode 2\")\n",
    "\n",
    "fig.update_layout(height=900, width=800, showlegend=False)\n",
    "fig.update_xaxes(title_text=\"Iteration\", row=3, col=1)\n",
    "fig.update_yaxes(title_text=\"Value\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73382eea",
   "metadata": {},
   "source": [
    "### Modern MCMC: The No-U-Turn Sampler (NUTS)\n",
    "\n",
    "While Metropolis-Hastings and its variants are foundational, they can be inefficient for complex, high-dimensional models. Modern MCMC algorithms like Hamiltonian Monte Carlo (HMC) and its extension, the No-U-Turn Sampler (NUTS), use gradient information to make more intelligent proposals.\n",
    "\n",
    "#### Key Advantages of NUTS:\n",
    "\n",
    "1. **Automatic tuning**: NUTS adapts its parameters during warmup, eliminating the need for manual tuning\n",
    "2. **Efficient exploration**: Uses gradient information to propose distant points that are still likely to be accepted\n",
    "3. **Handles correlations**: Can efficiently sample from highly correlated posteriors\n",
    "4. **Fewer tuning parameters**: Works well \"out of the box\" for many models\n",
    "\n",
    "#### When NUTS Shines:\n",
    "\n",
    "- High-dimensional parameter spaces (dozens to thousands of parameters)\n",
    "- Complex posterior geometries with strong correlations\n",
    "- Models with continuous parameters\n",
    "\n",
    "#### Implementation in PyMC:\n",
    "\n",
    "PyMC uses NUTS as the default sampler for continuous variables. When you call `pm.sample()`, PyMC:\n",
    "\n",
    "1. Automatically differentiates your model to compute gradients\n",
    "2. Runs adaptation to tune the step size and mass matrix\n",
    "3. Generates samples using the tuned sampler\n",
    "4. Provides diagnostics to assess convergence\n",
    "\n",
    "The beauty of PyMC is that all this complexity is hidden behind a simple interface. However, understanding the basics helps us interpret diagnostics and troubleshoot when things go wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cf3433",
   "metadata": {},
   "source": [
    "## Part 6: Simple Linear Regression in PyMC\n",
    "\n",
    "Let's conclude with a complete example of Bayesian linear regression. This fundamental model serves as a building block for more complex analyses and demonstrates the complete Bayesian workflow.\n",
    "\n",
    "### The Fish Market Problem\n",
    "\n",
    "Imagine we work for an e-commerce company that sells fresh fish. We need to predict fish weights for inventory and shipping purposes, but weighing each fish individually is time-consuming and expensive. However, we can easily measure fish dimensions using automated cameras.\n",
    "\n",
    "Our goal is to build a model that:\n",
    "1. Predicts fish weight from easily measured dimensions\n",
    "2. Quantifies uncertainty in predictions\n",
    "3. Identifies which measurements are most informative\n",
    "4. Handles different fish species appropriately\n",
    "\n",
    "This is a perfect application for Bayesian regression because:\n",
    "- We care about prediction uncertainty (shipping costs have tiers)\n",
    "- We have prior knowledge about fish (weight should increase with size)\n",
    "- We want to understand which features matter most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc2fa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fish market data\n",
    "fish_data = pl.read_csv(\"../data/fish-market.csv\")\n",
    "\n",
    "print(f\"Dataset shape: {fish_data.shape}\")\n",
    "print(f\"\\nColumns: {fish_data.columns}\")\n",
    "print(f\"\\nSpecies: {fish_data['Species'].unique()}\")\n",
    "\n",
    "# Display first few rows\n",
    "fish_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d13d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore relationships between variables\n",
    "fig = px.scatter_matrix(\n",
    "    fish_data.to_pandas(),\n",
    "    dimensions=['Weight', 'Length1', 'Length2', 'Length3', 'Height', 'Width'],\n",
    "    color='Species',\n",
    "    title='Fish Measurements Scatter Matrix',\n",
    "    height=800\n",
    ")\n",
    "fig.update_traces(diagonal_visible=False, showupperhalf=False, marker=dict(size=3))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821dc013",
   "metadata": {},
   "source": [
    "### Modeling Approach\n",
    "\n",
    "The scatter matrix reveals several important patterns:\n",
    "1. Strong positive correlations between weight and all size measurements\n",
    "2. The three length measurements are highly correlated (multicollinearity)\n",
    "3. Different species show distinct patterns (suggesting species-specific models might be better)\n",
    "4. The relationship appears non-linear (weight increases with volume, not length)\n",
    "\n",
    "For this initial model, we'll:\n",
    "1. Use all available predictors despite multicollinearity (Bayesian methods handle this gracefully)\n",
    "2. Model all species together (keeping it simple)\n",
    "3. Use log transformation on weight to linearize relationships\n",
    "4. Standardize predictors for numerical stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c590172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "# Remove rows with zero weight (data quality issue)\n",
    "fish_clean = fish_data.filter(pl.col('Weight') > 0)\n",
    "\n",
    "# Log transform the target\n",
    "fish_clean = fish_clean.with_columns(\n",
    "    pl.col('Weight').log().alias('LogWeight')\n",
    ")\n",
    "\n",
    "# Select and standardize predictors\n",
    "predictor_vars = ['Length1', 'Length2', 'Length3', 'Height', 'Width']\n",
    "X_fish = fish_clean.select(predictor_vars).to_numpy()\n",
    "\n",
    "# Standardize\n",
    "X_mean = X_fish.mean(axis=0)\n",
    "X_std = X_fish.std(axis=0)\n",
    "X_fish_std = (X_fish - X_mean) / X_std\n",
    "\n",
    "# Target variable\n",
    "y_fish = fish_clean['LogWeight'].to_numpy()\n",
    "\n",
    "print(f\"Final dataset: {X_fish_std.shape[0]} fish, {X_fish_std.shape[1]} predictors\")\n",
    "print(f\"Target range: {y_fish.min():.2f} to {y_fish.max():.2f} (log scale)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874f2afd",
   "metadata": {},
   "source": [
    "### Building the Bayesian Linear Regression Model\n",
    "\n",
    "Our model follows the standard Bayesian regression framework:\n",
    "\n",
    "$$\\log(\\text{Weight}_i) = \\alpha + \\sum_{j=1}^{5} \\beta_j X_{ij} + \\epsilon_i$$\n",
    "\n",
    "where:\n",
    "- $\\alpha$ is the intercept\n",
    "- $\\beta_j$ are coefficients for each predictor\n",
    "- $\\epsilon_i \\sim \\text{Normal}(0, \\sigma)$ is the error term\n",
    "\n",
    "We'll use weakly informative priors that encode our basic knowledge about the problem while letting the data dominate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a0761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the regression model\n",
    "coords_fish = {\"predictor\": predictor_vars}\n",
    "\n",
    "with pm.Model(coords=coords_fish) as fish_model:\n",
    "    \n",
    "    # Intercept - centered on mean log weight\n",
    "    alpha = pm.Normal('alpha', mu=np.mean(y_fish), sigma=2)\n",
    "    \n",
    "    # Regression coefficients\n",
    "    # Prior: We expect standardized coefficients to be moderate in size\n",
    "    beta = pm.Normal('beta', mu=0, sigma=1, dims=\"predictor\")\n",
    "    \n",
    "    # Error standard deviation\n",
    "    # HalfStudentT provides robustness to outliers\n",
    "    sigma = pm.HalfStudentT('sigma', nu=4, sigma=0.5)\n",
    "    \n",
    "    # Expected value\n",
    "    mu = pm.Deterministic('mu', alpha + X_fish_std @ beta)\n",
    "    \n",
    "    # Likelihood\n",
    "    pm.Normal('log_weight', mu=mu, sigma=sigma, observed=y_fish)\n",
    "    \n",
    "# Visualize model\n",
    "pm.model_to_graphviz(fish_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e3cc21",
   "metadata": {},
   "source": [
    "### Prior Predictive Checks\n",
    "\n",
    "Before fitting the model, let's check that our priors generate reasonable predictions. This is an important step that can catch specification errors early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d83596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from prior predictive distribution\n",
    "with fish_model:\n",
    "    prior_pred = pm.sample_prior_predictive(samples=500, random_seed=42)\n",
    "\n",
    "# Convert prior predictions back to original scale\n",
    "prior_weights = np.exp(prior_pred.prior_predictive['log_weight'].values.flatten())\n",
    "\n",
    "# Plot prior predictive distribution\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=prior_weights[prior_weights < 10000], # Remove extreme values for visualization\n",
    "                          name='Prior Predictions',\n",
    "                          nbinsx=50,\n",
    "                          histnormalization='probability'))\n",
    "fig.add_trace(go.Histogram(x=fish_clean['Weight'], \n",
    "                          name='Actual Data',\n",
    "                          nbinsx=50,\n",
    "                          histnormalization='probability',\n",
    "                          opacity=0.7))\n",
    "fig.update_layout(title='Prior Predictive Check: Fish Weights',\n",
    "                 xaxis_title='Weight (g)',\n",
    "                 yaxis_title='Probability')\n",
    "fig.show()\n",
    "\n",
    "print(f\"Prior predictions range: {np.percentile(prior_weights, 5):.0f} to {np.percentile(prior_weights, 95):.0f} grams\")\n",
    "print(f\"Actual data range: {fish_clean['Weight'].min():.0f} to {fish_clean['Weight'].max():.0f} grams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4875bc",
   "metadata": {},
   "source": [
    "### Fitting the Model\n",
    "\n",
    "With reasonable priors confirmed, we can now fit the model using MCMC. PyMC will use the NUTS sampler, which is particularly effective for regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9396388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "with fish_model:\n",
    "    # Sample from posterior\n",
    "    fish_trace = pm.sample(2000, tune=2000, random_seed=42)\n",
    "    \n",
    "    # Sample posterior predictive for model checking\n",
    "    post_pred = pm.sample_posterior_predictive(fish_trace, random_seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa0638e",
   "metadata": {},
   "source": [
    "### Analyzing Results\n",
    "\n",
    "Now we examine the posterior distributions to understand what the model has learned from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73893d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"Regression Coefficients:\")\n",
    "print(az.summary(fish_trace, var_names=['alpha', 'beta', 'sigma'], \n",
    "                filter_vars=\"like\"))\n",
    "\n",
    "# Visualize coefficients\n",
    "az.plot_forest(fish_trace, \n",
    "               var_names=['beta'],\n",
    "               combined=True,\n",
    "               hdi_prob=0.95,\n",
    "               figsize=(10, 6))\n",
    "plt.title('Posterior Distributions of Standardized Coefficients')\n",
    "plt.xlabel('Standardized Coefficient Value')\n",
    "plt.show()\n",
    "\n",
    "# Interpret coefficients\n",
    "beta_means = fish_trace.posterior['beta'].mean(dim=['chain', 'draw'])\n",
    "for i, var in enumerate(predictor_vars):\n",
    "    print(f\"{var}: {beta_means[i].item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2402f13",
   "metadata": {},
   "source": [
    "### Model Checking\n",
    "\n",
    "A crucial part of Bayesian workflow is checking whether our model adequately captures the data generating process. We'll use posterior predictive checks to compare model predictions with actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c2633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior predictive check\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Predicted vs Actual\n",
    "y_pred = post_pred.posterior_predictive['log_weight'].mean(dim=['chain', 'draw'])\n",
    "axes[0].scatter(y_fish, y_pred, alpha=0.5)\n",
    "axes[0].plot([y_fish.min(), y_fish.max()], [y_fish.min(), y_fish.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual Log Weight')\n",
    "axes[0].set_ylabel('Predicted Log Weight')\n",
    "axes[0].set_title('Predicted vs Actual Values')\n",
    "\n",
    "# Plot 2: Residuals\n",
    "residuals = y_fish - y_pred\n",
    "axes[1].scatter(y_pred, residuals, alpha=0.5)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[1].set_xlabel('Predicted Log Weight')\n",
    "axes[1].set_ylabel('Residuals')\n",
    "axes[1].set_title('Residual Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate R-squared (Bayesian R²)\n",
    "ss_res = np.sum(residuals**2)\n",
    "ss_tot = np.sum((y_fish - y_fish.mean())**2)\n",
    "r_squared = 1 - (ss_res / ss_tot)\n",
    "print(f\"\\nBayesian R²: {r_squared:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1ffc9d",
   "metadata": {},
   "source": [
    "### Making Predictions with Uncertainty\n",
    "\n",
    "One of the key advantages of Bayesian regression is that we get full posterior distributions for predictions, not just point estimates. Let's predict the weight of a new fish and quantify our uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef5597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict for a new fish\n",
    "# Let's say we have a fish with these measurements:\n",
    "new_fish = np.array([[\n",
    "    25.0,  # Length1\n",
    "    27.0,  # Length2  \n",
    "    30.0,  # Length3\n",
    "    11.5,  # Height\n",
    "    6.5    # Width\n",
    "]])\n",
    "\n",
    "# Standardize using training statistics\n",
    "new_fish_std = (new_fish - X_mean) / X_std\n",
    "\n",
    "# Generate predictions\n",
    "with fish_model:\n",
    "    # Update predictors for prediction\n",
    "    mu_pred = alpha + new_fish_std @ beta\n",
    "    \n",
    "    # Sample predictions\n",
    "    predictions = pm.sample_posterior_predictive(\n",
    "        fish_trace, \n",
    "        var_names=['alpha', 'beta'],\n",
    "        random_seed=42\n",
    "    )\n",
    "\n",
    "# Get predicted log weights\n",
    "log_weight_pred = predictions.posterior['alpha'] + (new_fish_std @ predictions.posterior['beta'].T).T\n",
    "\n",
    "# Convert to weight scale\n",
    "pred_weights = np.exp(log_weight_pred.values.flatten())\n",
    "\n",
    "# Summarize predictions\n",
    "mean_weight = pred_weights.mean()\n",
    "hdi = az.hdi(pred_weights, hdi_prob=0.95)\n",
    "\n",
    "print(f\"Predicted weight: {mean_weight:.0f} grams\")\n",
    "print(f\"95% HDI: [{hdi[0]:.0f}, {hdi[1]:.0f}] grams\")\n",
    "\n",
    "# Visualize prediction distribution\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=pred_weights, nbinsx=50, name='Predicted Weight'))\n",
    "fig.add_vline(x=mean_weight, line_dash=\"dash\", line_color=\"red\", \n",
    "              annotation_text=f\"Mean: {mean_weight:.0f}g\")\n",
    "fig.update_layout(title='Predicted Weight Distribution for New Fish',\n",
    "                 xaxis_title='Weight (g)',\n",
    "                 yaxis_title='Count')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e708d5e",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "In this session, we've covered the essential components of building Bayesian models with PyMC:\n",
    "\n",
    "### PyMC Fundamentals\n",
    "- **Model Context**: All PyMC models exist within a context that tracks variable relationships\n",
    "- **Distributions**: PyMC provides a rich library of probability distributions for priors and likelihoods\n",
    "- **Random Variables**: Parameters and data are represented as random variables with associated distributions\n",
    "- **Automatic Differentiation**: PyMC handles gradient calculations automatically\n",
    "\n",
    "### MCMC Concepts\n",
    "- **The Challenge**: Bayesian inference requires integration over high-dimensional spaces\n",
    "- **The Solution**: MCMC methods draw samples from the posterior distribution\n",
    "- **Metropolis-Hastings**: The foundational MCMC algorithm that inspired modern methods\n",
    "- **Tuning Matters**: Step size and other hyperparameters critically affect performance\n",
    "- **Modern Algorithms**: NUTS (used by PyMC) efficiently explores complex posteriors\n",
    "- **Convergence**: Multiple chains help assess whether sampling has converged\n",
    "\n",
    "### Practical Workflow\n",
    "1. **Model Specification**: Encode assumptions using priors and likelihoods\n",
    "2. **Prior Predictive Checks**: Verify that priors generate reasonable data\n",
    "3. **Sampling**: Use `pm.sample()` to draw from the posterior\n",
    "4. **Diagnostics**: Check convergence and sampling quality\n",
    "5. **Posterior Analysis**: Examine parameter estimates and uncertainties\n",
    "6. **Posterior Predictive Checks**: Verify that the model captures data patterns\n",
    "7. **Predictions**: Generate predictions with full uncertainty quantification\n",
    "\n",
    "### Key Advantages of Bayesian Modeling\n",
    "- **Uncertainty Quantification**: Get distributions, not just point estimates\n",
    "- **Prior Information**: Incorporate domain knowledge systematically\n",
    "- **Hierarchical Models**: Naturally handle grouped data (next session!)\n",
    "- **Model Comparison**: Principled ways to compare models\n",
    "- **Missing Data**: Automatic handling of missing values\n",
    "\n",
    "### Next Steps\n",
    "In the next session, we'll extend these concepts to hierarchical models, which allow us to model data with natural grouping structures. We'll see how Bayesian methods excel at sharing information across groups while respecting their differences.\n",
    "\n",
    "### Exercises\n",
    "1. Try different prior distributions and observe their effect on the posterior\n",
    "2. Fit separate models for different fish species and compare results\n",
    "3. Add interaction terms between predictors\n",
    "4. Implement a robust regression using Student-t likelihood instead of Normal\n",
    "5. Use cross-validation to assess predictive performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a711ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
