{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c22206b8",
   "metadata": {},
   "source": [
    "# Building Models with PyMC and MCMC Fundamentals\n",
    "\n",
    "In the previous session, we explored the foundations of Bayesian inference, including Bayes' theorem, conjugate priors, and the mechanics of Bayesian updating for simple models. \n",
    "\n",
    "This session introduces PyMC, a powerful probabilistic programming framework that enables us to build and analyze complex Bayesian models. We'll learn how to specify models using PyMC's intuitive API, understand the theoretical foundations of Markov Chain Monte Carlo (MCMC) methods that make modern Bayesian computation possible, and work through practical examples that demonstrate the complete modeling workflow.\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "\n",
    "1. **Build probabilistic models in PyMC**: Understand PyMC's core components including distributions, random variables, and the model context\n",
    "2. **Specify model structure**: Learn how to encode assumptions about data generating processes using priors and likelihoods\n",
    "3. **Understand MCMC fundamentals**: Grasp why we need MCMC, how it works conceptually, and what makes it powerful for Bayesian inference\n",
    "4. **Implement complete Bayesian analyses**: Build, fit, and interpret results from real-world models including linear regression\n",
    "\n",
    "## Why PyMC?\n",
    "\n",
    "PyMC provides several key advantages for Bayesian modeling:\n",
    "\n",
    "- **Expressive model specification**: Write models that look like their mathematical notation\n",
    "- **Automatic differentiation**: No need to derive gradients by hand\n",
    "- **State-of-the-art samplers**: Access to efficient MCMC algorithms like NUTS (No-U-Turn Sampler)\n",
    "- **Comprehensive diagnostics**: Built-in tools for assessing convergence and model quality\n",
    "- **Integration with the PyData ecosystem**: Works seamlessly with NumPy, Pandas, and visualization libraries\n",
    "\n",
    "Let's begin by setting up our environment and exploring PyMC's fundamental concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2599104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import polars as pl\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "\n",
    "# Visual style\n",
    "az.style.use('arviz-doc')\n",
    "pio.templates.default = 'plotly_white'\n",
    "\n",
    "# Plotly defaults\n",
    "px.defaults.template = 'plotly_white'\n",
    "px.defaults.width = 450\n",
    "px.defaults.height = 250\n",
    "_base = pio.templates['plotly_white']\n",
    "_tmpl = go.layout.Template(_base)\n",
    "_tmpl.layout.hovermode = 'x unified'\n",
    "pio.templates['hoverx'] = _tmpl\n",
    "pio.templates.default = 'plotly_white+hoverx'\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 20090425\n",
    "RNG = np.random.default_rng(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad612c5",
   "metadata": {},
   "source": [
    "## Building Models in PyMC\n",
    "\n",
    "Probabilistic programming represents a paradigm shift in how we approach statistical modeling. Instead of deriving update equations or coding samplers by hand, we declare the structure of our model and let the framework handle the computational details. PyMC exemplifies this approach by providing an intuitive interface that closely mirrors mathematical notation while leveraging sophisticated algorithms under the hood.\n",
    "\n",
    "### The Philosophy of Probabilistic Programming\n",
    "\n",
    "Probabilistic programming fundamentally changes how we think about statistical modeling. Rather than starting with computational constraints and working toward a simplified model, we begin with our actual beliefs about the world and let the appropriate algorithm handle the computational challenges. This approach allows us to build models that truly reflect our understanding of the problem domain, incorporating complex dependencies, hierarchical structures, and realistic uncertainty without being limited by what we can solve analytically or implement by hand.\n",
    "\n",
    "We start by specifying what we know:\n",
    "- **Prior knowledge** about parameters before seeing data\n",
    "- **The data generating process** that connects parameters to observations\n",
    "- **The observed data** itself\n",
    "\n",
    "From this specification, a probabilistic programming framework like PyMC automatically constructs the computational graph needed for inference, applies appropriate transformations for constrained parameters, and selects suitable sampling algorithms.\n",
    "\n",
    "### PyMC's Core Abstractions\n",
    "\n",
    "PyMC organizes probabilistic models around several key concepts:\n",
    "\n",
    "1. **The Model Context**: Every PyMC model exists within a context that tracks relationships between variables. This context manager pattern ensures that all model components are properly registered and connected.\n",
    "\n",
    "2. **Random Variables**: These represent quantities with uncertainty. In Bayesian modeling, parameters are random variables with prior distributions, and data are random variables with likelihood distributions.\n",
    "\n",
    "3. **Distributions**: PyMC provides a comprehensive library of probability distributions. Each distribution can create random variables when used within a model context.\n",
    "\n",
    "4. **Deterministic Transformations**: Often we need to transform parameters or compute derived quantities. PyMC tracks these deterministic relationships to maintain the full model structure.\n",
    "\n",
    "5. **Observed Data**: By marking random variables as observed, we condition the model on actual data, transforming prior distributions into posterior distributions.\n",
    "\n",
    "Let's explore these concepts through hands-on examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7465621c",
   "metadata": {},
   "source": [
    "## Model Contexts and Random Variables\n",
    "\n",
    "As we have seen, the canonical way to specify PyMC models is using a `Model` context manager. Generally speaking, a context manager is a Python idiom that define what happens when entering and exiting a with statement. They provide a clean, reliable way to set up and tear down resources,\n",
    "\n",
    "As an analogy, `Model` is a tape machine that records what is being added to the model; it keeps track the random variables (observed or unobserved) and other model components. The model context then computes some simple model properties, builds a **bijection** mapping that transforms between Python dictionaries and numpy/Pytensor ndarrays. \n",
    "\n",
    "More importantly, a `Model` contains methods to compile Pytensor functions that take Random Variables--that are also\n",
    "initialised within the same model--as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfce9f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    z = pm.Normal('z', mu=0., sigma=5.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93913c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.named_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4621811",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile_logp()({'z': 2.5})  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab789ca",
   "metadata": {},
   "source": [
    "## The PyMC API\n",
    "\n",
    "Bayesian inference begins with a probability model that relates unknown parameters to observed data. PyMC provides high-level building blocks for constructing these models:\n",
    "\n",
    "### Core Components\n",
    "\n",
    "- **Stochastic Random Variables**: Variables whose values are not completely determined by their parents. These represent uncertainty in the model parameters or data generating process.\n",
    "    - **Prior distributions** for model parameters\n",
    "    - **Likelihood distributions** for observed data\n",
    "\n",
    "    $$x \\sim \\text{Normal}(\\mu, \\sigma)$$\n",
    "\n",
    "- **Deterministic Variables**: Variables whose values are completely determined by their parents through a mathematical operation; no additional randomness is added by the operation. These represent transformations or combinations of other variables.\n",
    "\n",
    "    $$y = \\mu + \\beta x$$\n",
    "\n",
    "- **Observed Variables**: Variables whose values are observed and used to update the posterior distribution.\n",
    "\n",
    "    $$y_{obs} \\sim \\text{Normal}(\\mu, \\sigma)$$\n",
    "\n",
    "    While they look like stochastic variables, they are not. They are observed and used to update the posterior distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d2812f",
   "metadata": {},
   "source": [
    "### The Distribution Class\n",
    "\n",
    "A stochastic variable is represented in PyMC by a `Distribution` class. This structure adds functionality to Pytensor's `pytensor.tensor.random.op.RandomVariable` class, mainly by registering it with an associated PyMC `Model` -- so `Distribution` objects are only usable inside of a `Model` context.\n",
    "\n",
    "`Distribution` subclasses (i.e. implementations of specific statistical distributions) will accept several arguments when constructed. Some of the most important are:\n",
    "\n",
    "`name`\n",
    ": Name for the new model variable. This argument is **required**, and is used as a label and index value for the variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0627bb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model():\n",
    "    x = pm.Normal(name=\"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871e8fa1",
   "metadata": {},
   "source": [
    "`shape`\n",
    ": The variable's shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3c9750",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model():\n",
    "    x_matrix = pm.Normal(\"x_matrix\", shape=(3, 3))\n",
    "\n",
    "pm.draw(x_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f24616",
   "metadata": {},
   "source": [
    "`dims`\n",
    ": A tuple of dimension names known to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3446b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_names = [\"Vancouver\", \"Calgary\", \"Toronto\", \"Montreal\", \"Halifax\"]\n",
    "\n",
    "with pm.Model(coords={'city': city_names}) as model:\n",
    "    x_city = pm.Normal(\"x_city\", dims=\"city\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3623a0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    samples = pm.sample_prior_predictive(1000)\n",
    "\n",
    "az.plot_forest(samples.prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e070b96b",
   "metadata": {},
   "source": [
    "`initval`\n",
    ": Numeric or symbolic untransformed initial value of matching shape, or one of the following initial value strategies: \"moment\", \"prior\". Depending on the sampler's settings, a random jitter may be added to numeric, symbolic or moment-based initial values in the transformed space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ccdbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    x = pm.Normal('x', initval=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b16f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.rvs_to_initial_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb271cbe",
   "metadata": {},
   "source": [
    "PyMC includes most of the **probability density functions** (for continuous variables) and **probability mass functions** (for discrete variables) used in statistical modeling. These distributions are divided into five distinct categories:\n",
    "\n",
    "* Univariate continuous\n",
    "* Univariate discrete\n",
    "* Multivariate\n",
    "* Mixture\n",
    "* Timeseries\n",
    "\n",
    "Probability distributions are all subclasses of `Distribution`, which in turn has two major subclasses: `Discrete` and `Continuous`. In terms of data types, a `Continuous` random variable is given whichever floating point type is defined by `pytensor.config.floatX`, while `Discrete` variables are given `int16` types when `pytensor.config.floatX` is `float32`, and `int64` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d55cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefe8a57",
   "metadata": {},
   "source": [
    "Multivariate and Timeseries random variables are vector-valued, rather than scalar (though `Continuous` and `Discrete` variables may have non-scalar values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ce9892",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ba9f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_city.shape.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2b0ef0",
   "metadata": {},
   "source": [
    "All of the `Distribution` subclasses included in PyMC will have two key methods, `rng_fn()` and `logp()`, which are used to generate random values and compute the log-probability of a value, respectively.\n",
    "\n",
    "```python\n",
    "class SomeDistribution(Continuous):\n",
    "    def __init__(...):\n",
    "        ...\n",
    "    @classmethod\n",
    "    def rng_fn(cls, rng, size=None, ...):\n",
    "        ...\n",
    "        return random_samples\n",
    "\n",
    "    def logp(value, *params):\n",
    "        ...\n",
    "        return log_prob\n",
    "```\n",
    "\n",
    "PyMC expects the `logp()` method to return a log-probability evaluated at the passed `value` argument. This method is used internally by all of the inference methods to calculate the model log-probability that is used for fitting models.\n",
    "\n",
    "Users do not call this method directly; it is used internally by PyMC to implement the user-facing `logp()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b2b50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.logp(x, value=0).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c331caca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.logp(x_city, value=np.random.randn(5)).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c1e713",
   "metadata": {},
   "source": [
    "The `rng_fn()` method is used to simulate values from the variable, and is used internally for predictive sampling.\n",
    "\n",
    "Users call the `pm.draw()` function to simulate values from a random variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8290ddd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.draw(x, draws=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5445a88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.draw(x_city, draws=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f664040",
   "metadata": {},
   "source": [
    "Distributions will optionally have `cdf` and `icdf` methods, representing the cumulative distribution function and inverse cumulative distribution functions, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf5f86d",
   "metadata": {},
   "source": [
    "Sometimes we wish to use a particular statistical distribution, without using it as a variable in a model; for example, to generate random numbers from the distribution. For this purpose, `Distribution` objects have a method `dist` that returns a **stateless** probability distribution of that type; that is, without being wrapped in a PyMC random variable object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94013181",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pm.Exponential.dist(10)\n",
    "samples = pm.draw(x, draws=1000)\n",
    "\n",
    "px.histogram(\n",
    "    samples, title=\"Exponential Distribution Samples\"\n",
    ").update_layout(\n",
    "    xaxis_title=\"Value\", yaxis_title=\"Count\", showlegend=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836f9d8c",
   "metadata": {},
   "source": [
    "### Custom Distributions\n",
    "\n",
    "If you have a well-behaved density function, we can use it in a model to build a model log-likelihood function. Almost any Pytensor function can be turned into a\n",
    "distribution using the `CustomDist` function. For exmaple, a **uniformly-distributed** stochastic variable could be created manually from a function that computes its log-probability as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8a0a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_logp(value, lower, upper):\n",
    "    return pm.math.switch(\n",
    "        (value > upper) | (value < lower), \n",
    "        -np.inf, \n",
    "        -pm.math.log(upper - lower + 1)\n",
    "    )\n",
    "\n",
    "with pm.Model():\n",
    "\n",
    "    u = pm.CustomDist('u', 0, 10, logp=uniform_logp, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea16142",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.logp(u, 3.5).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6116b3f8",
   "metadata": {},
   "source": [
    "Passing values outside the support of the distribution to `logp()` will return `-inf`, since the value has no probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be40d80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.logp(u, -4).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31450c4",
   "metadata": {},
   "source": [
    "To emphasize, the Python function passed to `CustomDist` should compute the *log*-density or *log*-probability of the variable. That is why the return value in the example above is `-log(upper-lower+1)` rather than `1/(upper-lower+1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9282d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.logp(u, 1).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b9c50b",
   "metadata": {},
   "source": [
    "Passing values outside the support of the distribution to `logp()` will return `-inf`, since the value has no probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4da205",
   "metadata": {},
   "source": [
    "## Building Models in PyMC\n",
    "\n",
    "Now that we understand the basic building blocks of PyMC models, let's see how to combine them to build a complete model.\n",
    "\n",
    "### Fish Weight Prediction\n",
    "\n",
    "Let's imagine we are working in the data science team of an e-commerce company. In particular, we sell really good and fresh fish to our clients (mainly fancy restaurants). \n",
    "\n",
    "When we ship our products, there is a very important piece of information we need: the weight of the fish. This is important for two reasons: \n",
    "\n",
    "1. Because we _bill_ our clients according to weight. \n",
    "\n",
    "2. Because the company that delivers the fish to our clients has different price tiers for weights, and those tiers can get _really_ expensive. So we want to know the probability of an item being above that line. In other words, estimating uncertainty is important here!\n",
    "\n",
    "![](images/weighingfish.jpg)\n",
    "\n",
    "\n",
    "The problem we face is that we purchase our fish in bulk. This means we only know the total weight of our entire order, but we don't have the weights of the individual fish. You might think the obvious solution is simply to weigh each fish one by one.\n",
    "\n",
    "However, this approach has significant drawbacks. Manually weighing each fish is costly, requires a lot of time, and demands substantial labor. This process is inefficient and impractical for our needs.\n",
    "\n",
    "Given these challenges, we need to explore alternative solutions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adf58df",
   "metadata": {},
   "source": [
    "### A solution\n",
    "\n",
    "While researching the problem, we discovered that our wholesale supplier has detailed information on the size of each individual fish, including their length, height, and width. Since it is infeasible to weigh individual fish, the supplier uses a **camera** to record the size of each fish. \n",
    "\n",
    "However, the company used to try to weigh each fish manually until costs became prohibitive. As a result, we have a valuable **training dataset** consisting of different types of fish with their accurately -measured weights.\n",
    "\n",
    "![](images/fishvideo.png)\n",
    "\n",
    "### Linear regression analysis\n",
    "\n",
    "Let's import the data and take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2210f28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_market = pl.read_csv(\"../data/fish-market.csv\")\n",
    "fish_market.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a442298",
   "metadata": {},
   "source": [
    "We have collected 159 measurements, and all columns in our dataset have the appropriate data types.\n",
    "\n",
    "For each observation, the dataset includes the following information: the species of the fish, its weight, height, and width, as well as three distinct length measurements. You might be wondering why we have three different measurements for the fish's length. Let's delve into some summary statistics to better understand the data and its significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f3e358",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_market.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87bf86c",
   "metadata": {},
   "source": [
    "Things to note:\n",
    "\n",
    "- Though there are no missing data, there are some zero-weight fish! -- either the fish was below the minimum weight for the scale, or there was a mistake during data collection. \n",
    "- The standard deviation of the columns are very high, especially for weights.\n",
    "- There are three columns for length, which is interesting. We will explore this further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e959ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data = fish_market.drop(\"Species\")\n",
    "corr_matrix = numeric_data.corr().to_numpy().round(2)\n",
    "\n",
    "fig = px.imshow(\n",
    "    corr_matrix,\n",
    "    x=numeric_data.columns,\n",
    "    y=numeric_data.columns,\n",
    "    zmin=-1,\n",
    "    zmax=1,\n",
    "    color_continuous_scale='RdBu_r',  \n",
    "    aspect='auto'\n",
    ")\n",
    "\n",
    "for i in range(len(numeric_data.columns)):\n",
    "    for j in range(len(numeric_data.columns)):\n",
    "        fig.add_annotation(\n",
    "            x=i,\n",
    "            y=j,\n",
    "            text=str(corr_matrix[j, i]),\n",
    "            showarrow=False,\n",
    "            font=dict(size=16, color='black')\n",
    "        )\n",
    "\n",
    "fig.update_layout(\n",
    "    coloraxis_colorbar_title='Correlation',\n",
    "    width=800,\n",
    "    height=800\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9af2e3",
   "metadata": {},
   "source": [
    "The three length measurements are highly correlated with each other. This means they essentially carry the same information. Without additional details to distinguish among them, we should arbitrarily choose one measurement and discard the other two. Keeping all three would be redundant and unnecessary since they do not provide unique information.\n",
    "\n",
    "There is nothing inherently Bayesian about this step. The concept of *multicollinearity* is a fundamental concern in both Bayesian and frequentist statistics. In essence, if you include multiple variables that convey similar information in your regression model, you will end up with very unstable parameter estimates. This redundancy does not improve your model's predictive power and can, in fact, lead to misleading results. Thus, it is crucial to identify and address multicollinearity to maintain the robustness and reliability of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a1358e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_market = fish_market.drop([\"Length2\", \"Length3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f4e9eb",
   "metadata": {},
   "source": [
    "## Visual data exploration\n",
    "\n",
    "It's always a good idea to plot your data! Plotly's `scatter_matrix` function is a great way to visualize the relationships between variables in your dataset. This function creates a matrix of scatterplots, with each variable plotted against every other variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f45f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_matrix(\n",
    "    fish_market,\n",
    "    dimensions=[\"Length1\", \"Height\", \"Width\", \"Weight\"],\n",
    "    color=\"Species\",\n",
    "    opacity=0.7,\n",
    "    height=1000,\n",
    "    width=1000\n",
    ")\n",
    "\n",
    "fig.update_traces(diagonal_visible=True, showupperhalf=True, showlowerhalf=True)\n",
    "fig.update_layout(\n",
    "    dragmode='select',\n",
    "    hovermode='closest'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5742ba6d",
   "metadata": {},
   "source": [
    "Thus, it is clear that any model we build must account for the differences in the relationships between variables across species. This is where Bayesian linear regression comes in handy. By incorporating **domain knowledge** about the relationships between variables and the differences across species, we can build a more robust and reliable model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88de34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [\"Length1\", \"Height\", \"Width\", \"Weight\"]\n",
    "fig = make_subplots(rows=2, cols=2, subplot_titles=variables)\n",
    "\n",
    "for i, var in enumerate(variables):\n",
    "    row = i // 2 + 1\n",
    "    col = i % 2 + 1\n",
    "    \n",
    "    for species in fish_market[\"Species\"].unique():\n",
    "        species_data = fish_market.filter(pl.col(\"Species\") == species).to_pandas()\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=species_data[var],\n",
    "                name=species,\n",
    "                boxpoints='all',\n",
    "                jitter=0.5,\n",
    "                pointpos=0,\n",
    "                marker=dict(opacity=0.5),\n",
    "                line=dict(width=1),\n",
    "                showlegend=(i == 0)\n",
    "            ),\n",
    "            row=row,\n",
    "            col=col\n",
    "        )\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    width=800,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2df8ce",
   "metadata": {},
   "source": [
    "The most diverse species are Bream, Whitefish, Perch, and Pike. This diversity likely makes them more versatile for sale and cooking because they come in a wide range of sizes, including different weights, widths, and heights. This variety allows for more options in preparation methods and recipes, catering to various culinary needs.\n",
    "\n",
    "On the other hand, the Smelt is a very small fish that is typically used in specialized recipes. Its smaller size and specific preparation methods make it less versatile than the more diverse species like Bream, Whitefish, Perch, and Pike. A quick internet search will show you that they are usually fried and served as appetizers, at least in Europe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1a60bc",
   "metadata": {},
   "source": [
    "## A non-Bayesian linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfbce37",
   "metadata": {},
   "source": [
    "Now that we have a clearer understanding of the data we're working with, let's move on to developing a predictive model. Our specific task is to **predict the weight of a fish based on its width, height, and length**. While we've chosen these particular variables for our analysis, it's important to note that different combinations of independent and dependent variables could also be used, depending on the specific requirements of the study.\n",
    "\n",
    "The most promising approach for our task is to develop a **physical model**. This involves leveraging the inherent relationships between height, width, and weight, which are governed by physical proportions that impose natural lower and upper bounds on these variables. In a professional context, such a model would likely yield the most accurate and reliable predictions due to its basis in the physical characteristics of fish.\n",
    "\n",
    "However, creating a detailed physical model can be quite complex. Therefore, for our initial attempt, we can use a simple **ordinary least squares (OLS)** regression to establish a relationship between the dependent variable (weight) and the independent variables (width, height, and length).\n",
    "\n",
    "From our data exploration, we observed that weight is not linearly related to the other variables. This non-linear relationship suggests that a direct application of linear regression may not be effective. To address this issue, we often need to apply some form of data transformation to better fit the model to the data.\n",
    "\n",
    "In this scenario, a **logarithmic transformation** of the data appears to be a suitable choice. This transformation can help counteract the exponential increase in weight as the fish's width, height, and length increase. By applying a log-transform, we can linearize the relationship between these variables, making it more appropriate for linear regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e4a491",
   "metadata": {},
   "source": [
    "### Taking the log of all covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156ba059",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_market = fish_market.with_columns([\n",
    "    pl.col(\"Width\").log().alias(\"log_width\"),\n",
    "    pl.col(\"Height\").log().alias(\"log_height\"), \n",
    "    pl.col(\"Length1\").log().alias(\"log_length\"),\n",
    "    pl.col(\"Weight\").log().alias(\"log_weight\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7298fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows to see the transformed data\n",
    "fish_market.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ed4008",
   "metadata": {},
   "source": [
    "### Simple OLS regression\n",
    "\n",
    "An easy way to perform OLS regression is via the `seaborn` graphics library. The `lmplot` function creates a scatterplot of the data and fits a regression line to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8b69ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fish_complete = fish_market.filter(pl.col(\"Weight\") != 0)\n",
    "\n",
    "sns.lmplot(\n",
    "    data=fish_complete,\n",
    "    x=\"log_height\",\n",
    "    y=\"log_weight\",\n",
    "    hue=\"Species\",\n",
    "    col=\"Species\",\n",
    "    height=3,\n",
    "    col_wrap=4,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bef800",
   "metadata": {},
   "source": [
    "The output here is purely visual, but in log space, our input variables seem linearly related to weight, so there is good reason to believe that a linear model is appropriate here.\n",
    "\n",
    "Let's go ahead and fit a linear model to the data using PyMC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e25e75a",
   "metadata": {},
   "source": [
    "## Multiple linear regression\n",
    "\n",
    "We can specify a linear model to predict the weight of a fish based on its width, height, and length.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{priors}\\\\\n",
    "\\mu[s] &\\sim \\mathrm{Normal}(0, 1)\\\\\n",
    "\\beta[s, k] &\\sim \\mathrm{Normal}(0, 0.5)\\\\\n",
    "\\sigma &\\sim \\mathrm{HalfNormal}(1)\\\\\n",
    "\\text{linear model}\\\\\n",
    "\\mu_i &= \\mu[s_i]\\\\\n",
    "        & \\quad + \\beta[s_i, 0] \\times \\log(\\text{width}_i)\\\\\n",
    "        & \\quad + \\beta[s_i, 1] \\times \\log(\\text{height}_i)\\\\\n",
    "        & \\quad + \\beta[s_i, 2] \\times \\log(\\text{length}_i)\\\\\n",
    "\\text{likelihood}\\\\\n",
    "\\log(\\text{weight}_i) &\\sim \\mathrm{Normal}(\\mu_i, \\sigma)\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "where $s_i$ is the species index corresponding to fish _i_:\n",
    "\n",
    "\n",
    "$$\n",
    "s_i \\in \\{ 0, 1, \\ldots, {S-1} \\}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb55a02",
   "metadata": {},
   "source": [
    "In Wilkinson notation, the model can be written as:\n",
    "\n",
    "\n",
    "`log(weight) ~ 0 + species + log(width):species + log(height):species + log(length):species`. \n",
    "\n",
    "\n",
    "The `0 + species` component means that we just have $S$ intercept terms, one for each species, with no global intercept. \n",
    "\n",
    "The remaining terms (e.g. `log(width):species`) represent an interaction between the predictor and the `species` category. So there will be one coefficient for the $\\log(width)$ slope (in this case) for each species.\n",
    "\n",
    "So, each species has its own intercept and slopes for width, height, and length. This is an **unpooled model** because we are essentially fitting a separate regression for each species!\n",
    "\n",
    "In order to make this work, we need to encode the species as a categorical variable. We can do this using the `Categorical` type in `polars`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2468724",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.Series(fish_complete[\"Species\"]).cast(pl.Categorical).to_physical().sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00854004",
   "metadata": {},
   "source": [
    "### Define dimensions & coordinates\n",
    "\n",
    "Having encoded species as a categorical column, we also extract the unique species values, to be used as coordinates (labels) for the parameters in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1d734f",
   "metadata": {},
   "source": [
    "### Prior Specification\n",
    "\n",
    "The first step in specifying a PyMC model is defining the prior distributions for each unknown parameter.\n",
    "\n",
    "- what are the unknown parameters?\n",
    "- which distributions should we use to characterize our beliefs about their values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c50d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_idx = pl.Series(fish_complete[\"Species\"]).cast(pl.Categorical).to_physical().to_numpy()\n",
    "species = pl.Series(fish_complete[\"Species\"]).cast(pl.Categorical).unique().sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05b8c46",
   "metadata": {},
   "source": [
    "### Prior Specification\n",
    "\n",
    "Let's start by specifying a prior for the intercept. Looking at the notation above, we see that we need a parameter for the intercept for each species. So we need $S$ parameters, one for each species.\n",
    "\n",
    "We will create a vector of length $S$ for the intercepts, and specify a normal prior for each element of the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d827aa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as fish_unpooled:\n",
    "\n",
    "    mu = pm.Normal('mu', mu=0, sigma=1, shape=len(species))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ecf45d",
   "metadata": {},
   "source": [
    "Better yet, we can used **named dimensions** to specify the shape of the random variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0166bad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = {\"species\": species}\n",
    "\n",
    "with pm.Model(coords=coords) as fish_unpooled:\n",
    "\n",
    "    mu = pm.Normal('mu', mu=0, sigma=1, dims='species')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca14b6d",
   "metadata": {},
   "source": [
    "This endows the `mu` random variable with a `species` dimension, which we can use to index into the intercepts for each species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398e9fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu.shape.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315fc98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_unpooled.named_vars_to_dims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b8a50c",
   "metadata": {},
   "source": [
    "Next, we need to specify priors for the slopes. Looking at the notation above, we see that we need a parameter for the slope for each species and each predictor. So we need a matrix of $S \\times 3$ parameters. This means that the `dims` argument should be a tuple with two elements: the list of species and the list of predictors.\n",
    "\n",
    "We also need to specify predictor variable coordinates, which will be used to index into the slope parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e8507c",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = {\n",
    "    \"species\": species,\n",
    "    \"slopes\": [\"width_effect\", \"height_effect\", \"length_effect\"]    \n",
    "}\n",
    "\n",
    "with pm.Model(coords=coords) as fish_unpooled:\n",
    "\n",
    "    mu = pm.Normal('mu', mu=0, sigma=1, dims='species')\n",
    "\n",
    "    beta = pm.Normal('beta', mu=0, sigma=1, dims=('species', 'slopes'))\n",
    "\n",
    "beta.shape.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4360ecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_unpooled.coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59633c28",
   "metadata": {},
   "source": [
    "Finally, we need to specify a prior for the standard deviation. We will use a half-normal distribution, which is the positive half of a zero-mean normal distribution.\n",
    "\n",
    "Since `sigma` is a scalar, it does not require `dims`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b7e354",
   "metadata": {},
   "outputs": [],
   "source": [
    "with fish_unpooled:\n",
    "\n",
    "    sigma = pm.HalfNormal('sigma', sigma=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b07d0d",
   "metadata": {},
   "source": [
    "### Deterministic Variables\n",
    "\n",
    "A deterministic variable is one whose values are **completely determined** by the values of their parents.\n",
    "\n",
    "In our model, the expected weight is a deterministic function of the intercept, slopes, and their associated predictor variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f424b019",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_width = fish_complete.get_column(\"log_width\").to_numpy()\n",
    "log_height = fish_complete.get_column(\"log_height\").to_numpy()\n",
    "log_length = fish_complete.get_column(\"log_length\").to_numpy()\n",
    "\n",
    "with fish_unpooled:\n",
    "\n",
    "    expected_weight = (\n",
    "        mu[species_idx]\n",
    "        + beta[species_idx, 0] * log_width\n",
    "        + beta[species_idx, 1] * log_height\n",
    "        + beta[species_idx, 2] * log_length\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826dbd55",
   "metadata": {},
   "source": [
    "There are two types of deterministic variables in PyMC:\n",
    "\n",
    "#### Anonymous deterministic variables\n",
    "\n",
    "The easiest way to create a deterministic variable is to operate on or transform one or more variables in a model directly, as we have done above for `expected_weight`.\n",
    "\n",
    "These are called *anonymous* variables because they are not named variables, as we did for `beta` above. We simply specified the variable as a Python (or, Pytensor) expression. This is therefore the simplest way to construct a determinstic variable. The only caveat is that the values generated by anonymous determinstics are not recorded to the model output during model fitting. So, this approach is only appropriate for intermediate values in your model that you do not wish to obtain posterior estimates for, alongside the other variables in the model.\n",
    "\n",
    "#### Named deterministic variables\n",
    "\n",
    "To ensure that deterministic variables' values are accumulated during sampling, they should be instantiated using the **named deterministic** interface; this uses the `Deterministic` function to create the variable. Two things happen when a variable is created this way:\n",
    "\n",
    "1. The variable is given a name (passed as the first argument)\n",
    "2. The variable is appended to the model's list of random variables, which ensures that its values are tallied.\n",
    "\n",
    "If we had wanted to track the expected weight, we could have done so by specifying a named deterministic variable:\n",
    "with fish_unpooled:\n",
    "\n",
    "```python\n",
    "expected_weight = pm.Deterministic('expected_weight', \n",
    "    mu[species_idx]\n",
    "    + beta[species_idx, 0] * log_width\n",
    "    + beta[species_idx, 1] * log_height\n",
    "    + beta[species_idx, 2] * log_length)\n",
    ")\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d518ae96",
   "metadata": {},
   "source": [
    "### Observed Random Variables\n",
    "\n",
    "Stochastic random variables whose values are observed are represented by a different class than unobserved random variables. An `ObservedRV` object is instantiated any time a stochastic variable is specified with data passed as the `observed` argument. \n",
    "\n",
    "In our model, the observed fish weights (on the log scale) are represented by an `ObservedRV` using a normal random variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f5d989",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_weight = fish_complete.get_column(\"log_weight\").to_numpy()\n",
    "\n",
    "with fish_unpooled:\n",
    "\n",
    "    pm.Normal(\n",
    "        \"log_obs\",\n",
    "        mu=expected_weight,\n",
    "        sigma=sigma,\n",
    "        observed=log_weight,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c145813d",
   "metadata": {},
   "source": [
    "## Parameter Transformation\n",
    "\n",
    "To support efficient sampling by PyMC's MCMC algorithms, any continuous variables that are **constrained** to a sub-interval of the real line are **automatically transformed** so that their support is unconstrained. This frees sampling algorithms from having to deal with boundary constraints.\n",
    "\n",
    "For example, if we look at the variables we have created in the model so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c87dcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fish_unpooled.value_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a560d50",
   "metadata": {},
   "source": [
    "The model's `value_vars` attribute stores the values of each random variable actually used by the model's log-likelihood.\n",
    "\n",
    "As the name suggests, the variable `sigma` has been log-transformed, and this is the space over which posterior sampling takes place. When a sample is drawn, the value of the transformed variable is simply back-transformed to recover the original variable.\n",
    "\n",
    "By default, auto-transformed variables are ignored when summarizing and plotting model output, since they are not generally of interest to the user.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e69c96",
   "metadata": {},
   "source": [
    "### The Model DAG\n",
    "\n",
    "Having specified the model, we can visualize the model  using the `to_graphviz` method.\n",
    "\n",
    "This displays the model as a directed acyclic graph (DAG). The nodes represent the random variables, and the edges represent the dependencies between them. All Bayesian models are DAGs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834d03a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_unpooled.to_graphviz()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146d2643",
   "metadata": {},
   "source": [
    "## Markov Chain Monte Carlo Methods\n",
    "\n",
    "Now that we understand how to specify models in PyMC, we need to address a fundamental question: how do we actually fit these models? For all but the simplest cases, the posterior distribution cannot be derived analytically. This is where Markov Chain Monte Carlo (MCMC) methods become essential.\n",
    "\n",
    "To understand how MCMC works in practice, let's explore the fundamental concepts through concrete examples. We'll start with basic Monte Carlo integration and build up to understanding why sophisticated algorithms like those in PyMC are necessary.\n",
    "\n",
    "### The Challenge of Bayesian Computation\n",
    "\n",
    "Recall Bayes' theorem:\n",
    "\n",
    "$$P(\\theta|x) = \\frac{P(x|\\theta) P(\\theta)}{P(x)}$$\n",
    "\n",
    "The denominator, $P(x) = \\int P(x|\\theta) P(\\theta) \\, d\\theta$, is called the marginal likelihood or evidence. For most models, this integral is intractable:\n",
    "\n",
    "1. **High dimensionality**: With multiple parameters, we need to integrate over many dimensions\n",
    "2. **Complex dependencies**: Parameters often have intricate relationships\n",
    "3. **Non-standard distributions**: The posterior rarely has a recognizable form\n",
    "\n",
    "### Monte Carlo Integration\n",
    "\n",
    "The foundation of all Monte Carlo methods is a simple but powerful idea: we can approximate integrals using random samples. Consider estimating the expected value of a function $h(\\theta)$ under a probability distribution $p(\\theta)$:\n",
    "\n",
    "$$E[h(\\theta)] = \\int h(\\theta) p(\\theta) d\\theta$$\n",
    "\n",
    "If we can draw samples $\\theta_1, \\theta_2, ..., \\theta_n$ from $p(\\theta)$, then by the Law of Large Numbers:\n",
    "\n",
    "$$E[h(\\theta)] \\approx \\frac{1}{n} \\sum_{i=1}^n h(\\theta_i)$$\n",
    "\n",
    "This approximation becomes exact as $n \\to \\infty$, and we can quantify the uncertainty in our estimate using the Central Limit Theorem.\n",
    "\n",
    "The key challenge is: **how do we draw samples from a distribution when we only know it up to a normalizing constant?**\n",
    "\n",
    "While Monte Carlo integration is powerful, it assumes we can directly sample from the distribution of interest. Markov Chain Monte Carlo (MCMC) methods provide an elegant solution to this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c336218",
   "metadata": {},
   "source": [
    "### The Markov Chain Monte Carlo Solution\n",
    "\n",
    "MCMC methods construct a Markov chain whose stationary distribution is the posterior distribution we want to sample from. The \"Markov\" property means that each sample depends only on the previous sample, not the entire history. The \"Monte Carlo\" aspect refers to the use of random sampling.\n",
    "\n",
    "> A Markov chain is a sequence of random variables $\\theta_1, \\theta_2, ..., \\theta_n$ that satisfy the Markov property:\n",
    ">\n",
    "> $$P(\\theta_t | \\theta_1, \\theta_2, ..., \\theta_{t-1}) = P(\\theta_t | \\theta_{t-1})$$\n",
    "> \n",
    "> This means that the probability of the next state depends only on the current state, not the entire history.\n",
    "\n",
    "The general MCMC algorithm follows this pattern:\n",
    "\n",
    "1. Start at some initial parameter values $\\theta^{(0)}$\n",
    "2. For iteration $t = 1, 2, ...$:\n",
    "   - Propose new parameter values $\\theta^*$ based on current values $\\theta^{(t-1)}$\n",
    "   - Accept or reject the proposal based on the posterior probability\n",
    "   - Set $\\theta^{(t)} = \\theta^*$ if accepted, otherwise $\\theta^{(t)} = \\theta^{(t-1)}$\n",
    "\n",
    "Different MCMC algorithms vary in how they propose new values and decide whether to accept them. The art lies in designing proposals that efficiently explore the posterior distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c880ec6a",
   "metadata": {},
   "source": [
    "#### The Metropolis-Hastings Algorithm\n",
    "\n",
    "The Metropolis-Hastings algorithm generates a Markov chain whose stationary distribution is the target posterior distribution. The key insight is the **Metropolis acceptance criterion**, which ensures detailed balance:\n",
    "\n",
    "$$A(\\theta^* | \\theta) = \\min\\left\\{1, \\frac{\\tilde{p}(\\theta^*)}{\\tilde{p}(\\theta)} \\cdot \\frac{q(\\theta|\\theta^*)}{q(\\theta^*|\\theta)}\\right\\}$$\n",
    "\n",
    "where:\n",
    "- $\\tilde{p}(\\theta)$ is the unnormalized posterior\n",
    "- $q(\\theta^*|\\theta)$ is the proposal distribution\n",
    "- $A(\\theta^* | \\theta)$ is the acceptance probability\n",
    "\n",
    "This criterion automatically adjusts for asymmetric proposals and ensures the chain converges to the correct distribution.\n",
    "\n",
    "Let's implement the general Metropolis-Hastings algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff3fb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metropolis_hastings(pdf, prop_dist, init=0):\n",
    "    \"\"\"General Metropolis-Hastings sampler.\n",
    "    \n",
    "    Args:\n",
    "        pdf: Target probability density function (unnormalized is ok)\n",
    "        prop_dist: Proposal distribution (scipy.stats distribution)\n",
    "        init: Initial value\n",
    "        \n",
    "    Yields:\n",
    "        (sample, accepted): Current sample and whether it was accepted\n",
    "    \"\"\"\n",
    "    current = init\n",
    "    while True:\n",
    "        \n",
    "        # Propose new state from proposal distribution\n",
    "        prop = prop_dist.rvs()\n",
    "        \n",
    "        # Calculate acceptance ratio\n",
    "        p_accept = min(1, pdf(prop) / pdf(current) * \n",
    "                      prop_dist.pdf(current) / prop_dist.pdf(prop))\n",
    "        \n",
    "        # Accept or reject\n",
    "        accept = np.random.rand() < p_accept\n",
    "        if accept:\n",
    "            current = prop\n",
    "        yield current, accept\n",
    "        \n",
    "def gen_samples(draws, sampler):\n",
    "    \"\"\"Generate samples from a sampler.\"\"\"\n",
    "    samples = np.empty(draws)\n",
    "    accepts = 0\n",
    "    for idx, (z, accept) in itertools.takewhile(lambda j: j[0] < draws, enumerate(sampler)):\n",
    "        accepts += accept\n",
    "        samples[idx] = z\n",
    "    return samples, accepts\n",
    "\n",
    "# Example: Sample from a mixture of Gaussians\n",
    "def target_pdf(x):\n",
    "    \"\"\"Mixture of two Gaussians\"\"\"\n",
    "    return 0.3 * st.norm.pdf(x, -2, 0.8) + 0.7 * st.norm.pdf(x, 3, 1.2)\n",
    "\n",
    "# Use a wide normal as proposal\n",
    "proposal_dist = st.norm(0, 10)\n",
    "\n",
    "# Generate samples\n",
    "samples, accepts = gen_samples(10_000, metropolis_hastings(target_pdf, proposal_dist))\n",
    "\n",
    "# Visualize results\n",
    "t = np.linspace(-6, 8, 500)\n",
    "pdf_values = [target_pdf(x) for x in t]\n",
    "\n",
    "hist = go.Histogram(\n",
    "    x=samples,\n",
    "    histnorm='probability density',\n",
    "    name='Samples',\n",
    "    marker=dict(color='rgba(0, 0, 255, 0.7)')\n",
    ")\n",
    "\n",
    "pdf_curve = go.Scatter(\n",
    "    x=t, y=pdf_values,\n",
    "    mode='lines',\n",
    "    name='True PDF',\n",
    "    line=dict(color='orange', width=2)\n",
    ")\n",
    "\n",
    "go.Figure(\n",
    "    data=[hist, pdf_curve]\n",
    ").update_layout(\n",
    "    title=f'Metropolis-Hastings: {samples.size:,d} samples with {100 * accepts / samples.size:.1f}% acceptance rate',\n",
    "    xaxis_title='Value',\n",
    "    yaxis_title='Density',\n",
    "    width=750,\n",
    "    height=400\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cd6798",
   "metadata": {},
   "source": [
    "### Random Walk Metropolis-Hastings\n",
    "\n",
    "The general Metropolis-Hastings algorithm can be inefficient if the proposal distribution is poorly chosen. A popular special case is **Random Walk Metropolis**, where the proposal is centered at the current position:\n",
    "\n",
    "$$\\theta^* \\sim \\mathcal{N}(\\theta, \\sigma^2)$$\n",
    "\n",
    "This symmetric proposal simplifies the acceptance ratio because the proposal terms cancel out:\n",
    "\n",
    "$$A(\\theta^* | \\theta) = \\min\\left\\{1, \\frac{\\tilde{p}(\\theta^*)}{\\tilde{p}(\\theta)}\\right\\}$$\n",
    "\n",
    "The key hyperparameter is the step size $\\sigma$:\n",
    "- **Too small**: Chain takes tiny steps and explores slowly (high acceptance, slow mixing)\n",
    "- **Too large**: Many proposals are rejected (low acceptance, slow mixing)\n",
    "- **Just right**: Balance between acceptance rate and step size (typically 20-50% acceptance)\n",
    "\n",
    "Let's implement and compare different step sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0be924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_walk_metropolis(pdf, step_size, init=0):\n",
    "    \"\"\"Random walk Metropolis algorithm.\n",
    "    \n",
    "    Args:\n",
    "        pdf: Target probability density function (unnormalized is ok)\n",
    "        step_size: Standard deviation of proposal distribution\n",
    "        init: Initial value\n",
    "        \n",
    "    Yields:\n",
    "        (sample, accepted): Current sample and whether it was accepted\n",
    "    \"\"\"\n",
    "    current = init\n",
    "    while True:\n",
    "        # Random walk proposal\n",
    "        prop = current + np.random.normal(0, step_size)\n",
    "        \n",
    "        # Simple acceptance ratio (proposal terms cancel)\n",
    "        p_accept = min(1, pdf(prop) / pdf(current))\n",
    "        \n",
    "        # Accept or reject\n",
    "        accept = np.random.rand() < p_accept\n",
    "        if accept:\n",
    "            current = prop\n",
    "        yield current, accept\n",
    "\n",
    "# Compare different step sizes\n",
    "fig = make_subplots(rows=3, cols=1, \n",
    "                    subplot_titles=['Small Step Size (=0.1)', \n",
    "                                   'Medium Step Size (=8.0)', \n",
    "                                   'Large Step Size (=70.0)'])\n",
    "\n",
    "step_sizes = [0.1, 8.0, 70.0]\n",
    "\n",
    "for i, step_size in enumerate(step_sizes, 1):\n",
    "    # Generate samples\n",
    "    samples, accepts = gen_samples(10_000, random_walk_metropolis(target_pdf, step_size))\n",
    "    \n",
    "    # Calculate t and pdf values for the line\n",
    "    t = np.linspace(samples.min(), samples.max(), 500)\n",
    "    pdf_values = [target_pdf(x) for x in t]\n",
    "    \n",
    "    # Add histogram\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=samples,\n",
    "            histnorm='probability density',\n",
    "            marker=dict(color='rgba(0, 0, 255, 0.7)'),\n",
    "            name=f\"Samples (={step_size})\",\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=i, col=1\n",
    "    )\n",
    "    \n",
    "    # Add PDF line\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=t,\n",
    "            y=pdf_values,\n",
    "            mode='lines',\n",
    "            line=dict(color='orange', width=2),\n",
    "            name=f\"True PDF\",\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=i, col=1\n",
    "    )\n",
    "    \n",
    "    # Add annotation with acceptance rate\n",
    "    fig.add_annotation(\n",
    "        text=f\"Acceptance rate: {100 * accepts / samples.size:.1f}%\",\n",
    "        xref=f\"x{i}\", yref=f\"y{i}\",\n",
    "        x=0.95, y=0.95,\n",
    "        xanchor=\"right\", yanchor=\"top\",\n",
    "        showarrow=False,\n",
    "        bgcolor=\"white\",\n",
    "        bordercolor=\"black\",\n",
    "        borderwidth=1\n",
    "    )\n",
    "\n",
    "fig.update_layout(height=900, width=700, showlegend=False)\n",
    "fig.update_xaxes(title_text=\"Value\", row=3, col=1)\n",
    "fig.update_yaxes(title_text=\"Density\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8906701e",
   "metadata": {},
   "source": [
    "### Visualizing MCMC Behavior\n",
    "\n",
    "To better understand how different step sizes affect the sampler's behavior, let's look at trace plots that show the evolution of the chain over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8d0f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 1000\n",
    "\n",
    "fig = make_subplots(rows=3, cols=1, \n",
    "                    subplot_titles=['Small Step Size (=0.1): Slow Exploration', \n",
    "                                   'Medium Step Size (=8.0): Good Mixing', \n",
    "                                   'Large Step Size (=70.0): Many Rejections'])\n",
    "\n",
    "for i, step_size in enumerate(step_sizes, 1):\n",
    "    sampler = random_walk_metropolis(target_pdf, step_size)\n",
    "    trace = []\n",
    "    for _ in range(n_steps):\n",
    "        sample, _ = next(sampler)\n",
    "        trace.append(sample)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            y=trace,\n",
    "            mode='lines',\n",
    "            line=dict(width=1),\n",
    "            name=f\"={step_size}\",\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=i, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_hline(y=-2, line_dash=\"dash\", line_color=\"red\", \n",
    "                  row=i, col=1, annotation_text=\"Mode 1\")\n",
    "    fig.add_hline(y=3, line_dash=\"dash\", line_color=\"red\", \n",
    "                  row=i, col=1, annotation_text=\"Mode 2\")\n",
    "\n",
    "fig.update_layout(height=900, width=800, showlegend=False)\n",
    "fig.update_xaxes(title_text=\"Iteration\", row=3, col=1)\n",
    "fig.update_yaxes(title_text=\"Value\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73382eea",
   "metadata": {},
   "source": [
    "### Modern MCMC: The No-U-Turn Sampler (NUTS)\n",
    "\n",
    "While Metropolis-Hastings and its variants are foundational, they suffer from a critical limitation: they explore parameter space through a **random walk**. This means proposals are made blindly, without any information about where high-probability regions might lie. It's like searching for treasure in a dark room by taking random stepsyou'll eventually find it, but it's terribly inefficient.\n",
    "\n",
    "#### The Random Walk Problem\n",
    "\n",
    "Random walk samplers face several challenges:\n",
    "- **Inefficient exploration**: Most proposals in high dimensions get rejected\n",
    "- **Slow mixing**: Takes many steps to move between distant regions of high probability  \n",
    "- **Correlation issues**: When parameters are correlated, the sampler zigzags inefficiently through parameter space\n",
    "- **Tuning nightmare**: Step size requires careful manual tuningtoo small and you explore slowly, too large and most proposals get rejected\n",
    "\n",
    "#### Enter Gradient Information: A Guided Search\n",
    "\n",
    "Modern algorithms like Hamiltonian Monte Carlo (HMC) and its extension, the No-U-Turn Sampler (NUTS), revolutionize MCMC by using **gradient information**the derivative of the log-posterior with respect to parameters. This is like having a compass that always points toward regions of higher probability.\n",
    "\n",
    "Instead of stumbling randomly, NUTS:\n",
    "1. **Follows the gradient contours**: Uses derivatives to understand the \"shape\" of the posterior\n",
    "2. **Makes intelligent proposals**: Proposes distant points along paths of high probability\n",
    "3. **Maintains detailed balance**: Still guarantees correct sampling from the posterior\n",
    "\n",
    "The fundamental difference is stark: random walk methods explore blindly through trial and error, accepting or rejecting moves based solely on posterior probability. Gradient-based methods, by contrast, use calculus to determine the direction of steepest ascent in probability, enabling them to make informed, efficient moves toward high-probability regions.\n",
    "\n",
    "#### Key Advantages of NUTS:\n",
    "\n",
    "1. **Automatic tuning**: NUTS adapts its parameters during warmup, eliminating manual tuning headaches\n",
    "2. **Efficient exploration**: Can traverse the entire posterior in a single trajectory rather than thousands of random steps\n",
    "3. **Handles correlations naturally**: Moves along the principal axes of variation, not fighting against correlation structure\n",
    "4. **Scales to high dimensions**: Performance doesn't degrade catastrophically as dimensionality increases\n",
    "5. **Fewer samples needed**: Often 100s of effective samples from NUTS equals 10,000s from random walk methods\n",
    "\n",
    "#### When NUTS Dominates:\n",
    "\n",
    "- **High-dimensional parameter spaces** (dozens to thousands of parameters)\n",
    "- **Complex posterior geometries** with strong correlations, ridges, or funnel shapes\n",
    "- **Hierarchical models** where parameters at different levels interact\n",
    "- **Any model with continuous parameters** where gradients can be computed\n",
    "\n",
    "#### The Computational Trade-off:\n",
    "\n",
    "NUTS requires computing gradients at each step, making individual iterations more expensive than simple random walk methods. However, this cost is overwhelmingly offset by needing far fewer samples for the same effective sample size. It's like paying more for a sports car that gets you there 100x faster than walking.\n",
    "\n",
    "#### Implementation in PyMC:\n",
    "\n",
    "PyMC uses NUTS as the default sampler for continuous variables. When you call `pm.sample()`, PyMC:\n",
    "\n",
    "1. **Automatically differentiates** your model to compute gradients (via automatic differentiation)\n",
    "2. **Runs adaptation** to learn the geometry of your posterior (mass matrix and step size)\n",
    "3. **Generates efficient samples** using momentum to explore the posterior\n",
    "4. **Provides diagnostics** to assess convergence and sampling quality\n",
    "\n",
    "The beauty of PyMC is that all this sophisticated machinery is hidden behind a simple interface. However, understanding why gradient-based methods are superior helps us appreciate why modern Bayesian computation is so powerful and why NUTS has become the workhorse of practical Bayesian analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdf13e5",
   "metadata": {},
   "source": [
    "### Sampling with NUTS\n",
    "\n",
    "The `pm.sample()` function is the main interface for sampling from a model. It has a number of optional arguments that allow us to customize the sampling process.\n",
    "\n",
    "The most important arguments are:\n",
    "\n",
    "- `draws`: The number of samples to draw\n",
    "- `tune`: The number of samples to use for tuning\n",
    "- `chains`: The number of chains to run\n",
    "\n",
    "We will use the default values for these arguments, which are:\n",
    "\n",
    "- `draws`: 1000\n",
    "- `tune`: 1000\n",
    "- `chains`: min(4, number of CPU cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abc2e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "with fish_unpooled:\n",
    "    trace_unpooled = pm.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54be745",
   "metadata": {},
   "source": [
    "The `plot_trace` function in ArviZ creates diagnostic plots showing both the trace (time series of samples) and marginal posterior distributions for each parameter.\n",
    "\n",
    "Traceplots are useful for evaluating the performance of our MCMC sampling. In these plots, we aim to see a \"fuzzy caterpillar\" pattern on the right side, which indicates that the chains are **mixing well** and exploring the parameter space effectively. This is evidence to suggest the chains have converged (to something!) and are providing a reasonable representation of the posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca23e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_unpooled, var_names='sigma');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75ce8f6",
   "metadata": {},
   "source": [
    "Let's look at the posterior distributions for the intercepts. Multivariate posterior distributions are best visualized using a forest plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b33af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(trace_unpooled, var_names='mu', transform=np.exp);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fbf79f",
   "metadata": {},
   "source": [
    "The intercepts look small (even on the nominal scale) which seems odd. But recall how intercepts are interpreted: they are the expected value of the outcome when all predictors are zero. In this case, that means when the log of the width, height, and length are zero. This is an awkward from an interpretive standpoint. \n",
    "\n",
    "How could we improve this?\n",
    "\n",
    "Give it a try, and re-run the improved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ad0ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_width = (fish_complete.get_column(\"log_width\") - fish_complete.get_column(\"log_width\").mean()).to_numpy()\n",
    "log_height = (fish_complete.get_column(\"log_height\") - fish_complete.get_column(\"log_height\").mean()).to_numpy()\n",
    "log_length = (fish_complete.get_column(\"log_length\") - fish_complete.get_column(\"log_length\").mean()).to_numpy()\n",
    "\n",
    "with pm.Model(coords=coords) as fish_unpooled:\n",
    "\n",
    "    # priors\n",
    "    mu = pm.Normal(\"mu\", sigma=1.0, dims=\"species\")\n",
    "    beta = pm.Normal(\"beta\", sigma=0.5, dims=(\"slopes\", \"species\"))\n",
    "\n",
    "    # linear regression\n",
    "    expected_weight = (\n",
    "        mu[species_idx]\n",
    "        + beta[0, species_idx] * log_width\n",
    "        + beta[1, species_idx] * log_height\n",
    "        + beta[2, species_idx] * log_length\n",
    "    )\n",
    "    # observational noise\n",
    "    sigma = pm.HalfNormal(\"sigma\", 1.0)\n",
    "\n",
    "    # likelihood\n",
    "    log_obs = pm.Normal(\n",
    "        \"log_obs\", mu=expected_weight, sigma=sigma, observed=log_weight\n",
    "    )\n",
    "\n",
    "    # sampling\n",
    "    trace_unpooled = pm.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0ddf80",
   "metadata": {},
   "source": [
    "Now we have meaningful intercepts -- the expected weight of a fish with average width, height, and length for each species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c323ac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_unpooled, var_names='mu', transform=np.exp);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1553697",
   "metadata": {},
   "source": [
    "When we have vector-valued parameters, a forest plot is convenient for visualizing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06f54a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(trace_unpooled, var_names=\"beta\", transform=np.exp);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ef0581",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_unpooled, var_names=\"sigma\", transform=np.exp);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1b58bb",
   "metadata": {},
   "source": [
    "## Predicting out-of-sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2402f1e8",
   "metadata": {},
   "source": [
    "In statistical workflows, a common task is to make predictions using new, unseen data, often referred to as \"out-of-sample\" data. In PyMC, the most straightforward approach to achieve this is by utilizing the `Data` container. This container allows PyMC and ArviZ to specify the data used for training the model, and then allow you to modify it later on.\n",
    "\n",
    "#### Splitting Data into Training and Test Sets\n",
    "\n",
    "To illustrate this functionality, let's randomly select 90% of our data as the training dataset for the model, while reserving the remaining 10% as the test data. This test data will be unseen by the model during the training process, allowing us to evaluate its performance on new, previously unseen data when making predictions.\n",
    "\n",
    "By following this approach, you can effectively train your model on a subset of the available data and then assess its predictive capabilities on the held-out test data, mimicking real-world scenarios where predictions need to be made on new, unobserved data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3872639b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_test = (\n",
    "    fish_complete.sample(fraction=0.1, seed=1)\n",
    "    .with_row_index()\n",
    ")\n",
    "test_idx = fish_test.get_column(\"index\")\n",
    "fish_train = (\n",
    "    fish_complete.with_row_index()\n",
    "    .filter(~pl.col(\"index\").is_in(test_idx))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b33e6a7",
   "metadata": {},
   "source": [
    "Since the dataset changed compared to the previous model, we also have to redefine our coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706e3761",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_idx = pl.Series(fish_train.get_column(\"Species\")).cast(pl.Categorical).to_physical().to_numpy()\n",
    "species = fish_train.get_column(\"Species\").unique(maintain_order=True).sort()\n",
    "\n",
    "coords = {\n",
    "    \"slopes\": [\"width_effect\", \"height_effect\", \"length_effect\"],\n",
    "    \"species\": species,\n",
    "    \"obs_idx\": range(fish_train.height),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb4ab68",
   "metadata": {},
   "source": [
    "In PyMC, we can use `pm.Data` objects for our data. It allows you to define data as a symbolic node in the model that you can later switch out for other data. \n",
    "\n",
    "Let's re-write our model to use `pm.Data` objects for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdb6b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords=coords) as fish_unpooled_oos:\n",
    "    # data\n",
    "    log_width = pm.Data(\n",
    "        \"log_width\", \n",
    "        (fish_train.get_column(\"log_width\") - fish_train.get_column(\"log_width\").mean()).to_numpy(),\n",
    "        dims=\"obs_idx\"\n",
    "    )\n",
    "    log_height = pm.Data(\n",
    "        \"log_height\",\n",
    "        (fish_train.get_column(\"log_height\") - fish_train.get_column(\"log_height\").mean()).to_numpy(),\n",
    "        dims=\"obs_idx\"\n",
    "    )\n",
    "    log_length = pm.Data(\n",
    "        \"log_length\",\n",
    "        (fish_train.get_column(\"log_length\") - fish_train.get_column(\"log_length\").mean()).to_numpy(),\n",
    "        dims=\"obs_idx\"\n",
    "    )\n",
    "    log_weight = pm.Data(\n",
    "        \"log_weight\",\n",
    "        fish_train.get_column(\"log_weight\").to_numpy(),\n",
    "        dims=\"obs_idx\"\n",
    "    )\n",
    "    s = pm.Data(\"species_idx\", species_idx, dims=\"obs_idx\")\n",
    "\n",
    "    # priors\n",
    "    mu = pm.Normal(\"mu\", sigma=1.0, dims=\"species\")\n",
    "    beta = pm.Normal(\"beta\", sigma=0.5, dims=(\"slopes\", \"species\"))\n",
    "\n",
    "    # linear regression\n",
    "    expected_weight = (\n",
    "        mu[s]\n",
    "        + beta[0, s] * log_width\n",
    "        + beta[1, s] * log_height\n",
    "        + beta[2, s] * log_length\n",
    "    )\n",
    "    # observational noise\n",
    "    sigma = pm.HalfNormal(\"sigma\", 1.0)\n",
    "\n",
    "    # likelihood\n",
    "    log_obs = pm.Normal(\n",
    "        \"log_obs\", mu=expected_weight, sigma=sigma, observed=log_weight, dims=\"obs_idx\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c2335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(fish_unpooled_oos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a6a728",
   "metadata": {},
   "outputs": [],
   "source": [
    "with fish_unpooled_oos:\n",
    "    trace_unpooled_oos = pm.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a55325",
   "metadata": {},
   "source": [
    "Checking the traceplots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7099c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_unpooled_oos, transform=np.exp);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f87330f",
   "metadata": {},
   "source": [
    "Now we want to see how this model would work in production: given some fish morphometrics, can we accurately predict the weight of the fish?\n",
    "\n",
    "To do this, we use `set_data` to change the inputs from the training set to the test set. First, let's query our test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb22c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the species\n",
    "species_idx_test = pl.Series(fish_test.get_column(\"Species\")).cast(pl.Categorical).to_physical().cast(pl.Int64)\n",
    "\n",
    "species_idx_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e606b0b",
   "metadata": {},
   "source": [
    "Now we apply these values to the `Data` nodes in the model.\n",
    "\n",
    "Note that we are shifting the input variables using the training set mean and standard deviation. You always want to use the same transformation on the test set as you did on the training set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51820676",
   "metadata": {},
   "outputs": [],
   "source": [
    "with fish_unpooled_oos:\n",
    "    pm.set_data(\n",
    "        coords={\"obs_idx\": range(len(fish_test))},\n",
    "        new_data={\n",
    "            \"log_height\": fish_test.get_column(\"log_height\").to_numpy() - fish_train.get_column(\"log_height\").mean(),\n",
    "            \"log_length\": fish_test.get_column(\"log_length\").to_numpy() - fish_train.get_column(\"log_length\").mean(), \n",
    "            \"log_width\": fish_test.get_column(\"log_width\").to_numpy() - fish_train.get_column(\"log_width\").mean(),\n",
    "            \"log_weight\": np.zeros(len(fish_test)),\n",
    "            \"species_idx\": species_idx_test.cast(pl.UInt32).to_numpy(),\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837cc994",
   "metadata": {},
   "source": [
    "### Use updated values to predict outcomes\n",
    "\n",
    "We want to use our fitted model to predict the weight of the fish in the test set. This involves simulating observations from the model for each fish in the test set using the posterior distribution of the parameters.\n",
    "\n",
    "Fortunately, simulating data from the model is a natural component of the Bayesian modelling framework. Recall, from the discussion on prediction, the posterior predictive distribution:\n",
    "\n",
    "$$p(\\tilde{y}|y) = \\int p(\\tilde{y}|\\theta) f(\\theta|y) d\\theta$$\n",
    "\n",
    "Here, $\\tilde{y}$ represents some hypothetical new data that would be expected, taking into account the posterior uncertainty in the model parameters. \n",
    "\n",
    "Sampling from the posterior predictive distribution is easy in PyMC. The `sample_posterior_predictive` function draws posterior predictive samples from all of the observed variables in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fca901",
   "metadata": {},
   "outputs": [],
   "source": [
    "with fish_unpooled_oos:\n",
    "    pm.sample_posterior_predictive(\n",
    "        trace_unpooled_oos,\n",
    "        predictions=True,\n",
    "        extend_inferencedata=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3690ef85",
   "metadata": {},
   "source": [
    "How good are these imputations? Glad you asked. Remember that our data are not _really_ out-of-sample; we just cut them out from our original dataset, so we can compare our predictions to the true weights. This is a simple line of code in ArviZ (we just exponentiate the predicted log weights to compare them to the true weights):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34412798",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(\n",
    "    trace_unpooled_oos.predictions,\n",
    "    ref_val=fish_test.get_column(\"Weight\").to_list(),\n",
    "    transform=np.exp,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109f88a4",
   "metadata": {},
   "source": [
    "So the predicted values all fell within the predictive distributions -- not all within the 95% interval, but there were no extreme predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7bdae8",
   "metadata": {},
   "source": [
    "## From predictions to business insights\n",
    "\n",
    "Recall from the introduction that there are different price tiers for weights, and those tiers can get _really_ expensive, so we want to know the probability of an item being above any theshold.\n",
    "\n",
    "- $> 250$\n",
    "- $> 500$\n",
    "- $> 750$\n",
    "- $> 1000$\n",
    "\n",
    "Since we have calculated posterior distributions, we have the ability to compute these probabilities for any new fish we observe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b971471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract projections to numpy array\n",
    "predictions = (\n",
    "    np.exp(\n",
    "        az.extract(trace_unpooled_oos.predictions)\n",
    "        .to_array()\n",
    "        .to_numpy()\n",
    "        .squeeze()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493affef",
   "metadata": {},
   "source": [
    "Now we can see what proportion are above $250$ grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7a5bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 250\n",
    "(predictions >= threshold).mean(axis=1).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e055e11",
   "metadata": {},
   "source": [
    "If we take something like a 0.5 probability as being \"above\", we can make a decision about each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d77559",
   "metadata": {},
   "outputs": [],
   "source": [
    "(predictions >= threshold).mean(axis=1).round(2) > 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f059d7",
   "metadata": {},
   "source": [
    "\n",
    "But remember that there are four thresholds $(250, 500, 750, 1000)$, so let's generalize this approach to the other three thresholds. We'll also plot these probabilities of being above thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9277efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.exp(trace_unpooled_oos.predictions)\n",
    "\n",
    "axes = az.plot_posterior(predictions, color=\"k\")\n",
    "\n",
    "for k, threshold in enumerate([250, 500, 750, 1000]):\n",
    "    probs_above_threshold = (predictions >= threshold).mean(dim=(\"chain\", \"draw\"))\n",
    "\n",
    "    for i, ax in enumerate(axes.ravel()):\n",
    "        ax.axvline(threshold, color=f\"C{k}\")\n",
    "        _, pdf = az.kde(\n",
    "            predictions[\"log_obs\"].sel(obs_idx=i).stack(sample=(\"chain\", \"draw\")).data\n",
    "        )\n",
    "        ax.text(\n",
    "            x=threshold - 35,\n",
    "            y=pdf.max() / 2,\n",
    "            s=f\">={threshold}\",\n",
    "            color=f\"C{k}\",\n",
    "            fontsize=\"16\",\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "        ax.text(\n",
    "            x=threshold - 20,\n",
    "            y=pdf.max() / 2.3,\n",
    "            s=f\"{probs_above_threshold.sel(obs_idx=i)['log_obs'].data * 100:.0f}%\",\n",
    "            color=f\"C{k}\",\n",
    "            fontsize=\"16\",\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "        ax.set_title(f\"New fish\\n{i}\", fontsize=16)\n",
    "        ax.set(xlabel=\"Weight\\n\", ylabel=\"Plausible values\")\n",
    "plt.suptitle(\n",
    "    \"Probability of weighing more than thresholds\", fontsize=26, fontweight=\"bold\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d94b1e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). *Bayesian Data Analysis* (3rd ed.). Chapman and Hall/CRC.\n",
    "\n",
    "Martin, O. A., Kumar, R., & Lao, J. (2021). *Bayesian Analysis with Python: Introduction to statistical modeling and probabilistic programming using PyMC3 and ArviZ* (2nd ed.). Packt Publishing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a711ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
