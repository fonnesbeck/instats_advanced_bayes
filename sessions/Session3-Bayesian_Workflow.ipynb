{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016911,
     "end_time": "2020-03-27T06:09:14.400757",
     "exception": false,
     "start_time": "2020-03-27T06:09:14.383846",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# The Bayesian Workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 2.069288,
     "end_time": "2020-03-27T06:09:16.527404",
     "exception": false,
     "start_time": "2020-03-27T06:09:14.458116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import polars as pl\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "import matplotlib.pyplot as plt\n",
    "import load_covid_data\n",
    "\n",
    "# Visual style\n",
    "az.style.use('arviz-doc')\n",
    "pio.templates.default = 'plotly_white'\n",
    "\n",
    "# Plotly defaults\n",
    "px.defaults.template = 'plotly_white'\n",
    "px.defaults.width = 900\n",
    "px.defaults.height = 500\n",
    "_base = pio.templates['plotly_white']\n",
    "_tmpl = go.layout.Template(_base)\n",
    "_tmpl.layout.hovermode = 'x unified'\n",
    "pio.templates['hoverx'] = _tmpl\n",
    "pio.templates.default = 'plotly_white+hoverx'\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 20090425\n",
    "RNG = np.random.default_rng(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this session, we will learn to evaluate the quality of our models using statistical and visual diagnostics. We'll also discuss a comprehensive **Bayesian workflow** that promotes model development through an iterative process.The Bayesian Workflow is a process for developing and testing Bayesian models, intended to detect problems and produce reliable results.\n",
    "\n",
    "Here are the steps we'll follow:\n",
    "\n",
    "1. Explore data: find patterns the model should capture, identify anomalies\n",
    "\n",
    "2. Build a model: translate ideas into code\n",
    "\n",
    "3. Prior predictive check: confirm that the priors make sense and the data-generating process is appropriate\n",
    "\n",
    "4. Fit the model: run the sampler\n",
    "\n",
    "5. Convergence diagnostics: confirm that the results are a sample from the posterior distribution\n",
    "\n",
    "6. Posterior predictive check: confirm that the model includes the features it needs to reproduce the important patterns in the data\n",
    "\n",
    "7. Model improvement: modify the model until fit is satisfactory\n",
    "\n",
    "Each step of this process is iterative, and at every step you have to decide whether to go on or loop back.\n",
    "\n",
    "\n",
    "Strengths of Bayesian statistics that are critical here:\n",
    "* Great flexibility to quickly and iteratively build statistical models\n",
    "* Offers principled way of dealing with uncertainty\n",
    "* Don't just want most likely outcome but distribution of all possible outcomes\n",
    "* Allows expert information to guide model by using informative priors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.009784,
     "end_time": "2020-03-27T06:09:16.547140",
     "exception": false,
     "start_time": "2020-03-27T06:09:16.537356",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## COVID-19 Dataset\n",
    "\n",
    "First we'll load data on COVID-19 cases from the WHO. In order to ease analysis we will remove any days were confirmed cases was below 100 (as reporting is often very noisy in this time frame). It also allows us to align countries with each other for easier comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1.663552,
     "end_time": "2020-03-27T06:09:18.220032",
     "exception": false,
     "start_time": "2020-03-27T06:09:16.556480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = load_covid_data.load_data(drop_states=True, filter_n_days_100=2)\n",
    "countries = df['country'].unique()\n",
    "n_countries = len(countries)\n",
    "df = df.filter(pl.col('days_since_100') >= 0)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's step through the workflow!\n",
    "\n",
    "### 1. Explore the data\n",
    "\n",
    "We will look at German COVID-19 cases. At first, we will only look at the first 30 days after Germany crossed 100 cases, later we will look at the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'Germany'\n",
    "date = '2020-07-31'\n",
    "df_country = df.filter(pl.col('country') == country).filter(pl.col('date') <= pl.datetime(2020, 7, 31)).head(30)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add confirmed cases line\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df_country['date'],\n",
    "        y=df_country['confirmed'],\n",
    "        mode='lines+markers',\n",
    "        name='Confirmed cases',\n",
    "        line=dict(color='royalblue', width=2),\n",
    "        marker=dict(size=6)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=dict(\n",
    "        text=f'COVID-19 Cases in {country}',\n",
    "        x=0.5\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title='Date',\n",
    "        tickformat='%b %d\\n%Y'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Confirmed cases',\n",
    "        gridcolor='lightgray'\n",
    "    ),\n",
    "    width=1000,\n",
    "    height=800,\n",
    "    plot_bgcolor='white'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the above plot and think of what type of model you would build to model the data.\n",
    "\n",
    "## 2. Build model\n",
    "\n",
    "The above line kind of looks exponential. This matches with knowledge from epidemiology whereas early in an epidemic it grows exponentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get time-range of days since 100 cases were crossed\n",
    "t = df_country['days_since_100'].to_numpy()\n",
    "# Get number of confirmed cases for Germany\n",
    "confirmed = df_country['confirmed'].to_numpy()\n",
    "\n",
    "with pm.Model() as model_exp1:\n",
    "    # Intercept\n",
    "    a = pm.Normal('a', mu=0, sigma=100)\n",
    "\n",
    "    # Slope\n",
    "    b = pm.Normal('b', mu=0.3, sigma=0.3)\n",
    "\n",
    "    # Exponential regression\n",
    "    growth = a * (1 + b) ** t\n",
    "\n",
    "    # Error term\n",
    "    eps = pm.HalfNormal('eps', 100)\n",
    "\n",
    "    # Likelihood\n",
    "    pm.Normal('obs',\n",
    "              mu=growth,\n",
    "              sigma=eps,\n",
    "              observed=confirmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(model_exp1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just looking at the above model, what do you think? Is there anything you would have done differently?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run prior predictive check\n",
    "\n",
    "Without even fitting the model to our data, we generate new potential data from our priors. Usually we have less intuition about the parameter space, where we define our priors, and more intution about what data we might expect to see. A prior predictive check thus allows us to make sure the model can generate the types of data we expect to see.\n",
    "\n",
    "The process works as follows:\n",
    "\n",
    "1. Pick a point from the prior $\\theta_i$\n",
    "2. Generate data set $x_i \\sim f(\\theta_i)$ where $f$ is our likelihood function (e.g. normal).\n",
    "3. Rinse and repeat $n$ times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_exp1:\n",
    "    prior_pred = pm.sample_prior_predictive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.plot(prior_pred.prior_predictive['obs'].values.squeeze().T, color=\"0.5\", alpha=.1)\n",
    "ax.set(ylim=(-1000, 1000),\n",
    "       xlim=(0, 10),\n",
    "       title=\"Prior predictive\",\n",
    "       xlabel=\"Days since 100 cases\",\n",
    "       ylabel=\"Positive cases\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this look sensible? Why or why not? What is the prior predictive sample telling us?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's wrong with this model?\n",
    "\n",
    "Above you hopefully identified a few issues with this model:\n",
    "1. Cases can't be negative\n",
    "2. Cases can not start at 0, as we set it to start at above 100.\n",
    "3. Case counts can't go down\n",
    "\n",
    "Let's improve our model. The presence of negative cases is due to us using a Normal likelihood. Instead, let's use a `NegativeBinomial`, which is similar to `Poisson` which is commonly used for count-data but has an extra dispersion parameter that allows more flexiblity in modeling the variance of the data.\n",
    "\n",
    "We will also change the prior of the intercept to be centered at 100 and tighten the prior of the slope.\n",
    "\n",
    "The negative binomial distribution uses an overdispersion parameter, which we will describe using a gamma distribution. A companion package called `preliz`, a library for prior distribution elicitation, has a nice utility called `maxent` that will help us parameterize this prior, as the gamma distribution is not as intuitive to work with as the normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preliz as pz\n",
    "\n",
    "gamma_params = pz.maxent(\n",
    "    pz.Gamma(),\n",
    "    lower=0.1, upper=20,\n",
    "    mass=0.95\n",
    ")\n",
    "gamma_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pm.draw(pm.Gamma.dist(alpha=2, beta=0.2), 1000), bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df_country['days_since_100'].to_numpy()\n",
    "confirmed = df_country['confirmed'].to_numpy()\n",
    "\n",
    "with pm.Model() as model_exp2:\n",
    "    # Intercept\n",
    "    a = pm.Normal('a', mu=100, sigma=25)\n",
    "\n",
    "    # Slope\n",
    "    b = pm.Normal('b', mu=0.3, sigma=0.1)\n",
    "\n",
    "    # Exponential regression\n",
    "    growth = a * (1 + b) ** t\n",
    "\n",
    "    alpha = pz.maxent(\n",
    "        pz.Gamma(),\n",
    "        lower=0.1, upper=20,\n",
    "        mass=0.95,\n",
    "        plot=False\n",
    "    ).to_pymc(\"alpha\")\n",
    "\n",
    "    # Likelihood\n",
    "    pm.NegativeBinomial('obs',\n",
    "                 growth,\n",
    "                 alpha=alpha,\n",
    "                 observed=confirmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_exp2:\n",
    "    prior_pred = pm.sample_prior_predictive()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.plot(prior_pred.prior_predictive['obs'].values.squeeze().T, color=\"0.5\", alpha=.1)\n",
    "ax.set(ylim=(-100, 1000),\n",
    "       xlim=(0, 10),\n",
    "       title=\"Prior predictive\",\n",
    "       xlabel=\"Days since 100 cases\",\n",
    "       ylabel=\"Positive cases\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_exp2:\n",
    "    trace_exp2 = pm.sample(random_seed=RANDOM_SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks much better. However, we can include even more prior information. For example, we know that the intercept *can not* be below 100 because of how we sliced the data. We can thus create a prior that does not have probability mass below 100. For this, we use the PyMC `HalfNormal` distribution; we can apply the same for the slope which we know is not going to be negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df_country['days_since_100'].to_numpy()\n",
    "confirmed = df_country['confirmed'].to_numpy()\n",
    "\n",
    "with pm.Model() as model_exp3:\n",
    "    # Intercept\n",
    "    a0 = pm.HalfNormal('a0', sigma=25)\n",
    "    a = pm.Deterministic('a', a0 + 100)\n",
    "\n",
    "    # Slope\n",
    "    b = pm.HalfNormal('b', sigma=0.2)\n",
    "\n",
    "    # Exponential regression\n",
    "    growth = a * (1 + b) ** t\n",
    "\n",
    "    gamma_params = pm.find_constrained_prior(\n",
    "        pm.Gamma,\n",
    "        lower=0.1, upper=20,\n",
    "        init_guess={'alpha':6, 'beta':1},\n",
    "        mass=0.95\n",
    "    )\n",
    "    alpha = pm.Gamma(\"alpha\", **gamma_params)\n",
    "\n",
    "    # Likelihood\n",
    "    pm.NegativeBinomial('obs',\n",
    "                 growth,\n",
    "                 alpha=alpha,\n",
    "                 observed=confirmed)\n",
    "    \n",
    "    prior_pred = pm.sample_prior_predictive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(prior_pred, var_names=['a', 'b'], group='prior');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "ax.plot(az.extract(prior_pred.prior_predictive)['obs'], color=\"0.5\", alpha=.1)\n",
    "ax.set(ylim=(0, 1000),\n",
    "       xlim=(0, 10),\n",
    "       title=\"Prior predictive\",\n",
    "       xlabel=\"Days since 100 cases\",\n",
    "       ylabel=\"Positive cases\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that even though the intercept parameter can not be below 100 now, we still see data generated at below hundred. Why? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_exp3:\n",
    "    # Inference button (TM)\n",
    "    trace_exp3 = pm.sample(random_seed=RANDOM_SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Assess convergence\n",
    "\n",
    "There are **two components** to model checking:\n",
    "\n",
    "1. Convergence diagnostics\n",
    "2. Goodness of fit\n",
    "\n",
    "Convergence diagnostics are intended to detect **lack of convergence** in the Markov chain Monte Carlo sample; it is used to ensure that you have not halted your sampling too early. However, a converged model is not guaranteed to be a good model. \n",
    "\n",
    "### Convergence Diagnostics\n",
    "\n",
    "Valid inferences from sequences of MCMC samples are based on the\n",
    "assumption that the samples are derived from the true posterior\n",
    "distribution of interest. Theory guarantees this condition as the number\n",
    "of iterations approaches infinity. It is important, therefore, to\n",
    "determine the **minimum number of samples** required to ensure a reasonable\n",
    "approximation to the target posterior density. Unfortunately, no\n",
    "universal threshold exists across all problems, so convergence must be\n",
    "assessed independently each time MCMC estimation is performed. The\n",
    "procedures for verifying convergence are collectively known as\n",
    "*convergence diagnostics*.\n",
    "\n",
    "There are a handful of easy-to-use methods for checking convergence. Since you cannot prove convergence, but only show lack of convergence, there is no single method that is foolproof. So, its best to look at a suite of diagnostics together. \n",
    "\n",
    "We will cover the canonical set of checks:\n",
    "\n",
    "- Variable plotting\n",
    "- Divergences\n",
    "- R-hat\n",
    "- Effective Sample Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traceplot \n",
    "\n",
    "Perhaps the most-used ArviZ plot is the traceplot, obtained via the `plot_trace` function. This is a simple plot that is a good quick check to make sure nothing is obviously wrong, and is usually the first diagnostic step you will take. You've seen these already: just the time series of samples for an individual variable.\n",
    "\n",
    "The `plot_trace` function from ArViZ by default generates a kernel density plot and a trace plot, with a different lines style for each chain of the simulation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_exp3, var_names=['a', 'b', 'alpha']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good example of a traceplot; the chains are mixing well and that there is no evidence of non-convergence. Notice that the chains are almost identical, indicating that the chains have converged to the same distribution.\n",
    "\n",
    "This is not surprising since the model is fairly simple and we tuned the sampler for 1000 iterations (the default in PyMC).\n",
    "\n",
    "Let's look at a trace from the model sampled with deliberately inadequate tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_exp3:\n",
    "    trace_exp3_bad = pm.sample(tune=10,random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we now have some warnings from the sampler that we did not see before.\n",
    "\n",
    "Looking at the traceplot, we see that the chains are not mixing well, and you can visually discriminate between the chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_exp3_bad, var_names=['a', 'b', 'alpha']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Scale Reduction: $\\hat{R}$\n",
    "\n",
    "Roughly, $\\hat{R}$ (*R-Hat*, or the *Gelman-Rubin statistic*) is the ratio of between-chain variance to within-chain variance. This diagnostic uses multiple chains to\n",
    "check for lack of convergence, and is based on the notion that if\n",
    "multiple chains have converged, by definition they should appear very\n",
    "similar to one another; if not, one or more of the chains has failed to\n",
    "converge.\n",
    "\n",
    "$\\hat{R}$ uses an analysis of variance approach to\n",
    "assessing convergence. That is, it calculates both the between-chain\n",
    "variance (B) and within-chain variance (W), and assesses whether they\n",
    "are different enough to worry about convergence. Assuming $m$ chains,\n",
    "each of length $n$, quantities are calculated by:\n",
    "\n",
    "$$\\begin{align}B &= \\frac{n}{m-1} \\sum_{j=1}^m (\\bar{\\theta}_{.j} - \\bar{\\theta}_{..})^2 \\\\\n",
    "W &= \\frac{1}{m} \\sum_{j=1}^m \\left[ \\frac{1}{n-1} \\sum_{i=1}^n (\\theta_{ij} - \\bar{\\theta}_{.j})^2 \\right]\n",
    "\\end{align}$$\n",
    "\n",
    "for each scalar estimand $\\theta$. Using these values, an estimate of\n",
    "the marginal posterior variance of $\\theta$ can be calculated:\n",
    "\n",
    "$$\\hat{\\text{Var}}(\\theta | y) = \\frac{n-1}{n} W + \\frac{1}{n} B$$\n",
    "\n",
    "Assuming $\\theta$ was initialized to arbitrary starting points in each\n",
    "chain, this quantity will overestimate the true marginal posterior\n",
    "variance. At the same time, $W$ will tend to underestimate the\n",
    "within-chain variance early in the sampling run. However, in the limit\n",
    "as $n \\rightarrow \n",
    "\\infty$, both quantities will converge to the true variance of $\\theta$.\n",
    "In light of this, $\\hat{R}$ monitors convergence using\n",
    "the ratio:\n",
    "\n",
    "$$\\hat{R} = \\sqrt{\\frac{\\hat{\\text{Var}}(\\theta | y)}{W}}$$\n",
    "\n",
    "This is called the **potential scale reduction**, since it is an estimate of\n",
    "the potential reduction in the scale of $\\theta$ as the number of\n",
    "simulations tends to infinity. In practice, we look for values of\n",
    "$\\hat{R}$ close to one (say, less than 1.1) to be confident that a\n",
    "particular estimand has converged. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(trace_exp3, var_names=['a', 'b', 'alpha'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effective Sample Size\n",
    "\n",
    "In general, samples drawn from MCMC algorithms will be autocorrelated. Unless the autocorrelation is very severe, this is not a big deal, other than the fact that autocorrelated chains may require longer sampling in order to adequately characterize posterior quantities of interest. The calculation of autocorrelation is performed for each lag $i=1,2,\\ldots,k$ (the correlation at lag 0 is, of course, 1) by: \n",
    "\n",
    "$$\\hat{\\rho}_i = 1 - \\frac{V_i}{2\\hat{\\text{Var}}(\\theta | y)}$$\n",
    "\n",
    "where $\\hat{\\text{Var}}(\\theta | y)$ is the same estimated variance as calculated for the Gelman-Rubin statistic, and $V_i$ is the variogram at lag $i$ for $\\theta$:\n",
    "\n",
    "$$\\text{V}_i = \\frac{1}{m(n-i)}\\sum_{j=1}^m \\sum_{k=i+1}^n (\\theta_{jk} - \\theta_{j(k-i)})^2$$\n",
    "\n",
    "The amount of correlation in an MCMC sample influences the **effective sample size** (ESS) of the sample. The ESS estimates how many *independent* draws contain the same amount of information as the *dependent* sample obtained by MCMC sampling.\n",
    "\n",
    "Given a series of samples $x_j$, the empirical mean is\n",
    "\n",
    "$$\n",
    "\\hat{\\mu} = \\frac{1}{n}\\sum_{j=1}^n x_j\n",
    "$$\n",
    "\n",
    "and the variance of the estimate of the empirical mean is \n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(\\hat{\\mu}) = \\frac{\\sigma^2}{n},\n",
    "$$\n",
    "where $\\sigma^2$ is the true variance of the underlying distribution.\n",
    "\n",
    "Then the effective sample size is defined as the denominator that makes this relationship still be true:\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(\\hat{\\mu}) = \\frac{\\sigma^2}{n_{\\text{eff}}}.\n",
    "$$\n",
    "\n",
    "The effective sample size is estimated using the partial sum:\n",
    "\n",
    "$$\\hat{n}_{eff} = \\frac{n}{1 + 2\\sum_{i=1}^T \\hat{\\rho}_i}$$\n",
    "\n",
    "where $T$ is the first odd integer such that $\\hat{\\rho}_{T+1} + \\hat{\\rho}_{T+2}$ is negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(trace_exp3_bad, var_names=['a', 'b', 'alpha'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Fraction of Missing Information\n",
    "\n",
    "The Bayesian fraction of missing information (BFMI) is a measure of how hard it is to\n",
    "sample level sets of the posterior at each iteration. Specifically, it quantifies **how well momentum resampling matches the marginal energy distribution**. \n",
    "\n",
    "$$\\text{BFMI} = \\frac{\\mathbb{E}_{\\pi}[\\text{Var}_{\\pi_{E|q}}(E|q)]}{\\text{Var}_{\\pi_{E}}(E)}$$\n",
    "\n",
    "$$\\widehat{\\text{BFMI}} = \\frac{\\sum_{i=1}^N (E_n - E_{n-1})^2}{\\sum_{i=1}^N (E_n - \\bar{E})^2}$$\n",
    "\n",
    "BFMI is essentially a measure of the association between the energy of a state and the energy of the next state, or more precisely, it compares the average squared change in energy between successive samples to the overall variance of the energy across all samples. The \"missing information\" refers to the information that the sampler fails to gain about the posterior because it cannot efficiently traverse the energy landscape.\n",
    "\n",
    "A small value indicates that the adaptation phase of the sampler was unsuccessful, and invoking the central limit theorem may not be valid. It indicates whether the sampler is able to *efficiently* explore the posterior distribution.\n",
    "\n",
    "Though there is not an established rule of thumb for an adequate threshold, values close to one are optimal. Reparameterizing the model is sometimes helpful for improving this statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.bfmi(trace_exp3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of diagnosting this phenomenon is by comparing the overall distribution of \n",
    "energy levels with the *change* of energy between successive samples. Ideally, they should be very similar.\n",
    "\n",
    "If the distribution of energy transitions is narrow relative to the marginal energy distribution, this is a sign of inefficient sampling, as many transitions are required to completely explore the posterior. On the other hand, if the energy transition distribution is similar to that of the marginal energy, this is evidence of efficient sampling, resulting in near-independent samples from the posterior.\n",
    "\n",
    "As an example, if we look at the energy plot of our eight schools model, the low BFMI values (which result in poor overlap in the energy distributions) suggest taht the sampler is having trouble exploring different energy levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_energy(trace_exp3);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Check Model Fit\n",
    "\n",
    "The second component of model checking, goodness of fit, is used to check the **internal validity** of the model, by comparing predictions from the model to the data used to fit the model.\n",
    "\n",
    "Convergence diagnostics are only the first step in the evaluation\n",
    "of MCMC model outputs -- It is possible for an entirely unsuitable model to converge, so additional steps are needed to ensure that the estimated model adequately fits the data. \n",
    "\n",
    "One intuitive way of evaluating model fit is to compare model predictions with the observations used to fit\n",
    "the model. In other words, the fitted model can be used to simulate data, and the distribution of the simulated data should resemble the distribution of the actual data.\n",
    "\n",
    "Fortunately, simulating data from the model is a natural component of the Bayesian modelling framework. Recall, from the discussion on prediction, the posterior predictive distribution:\n",
    "\n",
    "$$p(\\tilde{y}|y) = \\int p(\\tilde{y}|\\theta) f(\\theta|y) d\\theta$$\n",
    "\n",
    "Here, $\\tilde{y}$ represents some hypothetical new data that would be expected, taking into account the posterior uncertainty in the model parameters. \n",
    "\n",
    "Sampling from the posterior predictive distribution is easy in PyMC. The `sample_posterior_predictive` function draws posterior predictive samples from all of the observed variables in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_exp3:\n",
    "    # Draw sampels from posterior predictive\n",
    "    post_pred = pm.sample_posterior_predictive(trace_exp3.posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The degree to which simulated data correspond to observations can be evaluated visually. This allows for a qualitative comparison of model-based replicates and observations. If there is poor fit, the true value of the data may appear in the tails of the histogram of replicated data, while a good fit will tend to show the true data in high-probability regions of the posterior predictive distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.plot(post_pred.posterior_predictive['obs'].sel(chain=0).values.squeeze().T, color='0.5', alpha=.05)\n",
    "ax.plot(confirmed, color='r', label='data')\n",
    "ax.set(xlabel=\"Days since 100 cases\", \n",
    "       ylabel=\"Confirmed cases (log scale)\",\n",
    "       # ylim=(0, 100_000), \n",
    "       title=country, \n",
    "       yscale=\"log\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, that does not look terrible, the data is at least inside of what the model can produce. Let's look at residuals for systematic errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "resid = post_pred.posterior_predictive[\"obs\"].sel(chain=0) - confirmed\n",
    "ax.plot(resid.T, color=\"0.5\", alpha=.01);\n",
    "ax.set(ylim=(-50_000, 200_000), ylabel=\"Residual\",\n",
    "       xlabel=\"Days since 100 cases\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you see?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and forecasting\n",
    "\n",
    "We might also be interested in predicting on unseen or data, or, in the case time-series data like here, in forecasting. In `PyMC` you can do so easily using `pm.Data` nodes. What it allows you to do is define data to a PyMC model that you can later switch out for other data. That way, when you for example do posterior predictive sampling, it will generate samples into the future.\n",
    "\n",
    "Let's change our model to use `pm.Data` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model_exp4:\n",
    "    # pm.Data needs to be in the model context so that we can\n",
    "    # keep track of it.\n",
    "    # Then, we can then use it like any other array.\n",
    "    t_data = pm.Data('t', df_country['days_since_100'].to_numpy())\n",
    "    confirmed_data = pm.Data('confirmed', df_country['confirmed'].to_numpy())\n",
    "\n",
    "    # Intercept\n",
    "    a0 = pm.HalfNormal('a0', sigma=25)\n",
    "    a = pm.Deterministic('a', a0 + 100)\n",
    "\n",
    "    # Slope\n",
    "    b = pm.HalfNormal('b', sigma=0.2)\n",
    "\n",
    "    # Exponential regression\n",
    "    growth = a * (1 + b) ** t_data\n",
    "\n",
    "    # Likelihood\n",
    "    pm.NegativeBinomial('obs',\n",
    "                 growth,\n",
    "                 alpha=pm.Gamma(\"alpha\", mu=6, sigma=1),\n",
    "                 observed=confirmed_data)\n",
    "    \n",
    "    trace_exp4 = pm.sample(random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_exp4:\n",
    "    # Update our data containers.\n",
    "    # Recall that because confirmed is observed, we do not\n",
    "    # need to specify any data, as that is only needed\n",
    "    # during inference. But do have to update it to match\n",
    "    # the shape.\n",
    "    pm.set_data({'t': np.arange(60),\n",
    "                 'confirmed': np.zeros(60, dtype='int')})\n",
    "\n",
    "    post_pred = pm.sample_posterior_predictive(trace_exp4.posterior)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we held data back before, we can now see how the predictions of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.plot(post_pred.posterior_predictive['obs'].sel(chain=0).squeeze().values.T, color='0.5', alpha=.05)\n",
    "ax.plot(df_country[\"confirmed\"].to_numpy(), color='r', label=\"in-sample\")\n",
    "df_confirmed = df.filter((pl.col('country') == country) & (pl.col('date') <= pl.lit(date).str.to_datetime()))['confirmed']\n",
    "ax.plot(np.arange(29, len(df_confirmed)), df_confirmed[29:].to_numpy(),\n",
    "        color='b', label=\"out-of-sample\")\n",
    "ax.set(xlabel='Days since 100 cases', ylabel='Confirmed cases',\n",
    "       title=country, yscale=\"log\");\n",
    "ax.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Improve model\n",
    "\n",
    "As an alternative to exponential growth, let's consider a logistic growth model:\n",
    "\n",
    "$$\n",
    "y(t) = \\frac{c}{1 + a e^{-b t}}\n",
    "$$\n",
    "\n",
    "where `c` is the carrying capacity, `a` controls the initial value, and `b` is the growth rate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic model\n",
    "\n",
    "<img src=\"https://s3-us-west-2.amazonaws.com/courses-images-archive-read-only/wp-content/uploads/sites/924/2015/11/25202016/CNX_Precalc_Figure_04_07_0062.jpg\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country = df.filter((pl.col('country') == country) & (pl.col('date') <= pl.lit(date).str.to_datetime()))\n",
    "\n",
    "with pm.Model() as logistic_model:\n",
    "    t_data = pm.Data('t', df_country[\"days_since_100\"].to_numpy())\n",
    "    confirmed_data = pm.Data('confirmed', df_country[\"confirmed\"].to_numpy())\n",
    "\n",
    "    # Intercept\n",
    "    a0 = pm.HalfNormal('a0', sigma=25)\n",
    "    intercept = pm.Deterministic('intercept', a0 + 100)\n",
    "\n",
    "    # Slope\n",
    "    b = pm.HalfNormal('b', sigma=0.2)\n",
    "    \n",
    "    carrying_capacity = pm.Uniform('carrying_capacity',\n",
    "                                   lower=1_000,\n",
    "                                   upper=80_000_000)\n",
    "    # Transform carrying_capacity to a\n",
    "    a = carrying_capacity / intercept - 1\n",
    "\n",
    "    # Logistic\n",
    "    growth = carrying_capacity / (1 + a * pm.math.exp(-b * t_data))\n",
    "\n",
    "    # Likelihood\n",
    "    pm.NegativeBinomial('obs',\n",
    "                 growth,\n",
    "                 alpha=pm.Gamma(\"alpha\", mu=6, sigma=1),\n",
    "                 observed=confirmed_data)\n",
    "    \n",
    "    prior_pred = pm.sample_prior_predictive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.plot(prior_pred.prior_predictive['obs'].squeeze().values.T, color=\"0.5\", alpha=.1)\n",
    "ax.set(title=\"Prior predictive\",\n",
    "       xlabel=\"Days since 100 cases\",\n",
    "       ylabel=\"Positive cases\",\n",
    "       yscale=\"log\",\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with logistic_model:\n",
    "    # Inference\n",
    "    trace_logistic = pm.sample(random_seed=RANDOM_SEED, target_accept=0.9)\n",
    "    \n",
    "    # Sample posterior predcitive\n",
    "    pm.sample_posterior_predictive(trace_logistic, extend_inferencedata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_logistic);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.plot(trace_logistic.posterior_predictive['obs'].sel(chain=0).squeeze().values.T, color='0.5', alpha=.05)\n",
    "ax.plot(df_confirmed.to_numpy(), color='r')\n",
    "ax.set(xlabel='Days since 100 cases', ylabel='Confirmed cases',\n",
    "       title=country);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "resid = trace_logistic.posterior_predictive[\"obs\"].sel(chain=0).squeeze().values - df_confirmed.to_numpy()\n",
    "ax.plot(resid.T, color=\"0.5\", alpha=.01);\n",
    "ax.set(ylabel=\"Residual\",\n",
    "       xlabel=\"Days since 100 cases\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model comparison\n",
    "\n",
    "While it seems clear that the logistic model is a better fit to the data, we can also use one of several model comparison techniques to make this more rigorous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leave-one-out Cross-validation (LOO)\n",
    "\n",
    "LOO cross-validation is an estimate of the **out-of-sample predictive fit**. In cross-validation, the data are repeatedly partitioned into training and holdout sets, iteratively fitting the model with the former and evaluating the fit with the holdout data. \n",
    "\n",
    "The estimate of out-of-sample predictive fit from applying LOO cross-validation to a Bayesian model is:\n",
    "\n",
    "$$lppd_{loo} = \\sum_{i=1}^N \\log p_{post(-i)}(y_i) =  \\sum_{i=1}^N \\log \\left(\\frac{1}{S} \\sum_{s=1}^S p(y_i| \\theta^{(is)})\\right)$$\n",
    "\n",
    "so, each prediction is conditioned on $N-1$ data points, which induces an **underestimation of the predictive fit** for smaller $N$. The resulting estimate of effective samples size is:\n",
    "\n",
    "$$p_{loo} = lppd - lppd_{loo}$$\n",
    "\n",
    "But, using cross-validation for a Bayesian model, fitting $N$ copies of the model under different subsets of the data is computationally expensive. However, Vehtari *et al.* (2016) introduced an efficient computation of LOO from MCMC sample, which are corrected using **Pareto-smoothed importance sampling (PSIS)** to provide an estimate of point-wise out-of-sample prediction accuracy.\n",
    "\n",
    "This involves estimating the importance sampling LOO predictive distribution\n",
    "\n",
    "$$p(\\tilde{y}_i | y_{-i}) \\approx \\frac{\\sum_{s=1}^S w_i(\\theta^{(s)}) p(\\tilde{y}_i|\\theta^{(s)})}{\\sum_{s=1}^S w_i(\\theta^{(s)})}$$\n",
    "\n",
    "where the importance weights are:\n",
    "\n",
    "$$w_i(\\theta^{(s)}) = \\frac{1}{p(y_i | \\theta^{(s)})} \\propto \\frac{p(\\theta^{(s)}|y_{-i})}{p(\\theta^{(s)}|y)}$$\n",
    "\n",
    "The predictive distribution evaluated at the held-out point is then:\n",
    "\n",
    "$$p(y_i | y_{-i}) \\approx \\frac{1}{\\frac{1}{S} \\sum_{s=1}^S \\frac{1}{p(y_i | \\theta^{(s)})}}$$\n",
    "\n",
    "However, the posterior is likely to have a *smaller variance and thinner tails* than the LOO posteriors, so this approximation induces instability due to the fact that the importance ratios can have high or infinite variance.\n",
    "\n",
    "To deal with this instability, a generalized **Pareto distribution** fit to the upper tail of the distribution of the importance ratios can be used to construct a test for a finite importance ratio variance. If the test suggests the variance is infinite then importance sampling is halted.\n",
    "\n",
    "LOO using Pareto-smoothed importance sampling is implemented in ArviZ in the `loo` function.\n",
    "\n",
    "> Model comparison requires the log-likelihoods of the respective models. For efficiency, these are not computed automatically, so we need to manually calculate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with logistic_model:\n",
    "    pm.compute_log_likelihood(trace_logistic)\n",
    "\n",
    "az.loo(trace_logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this a good model? It is really only interpretable relative to another model fit to the same data.\n",
    "\n",
    "First we need to sample from `model_exp4` with the full dataset, and compute the log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_exp4:\n",
    "    pm.set_data({\"t\": df_country[\"days_since_100\"].to_numpy(),\n",
    "                 \"confirmed\": df_country[\"confirmed\"].to_numpy()})\n",
    "    \n",
    "    trace_exp4_full = pm.sample(random_seed=RANDOM_SEED)\n",
    "    pm.compute_log_likelihood(trace_exp4_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the ArviZ `compare` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_compare(az.compare({\"exp4\": trace_exp4_full, \n",
    "            \"logistic\": trace_logistic}))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the logistic model provides a much better fit to the data. \n",
    "\n",
    "Although there is still some small bias in the residuals but overall we might think our model is quite good. Let's see how it does on a different country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'US'\n",
    "df_country = df.filter(pl.col('country') == country).filter(pl.col('date') <= pl.lit(date).str.to_datetime())\n",
    "df_confirmed = df_country[\"confirmed\"]\n",
    "px.line(\n",
    "    x=df_country[\"days_since_100\"].to_numpy(),\n",
    "    y=df_country[\"confirmed\"].to_numpy(),\n",
    "    title=f'{country} - Confirmed Cases',\n",
    "    labels={'x': 'Days since 100 cases', 'y': 'Confirmed cases'}\n",
    ").update_layout(width=800, height=600)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the data looks quite different. Let's see how our logistic model fits this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_confirmed = df.loc[lambda x: (x.country == country), 'confirmed']\n",
    "df_confirmed = df.filter(pl.col('country') == country).filter(pl.col('date') <= pl.lit(date).str.to_datetime())['confirmed']\n",
    "\n",
    "with pm.Model() as logistic_model:\n",
    "    t_data = pm.Data('t', df_country[\"days_since_100\"].to_numpy())\n",
    "    confirmed_data = pm.Data('confirmed', df_country[\"confirmed\"].to_numpy())\n",
    "\n",
    "    # Intercept\n",
    "    a0 = pm.HalfNormal('a0', sigma=25)\n",
    "    intercept = pm.Deterministic('intercept', a0 + 100)\n",
    "\n",
    "    # Slope\n",
    "    b = pm.HalfNormal('b', sigma=0.2)\n",
    "    \n",
    "    carrying_capacity = pm.Uniform('carrying_capacity',\n",
    "                                   lower=1_000,\n",
    "                                   upper=100_000_000)\n",
    "    # Transform carrying_capacity to a\n",
    "    a = carrying_capacity / intercept - 1\n",
    "\n",
    "    # Logistic\n",
    "    growth = carrying_capacity / (1 + a * pm.math.exp(-b * t_data))\n",
    "\n",
    "    # Likelihood\n",
    "    pm.NegativeBinomial('obs',\n",
    "                 growth,\n",
    "                 alpha=pm.Gamma(\"alpha\", mu=6, sigma=1),\n",
    "                 observed=confirmed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with logistic_model:\n",
    "    trace_logistic_us = pm.sample(random_seed=RANDOM_SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already we see some problems with sampling which should make us suspicious that this model might not be the best for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_logistic_us);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with logistic_model:\n",
    "    pm.sample_posterior_predictive(\n",
    "        trace_logistic_us, extend_inferencedata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.plot(trace_logistic_us.posterior_predictive['obs'].sel(chain=0).squeeze().values.T, color='0.5', alpha=.05)\n",
    "ax.plot(df_confirmed.to_numpy(), color='r')\n",
    "ax.set(xlabel='Days since 100 cases', ylabel='Confirmed cases',\n",
    "       title=country);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model is not a great fit to this data. Why? What assumptions does the model make about the spread of COVID-19?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "papermill": {
   "duration": 1049.211513,
   "end_time": "2020-03-27T06:26:42.767376",
   "environment_variables": {},
   "exception": null,
   "input_path": "2020-03-16-covid19_growth_bayes.ipynb",
   "output_path": "2020-03-16-covid19_growth_bayes.ipynb",
   "parameters": {},
   "start_time": "2020-03-27T06:09:13.555863",
   "version": "2.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
