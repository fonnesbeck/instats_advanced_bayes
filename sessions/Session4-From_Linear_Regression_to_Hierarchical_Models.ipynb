{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Linear Regression to Hierarchical Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import polars as pl\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import xarray as xr\n",
    "\n",
    "# Visual style\n",
    "az.style.use('arviz-doc')\n",
    "pio.templates.default = 'plotly_white'\n",
    "\n",
    "# Plotly defaults\n",
    "px.defaults.template = 'plotly_white'\n",
    "px.defaults.width = 900\n",
    "px.defaults.height = 500\n",
    "_base = pio.templates['plotly_white']\n",
    "_tmpl = go.layout.Template(_base)\n",
    "_tmpl.layout.hovermode = 'x unified'\n",
    "pio.templates['hoverx'] = _tmpl\n",
    "pio.templates.default = 'plotly_white+hoverx'\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 20090425\n",
    "RNG = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", module=\"mkl_fft\")\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Linear Models (GLMs)\n",
    "\n",
    "In the previous section, we worked with linear regression where the outcome variable was continuous and could be reasonably modeled with a normal distribution. However, many real-world outcomes don't fit this comfortable framework. Consider these common scenarios:\n",
    "\n",
    "- **Count data**: Number of accidents at an intersection, disease cases in a population, customer complaints per day, goals scored in a match\n",
    "- **Binary outcomes**: Customer churn (yes/no), disease presence (positive/negative), loan default (yes/no), email clicked (yes/no)\n",
    "- **Proportions**: Market share, exam pass rates, survey response rates, survival probabilities\n",
    "- **Strictly positive continuous**: Waiting times, insurance claim amounts, reaction times, rainfall amounts\n",
    "- **Ordinal data**: Customer satisfaction (1-5 stars), pain levels, educational attainment levels\n",
    "\n",
    "Trying to force these into a linear regression framework with normal errors often fails spectacularly:\n",
    "- Predictions can be nonsensical (negative counts, probabilities > 1)\n",
    "- Variance assumptions are violated (variance often depends on mean)\n",
    "- Inference is invalid (confidence intervals include impossible values)\n",
    "- Relationships are misspecified (effects are often multiplicative, not additive)\n",
    "\n",
    "Generalized Linear Models (GLMs) elegantly solve these problems by extending the linear regression framework in two key ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisson regression: Unbounded count data\n",
    "\n",
    "This model is inspired by [a project by Ian Osvald](http://ianozsvald.com/2016/05/07/statistically-solving-sneezes-and-sniffles-a-work-in-progress-report-at-pydatalondon-2016/), which is concerned with understanding the various effects of external environmental factors upon the allergic sneezing of a test subject.\n",
    "\n",
    "We're going to work with simpler data than the original study, which will allow you to clearly see the modeling stakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sneezes = pl.read_csv(\"../data/poisson_sneeze.csv\")\n",
    "sneezes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The subject sneezes N times per day, recorded as `nsneeze (int)`. The data are aggregated per day, to yield a total count of sneezes on that day.\n",
    "+ The subject may or may not drink alcohol during that day, recorded as `alcohol (boolean)`\n",
    "+ The subject may or may not take an antihistamine medication during that day, recorded as `meds (boolean)`\n",
    "\n",
    "We assume that sneezing occurs at some baseline rate, which increases if an antihistamine is not taken, and further increases if alcohol is consumed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data and set up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=2, cols=2, \n",
    "                   subplot_titles=[\"meds=0, alcohol=0\", \"meds=1, alcohol=0\",\n",
    "                                  \"meds=0, alcohol=1\", \"meds=1, alcohol=1\"])\n",
    "\n",
    "for i, alc in enumerate([0, 1]):\n",
    "    for j, med in enumerate([0, 1]):\n",
    "        filtered_data = sneezes.filter((pl.col('alcohol') == alc) & (pl.col('meds') == med))\n",
    "        counts = filtered_data.get_column('nsneeze').value_counts().sort('nsneeze')\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(x=counts.get_column('nsneeze'), y=counts.get_column('count'), marker_color='blue'),\n",
    "            row=i+1, col=j+1\n",
    "        )\n",
    "\n",
    "fig.update_layout(\n",
    "    height=600, \n",
    "    width=800,\n",
    "    showlegend=False,\n",
    "    title_text=\"Distribution of Sneezes by Medication and Alcohol\"\n",
    ").update_xaxes(\n",
    "    title_text=\"Number of Sneezes\"\n",
    ").update_yaxes(title_text=\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The usual way of thinking about data coming from a Poisson distribution is as number of occurences of an event in a *given timeframe*. Here *number of sneezes per day*. The intensity parameter $\\lambda$ specifies how many occurences we expect.\n",
    "\n",
    "A nice property of the Poisson is that it's defined with only one parameter $\\lambda$, which describes both the mean and variance of the Poisson, can be interpreted as the rate of events per unit -- here, if we inferred $\\lambda = 2.8$, that would mean the subject is thought to sneeze about 2.8 times per day (implying in addition a $2.8$ variance).\n",
    "\n",
    "In statistical terms, that means our likelihood is \n",
    "\n",
    "$$ Y_{\\text{sneeze}} \\sim \\mathrm{Poisson}(\\lambda)$$\n",
    "\n",
    "> The Poisson probability mass function is:\n",
    ">\n",
    "> $$ P(Y=k|\\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!} $$\n",
    ">\n",
    "> where $k$ is the number of occurrences (sneezes in our case) and $\\lambda$ is the rate parameter.\n",
    ">\n",
    "\n",
    "\n",
    "Now, we need a prior on $\\lambda$ ...\n",
    "\n",
    "This is where the regression component comes in: remember that we want to infer the effect of meds and alcohol on the number of sneezes. So, we can use the usual linear regression formula we are familiar with: \n",
    "\n",
    "$$\\lambda = \\alpha + \\beta_{\\text{meds}} * \\text{meds} + \\beta_{\\text{alcohol}} * \\text{alcohol}$$\n",
    "\n",
    "We will specify **weakly-informative priors** on the model latent variables.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\alpha &\\sim \\mathrm{Normal}(0, 5)\\\\\n",
    "\\beta_{\\text{meds}}, \\beta_{\\text{alcohol}} &\\sim \\mathrm{Normal}(0, 1)\\\\\n",
    "\\lambda &= \\alpha + \\beta_{\\text{meds}} * \\text{meds} + \\beta_{\\text{alcohol}} * \\text{alcohol}\\\\\n",
    "Y_{\\text{sneeze}} &\\sim \\mathrm{Poisson}(\\lambda)\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COORDS = {\n",
    "    \"regressor\": [\"meds\", \"alcohol\"], \n",
    "    \"obs_idx\": range(len(sneezes))\n",
    "}\n",
    "\n",
    "M, A, S = sneezes.select([\"meds\", \"alcohol\", \"nsneeze\"]).to_numpy().T\n",
    "\n",
    "with pm.Model(coords=COORDS) as m_sneeze:\n",
    "    # weakly informative Normal Priors\n",
    "    a = pm.Normal(\"intercept\", mu=0, sigma=5)\n",
    "    b = pm.Normal(\"slopes\", mu=0, sigma=1, dims=\"regressor\")\n",
    "\n",
    "    # define linear model\n",
    "    mu = pm.Deterministic(\"mu\", a + b[0] * M + b[1] * A, dims=\"obs_idx\")\n",
    "\n",
    "    ## Define Poisson likelihood\n",
    "    y = pm.Poisson(\"y\", mu=mu, observed=S, dims=\"obs_idx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting\n",
    "\n",
    "Most GLMs will be fit using the NUTS step method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with m_sneeze:\n",
    "    trace_sneeze = pm.sample(random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(trace_sneeze, var_names=[\"intercept\", \"slopes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the model sampled to completion, close inspection of the model reveals an issue that could have caused the sampler to fail. Can you spot it? In previous versions of PyMC, this model would have failed to sample.\n",
    "\n",
    "Propose a more robust version below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords=COORDS) as m_sneeze:\n",
    "    # weakly informative Normal Priors\n",
    "    a = pm.Normal(\"intercept\", mu=0, sigma=5)\n",
    "    b = pm.Normal(\"slopes\", mu=0, sigma=1, dims=\"regressor\")\n",
    "\n",
    "    # define linear model\n",
    "    mu = pm.math.exp(a + b[0] * M + b[1] * A)\n",
    "\n",
    "    ## Define Poisson likelihood\n",
    "    y = pm.Poisson(\"y\", mu=mu, observed=S, dims=\"obs_idx\")\n",
    "\n",
    "    trace_sneeze = pm.sample(random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before looking at the results, let's take a step back: the trick we used with the exponential transformation is an example of a generalized linear model. The exponential here is called **a link function**, and it's used to map the **output of our linear model** (a priori allowed to live in $(-\\infty, \\infty)$,\n",
    "\n",
    "### Link functions\n",
    "\n",
    "In Generalized Linear Models (GLMs), link functions play a crucial role in connecting the linear predictor to the response variable. The concept of link functions arises from the need to model the relationship between the mean of the response variable and the linear predictor, which can take any real value.\n",
    "\n",
    "The link function transforms the linear predictor to ensure that the predicted values of the response variable are within a valid range and satisfy the distributional assumptions of the response variable. It maps the linear predictor to the space of the relevant parameter of the response distribution.\n",
    "\n",
    "Different types of GLMs use different link functions based on the nature of the response variable and the desired distribution. Some commonly used link functions include:\n",
    "\n",
    "1. Identity Link: This link function is used for continuous response variables and maintains a linear relationship between the linear predictor and the mean of the response variable.\n",
    "\n",
    "2. Logit Link: This link function is used for binary response variables and maps the linear predictor to the probability of success in a logistic regression model. It ensures that the predicted probabilities are between 0 and 1.\n",
    "\n",
    "3. Log Link: This link function is used for count data and maps the linear predictor to the mean of a Poisson distribution. It ensures that the predicted mean is positive.\n",
    "\n",
    "4. Inverse Link: This link function is used for modeling the mean of a Gamma distribution and maps the linear predictor to the inverse of the mean.\n",
    "\n",
    "The choice of the link function depends on the nature of the response variable and the assumptions of the distribution. The link function allows for flexible modeling of the relationship between the linear predictor and the response variable, enabling the estimation of regression coefficients and making predictions within the appropriate range of the response distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking our inferences\n",
    "\n",
    "Now is the time to check if our model's results are credible, via posterior predictive checks, to which we were introduced in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with m_sneeze:\n",
    "\n",
    "    # Get posterior predictive samples, and add them to the InferenceData object\n",
    "    trace_sneeze.extend(pm.sample_posterior_predictive(trace_sneeze))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the posterior predictive checks for all the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sneeze_predictions(idata):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "    az.plot_ppc(\n",
    "        idata,\n",
    "        ax=axs[0, 0],\n",
    "        coords={\n",
    "            \"obs_idx\": np.where(np.logical_and(sneezes.get_column(\"alcohol\").to_numpy() == 0, sneezes.get_column(\"meds\").to_numpy() == 0))\n",
    "        },\n",
    "    )\n",
    "    az.plot_ppc(\n",
    "        idata,\n",
    "        ax=axs[0, 1],\n",
    "        coords={\n",
    "            \"obs_idx\": np.where(np.logical_and(sneezes.get_column(\"alcohol\").to_numpy() == 0, sneezes.get_column(\"meds\").to_numpy() == 1))\n",
    "        },\n",
    "    )\n",
    "    az.plot_ppc(\n",
    "        idata,\n",
    "        ax=axs[1, 0],\n",
    "        coords={\n",
    "            \"obs_idx\": np.where(np.logical_and(sneezes.get_column(\"alcohol\").to_numpy() == 1, sneezes.get_column(\"meds\").to_numpy() == 0))\n",
    "        },\n",
    "    )\n",
    "    az.plot_ppc(\n",
    "        idata,\n",
    "        ax=axs[1, 1],\n",
    "        coords={\n",
    "            \"obs_idx\": np.where(np.logical_and(sneezes.get_column(\"alcohol\").to_numpy() == 1, sneezes.get_column(\"meds\").to_numpy() == 1))\n",
    "        },\n",
    "    )\n",
    "    axs[0, 0].set_title(\"No alcohol : No meds\")\n",
    "    axs[0, 1].set_title(\"No alcohol : Meds\")\n",
    "    axs[1, 0].set_title(\"Alcohol : No meds\")\n",
    "    axs[1, 1].set_title(\"Alcohol : Meds\")\n",
    "    return fig, axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sneeze_predictions(trace_sneeze);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the model is in the ballpark of the actual data, it is not perfect. The model is underestimating the variance in the data. This is a common issue with Poisson regression, as the variance is constrained to be equal to the mean. When the data's mean and variance are not similar, a Poisson regression will underestimate the variance compared to the true variance observed in the data.\n",
    "\n",
    "This behavior is quite common with Poisson regression: it often underestimates the variation in the data, simply because real data are more dispersed than our regression expects -- in these cases, data are said to be \"overdispersed\".\n",
    "\n",
    "This phenomenon is particularly acute with the Poisson, because as we have seen its variance is mathematically constrained to be equal to its mean. So, when the data's mean and variance are not similar, a Poisson regression will get the variance estimate wrong when compared to the true variance observed in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convince ourselves, let's compare our data's mean and variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sneezes.group_by([\"meds\", \"alcohol\"]).agg([\n",
    "    pl.col(\"nsneeze\").mean().alias(\"mean\"),\n",
    "    pl.col(\"nsneeze\").var().alias(\"var\")\n",
    "]).sort([\"meds\", \"alcohol\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that for each combination of `alcohol` and `meds`, the variance of `nsneeze` is higher than the mean!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Gamma-Poisson Model\n",
    "\n",
    "Gamma-Poisson (aka [Negative binomial](https://en.wikipedia.org/wiki/Negative_binomial_distribution)) regression is used to model overdispersion in count data. The Gamma-Poisson distribution can be thought of as a Poisson distribution whose rate parameter is gamma distributed, so that rate parameter can be adjusted to account for the increased variance. If you want more details about these models (a.k.a. \"continuous mixture models\"), I refer you to chapter 12 of [Richard McElreath's excellent _Statistical Rethinking_](https://nbviewer.jupyter.org/github/pymc-devs/resources/blob/master/Rethinking_2/Chp_12.ipynb).\n",
    "\n",
    "In addition to the Poisson rate, $\\lambda$, Gamma-Poisson distributions are parametrized an additional overdispersion parameter, $\\alpha$, which controls the shape of the Gamma distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The Gamma-Poisson distribution_\n",
    "\n",
    "We start with a random variable $Y$ that follows a Poisson distribution with rate $\\lambda$. Turns out $\\lambda$ is also random, and it follows a gamma distribution with parameters $\\mu$ and $\\alpha$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Y &\\sim \\text{Poisson}(\\lambda) \\\\\n",
    "\\lambda &\\sim \\text{Gamma}\\left(\\mu, \\alpha\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "We can marginalize over $\\lambda$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(y \\mid \\mu, \\alpha) &= \\int_0^{\\infty}{p(y \\mid \\lambda) p(\\lambda \\mid \\mu, \\alpha) d\\lambda} \\\\\n",
    "&= \\binom{y + \\alpha - 1}{y}{\\left(\\frac{\\alpha}{\\mu + \\alpha}\\right)}^\\alpha {\\left(\\frac{\\mu}{\\mu + \\alpha}\\right)}^y\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "The above describes the probability mass function of a Gamma-Poisson distribution, then we can say \n",
    "\n",
    "$$\n",
    "Y \\sim \\text{GammaPoisson}(\\mu, \\alpha)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is it useful?\n",
    "\n",
    "Well, it relieves us from the previous constraint of our Poisson Distribution, to fix the **mean** to the **variance**.\n",
    "\n",
    "<br> </br>\n",
    "\n",
    "<center>\n",
    "  <img src=\"images/poisson_gamma_poisson_drake.png\" style=\"width:500px\"; />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The common name for this type of model is [negative binomial](https://en.wikipedia.org/wiki/Negative_binomial_distribution) regression.  The negative binomial distribution has two parameters: the mean $\\mu$ and the overdispersion parameter $\\alpha$. The mean $\\mu$ is the rate parameter of the Poisson distribution, and the overdispersion parameter $\\alpha$ controls the variance of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the following model...\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\beta_{\\text{intercept}} &\\sim \\mathrm{Normal}(0, 5)\\\\\n",
    "\\beta_{\\text{alcohol}} &\\sim \\mathrm{Normal}(0, 1)\\\\\n",
    "\\beta_{\\text{meds}} &\\sim \\mathrm{Normal}(0, 1) \\\\\n",
    "\\alpha &\\sim \\mathrm{Exponential}(1) \\\\\n",
    "\\mu_i &= \\exp(\\beta_{\\text{intercept}} + \\beta_{\\text{meds}} \\text{meds}_i + \\beta_{\\text{alcohol}} \\text{alcohol}_i) \\\\\n",
    "Y \\mid \\mu_i, \\alpha &\\sim \\mathrm{NegativeBinomial}(\\mu_i, \\alpha) \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this likelihood in PyMC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords=COORDS) as m_sneeze_gp:\n",
    "    # weakly informative priors\n",
    "    a = pm.Normal(\"intercept\", mu=0, sigma=5)\n",
    "    b = pm.Normal(\"slopes\", mu=0, sigma=1, dims=\"regressor\")\n",
    "    alpha = pm.Exponential(\"alpha\", 1.0)\n",
    "\n",
    "    # define linear model\n",
    "    mu = pm.math.exp(a + b[0] * M + b[1] * A)\n",
    "\n",
    "    ## likelihood\n",
    "    y = pm.NegativeBinomial(\"y\", mu=mu, alpha=alpha, observed=S, dims=\"obs_idx\")\n",
    "\n",
    "    trace_sneeze_gp = pm.sample(random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_sneeze_gp);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling went well; let's quickly check the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with m_sneeze_gp:\n",
    "    trace_sneeze_gp.extend(pm.sample_posterior_predictive(trace_sneeze_gp))\n",
    "\n",
    "plot_sneeze_predictions(trace_sneeze_gp);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Interaction effect\n",
    "\n",
    "The predictions look much better than before, **but** we can see there is bias relative to the observations in some groups. For example, in the plot for no alcohol with medication the model is biased to predict fewer sneezes than we actually observe in the medication condition, and the opposite phenomenon in the no medication condition. \n",
    "\n",
    "This suggests that we are missing some sort of **interaction effect** between medication and alcohol consumption. The thing is that our model is only able to account for the mean sneezes across both conditions.\n",
    "\n",
    "Try your hand at adding an interaction term to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COORDS = {\n",
    "\t\t\"regressor\": [\"meds\", \"alcohol\"], \n",
    "\t\t\"obs_idx\": range(len(sneezes))\n",
    "}\n",
    "with pm.Model(coords=COORDS) as m_sneeze_inter:\n",
    "\n",
    "    # weakly informative priors\n",
    "    a = pm.Normal(\"intercept\", mu=0, sigma=5)\n",
    "    b = pm.Normal(\"slopes\", mu=0, sigma=1, dims=\"regressor\")\n",
    "    alpha = pm.Exponential(\"alpha\", 1.0)\n",
    "\n",
    "    # define linear model\n",
    "    mu = pm.math.exp(a + b[0] * M + b[1] * A)\n",
    "\n",
    "    ## likelihood\n",
    "    y = pm.NegativeBinomial(\"y\", mu=mu, alpha=alpha, observed=S, dims=\"obs_idx\")\n",
    "\n",
    "    trace_sneeze_inter = pm.sample(random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the slope for the interaction is reliably negative, meaning that taking meds when drinking alcohol will still tame the effects of the latter on sneezing and thus decrease the number of sneezes compared to not taking meds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with m_sneeze_inter:\n",
    "    trace_sneeze_inter.extend(pm.sample_posterior_predictive(trace_sneeze_inter))\n",
    "\n",
    "plot_sneeze_predictions(trace_sneeze_inter);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the interaction term appears to have removed the biases, and the predictions look much better across the board."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Generalized Linear Models \n",
    "\n",
    "Poisson and negative binomial regressions are particular types of **Generalized Linear Model**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Models\n",
    "\n",
    "We assume the conditional distribution of the response variable is a normal distribution. We model the mean of that normal distribution with a linear combination of the predictors. Mathematically, we have\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\pmb{\\beta} &\\sim \\mathcal{P}_{\\pmb{\\beta}} \\\\\n",
    "\\sigma &\\sim \\mathcal{P}_\\sigma \\\\\n",
    "\\mu_i &= \\beta_0 + \\beta_1 X_{1i} + \\cdots + \\beta_p X_{pi} \\\\\n",
    "Y_i \\mid \\mu_i, \\sigma &\\sim \\text{Normal}(\\mu_i, \\sigma)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\mathcal{P}_{\\pmb{\\beta}}$ is the joint prior for the regression coefficients and $\\mathcal{P}_\\sigma$ is the prior on the residual standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Linear Models\n",
    "\n",
    "In Generalized Linear Models, we are not restricted to normal likelihoods and we model a function of the mean with a linear combination of the predictors.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\pmb{\\beta} &\\sim \\mathcal{P}_{\\pmb{\\beta}} \\\\\n",
    "\\pmb{\\theta} &\\sim \\mathcal{P}_{\\pmb{\\theta}} \\\\\n",
    "g(\\mu_i) &= \\eta_i = \\beta_0 + \\beta_1 X_{1i} + \\cdots + \\beta_p X_{pi} \\\\\n",
    "Y_i \\mid \\mu_i, \\pmb{\\theta} &\\sim \\mathcal{D}(\\mu_i, \\pmb{\\theta})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Which consists of:\n",
    "\n",
    "* $\\eta_i = \\beta_0 + \\beta_1 X_{1i} + \\cdots + \\beta_p X_{pi}$ is the **linear predictor**\n",
    "* $g$ is the **link function**\n",
    "    * In the Poisson regression model $g$ is the $\\log$ function.\n",
    "    * This point raises a lot of questions, as we directly work with the inverse link function $g^{-1}$ ($\\exp$ in the previous case)\n",
    "    * $g: \\Omega \\to \\mathbb{R}$\n",
    "    * $g^{-1}: \\mathbb{R} \\to \\Omega$\n",
    "    * $\\Omega$ is the space of the mean parameter\n",
    "* $\\mathcal{D}$ is the **sampling distribution**\n",
    "    * The conditional distribution of the response variable $Y$\n",
    "    * Is not necessarily normal\n",
    "\n",
    "Linear models are specific case of generalized linear models where $\\mathcal{D} \\equiv \\mathcal{N}$ and $g = I$ (*i.e.* the identity function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical or multilevel modeling is another generalization of regression modeling.\n",
    "\n",
    "*Multilevel models* are regression models in which the constituent model parameters are given **probability models**. This implies that model parameters are allowed to **vary by group**.\n",
    "\n",
    "Observational units are often naturally **clustered**. Clustering induces dependence between observations, despite random sampling of clusters and random sampling within clusters.\n",
    "\n",
    "A *hierarchical model* is a particular multilevel model where parameters are nested within one another.\n",
    "\n",
    "Some multilevel structures are not hierarchical. \n",
    "\n",
    "* e.g. \"country\" and \"year\" are not nested, but may represent separate, but overlapping, clusters of parameters\n",
    "\n",
    "We will motivate this topic using an environmental epidemiology example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Radon contamination (Gelman and Hill 2006)\n",
    "\n",
    "Radon is a radioactive gas that enters homes through contact points with the ground. It is a carcinogen that is the primary cause of lung cancer in non-smokers. Radon levels vary greatly from household to household.\n",
    "\n",
    "![radon](https://www.cgenarchive.org/uploads/2/5/2/6/25269392/7758459_orig.jpg)\n",
    "\n",
    "The EPA did a study of radon levels in 80,000 houses. There are two important predictors:\n",
    "\n",
    "* measurement in basement or first floor (radon higher in basements)\n",
    "* county uranium level (positive correlation with radon levels)\n",
    "\n",
    "We will focus on modeling radon levels in Minnesota.\n",
    "\n",
    "The hierarchy in this example is households within county."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the data from a local file, and extract Minnesota's data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original data exists as several independent datasets, which we will import, merge, and process here. First is the data on measurements from individual homes from across the United States. We will extract just the subset from Minnesota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srrs2 = pl.read_csv('../data/srrs2.dat')\n",
    "\n",
    "srrs2 = srrs2.rename({col: col.strip() for col in srrs2.columns})\n",
    "srrs_mn = srrs2.filter(pl.col(\"state\") == \"MN\")\n",
    "srrs_mn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, obtain the county-level predictor, uranium, by combining two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cty = pl.read_csv('../data/cty.dat')\n",
    "\n",
    "srrs_mn = srrs_mn.with_columns(\n",
    "    (pl.col(\"stfips\").str.strip_chars().cast(pl.Int64) * 1000 + pl.col(\"cntyfips\").str.strip_chars().cast(pl.Int64)).alias(\"fips\")\n",
    ")\n",
    "cty_mn = cty.filter(pl.col(\"st\") == \"MN\").with_columns(\n",
    "    (1000 * pl.col(\"stfips\") + pl.col(\"ctfips\")).alias(\"fips\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Polars `join` method to combine home- and county-level information in a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srrs_mn = srrs_mn.join(cty_mn.select([\"fips\", \"Uppm\"]), on=\"fips\")\n",
    "srrs_mn = srrs_mn.unique(subset=[\"idnum\"], maintain_order=True)\n",
    "\n",
    "n = len(srrs_mn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's encode the county names and make local copies of the variables we will use.\n",
    "We also need a lookup table (`dict`) for each unique county, for indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srrs_mn = srrs_mn.with_columns(\n",
    "    pl.col(\"county\").map_elements(str.strip, return_dtype=pl.Utf8).alias(\"county\")\n",
    ")\n",
    "\n",
    "unique_counties = srrs_mn.select(\"county\").unique(maintain_order=True).to_series().to_list()\n",
    "mn_counties = np.array(unique_counties)\n",
    "county_dict = {county: i for i, county in enumerate(unique_counties)}\n",
    "\n",
    "\n",
    "county_uranium_df = (srrs_mn.group_by(\"county\", maintain_order=True).agg(pl.col(\"Uppm\").first()))\n",
    "\n",
    "county_to_uranium = {row[0]: row[1] for row in county_uranium_df.iter_rows()}\n",
    "\n",
    "ordered_uranium = [county_to_uranium[county] for county in unique_counties]\n",
    "u = np.log(np.array(ordered_uranium))\n",
    "\n",
    "srrs_mn = srrs_mn.with_columns(\n",
    "    pl.col(\"county\").replace_strict(county_dict, default=None).alias(\"county_code\")\n",
    ")\n",
    "\n",
    "srrs_mn = srrs_mn.with_columns(\n",
    "    pl.col(\"activity\").str.strip_chars().cast(pl.Float64).alias(\"activity\")\n",
    ")\n",
    "\n",
    "county = srrs_mn.select(\"county_code\").to_numpy().flatten()\n",
    "radon = srrs_mn.select(\"activity\").to_numpy().flatten()\n",
    "log_radon = np.log(radon + 0.1)\n",
    "srrs_mn = srrs_mn.with_columns(\n",
    "pl.lit(log_radon).alias(\"log_radon\")\n",
    ")\n",
    "floor_measure = srrs_mn.select(\"floor\").to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of radon levels in MN (log scale):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(\n",
    "    srrs_mn.to_pandas(),  # polars to pandas for plotly compatibility\n",
    "    x=\"log_radon\",\n",
    "    nbins=50,\n",
    "    labels={\"log_radon\": \"log(radon)\"},\n",
    "    title=\"Distribution of log(radon) levels in MN\"\n",
    ").update_layout(\n",
    "    xaxis_title=\"log(radon)\",\n",
    "    yaxis_title=\"frequency\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conventional approaches\n",
    "\n",
    "The two conventional alternatives to modeling radon exposure represent the two extremes of the bias-variance tradeoff:\n",
    "\n",
    "***Complete pooling***: \n",
    "\n",
    "Treat all counties the same, and estimate a single radon level.\n",
    "\n",
    "$$y_i = \\alpha + \\beta x_i + \\epsilon_i$$\n",
    "\n",
    "***No pooling***:\n",
    "\n",
    "Model radon in each county independently.\n",
    "\n",
    "$$y_i = \\alpha_{j[i]} + \\beta x_i + \\epsilon_i$$\n",
    "\n",
    "where $j = 1,\\ldots,85$\n",
    "\n",
    "The errors $\\epsilon_i$ may represent measurement error, temporal within-house variation, or variation among houses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the point estimates of the slope and intercept for the complete pooling model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as pooled_model:\n",
    "    floor_ind = pm.Data(\"floor_ind\", floor_measure, dims=\"obs_id\")\n",
    "\n",
    "    alpha = pm.Normal(\"alpha\", 0, sigma=10)\n",
    "    beta = pm.Normal(\"beta\", mu=0, sigma=10)\n",
    "    sigma = pm.Exponential(\"sigma\", 5)\n",
    "\n",
    "    theta = alpha + beta * floor_ind\n",
    "\n",
    "    y = pm.Normal(\"y\", theta, sigma=sigma, observed=log_radon, dims=\"obs_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(pooled_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may be wondering why we are using the `pm.Data` container above even though the variable `floor_ind` is not an observed variable nor a parameter of the model. As you'll see, this will make our lives much easier when we'll plot and diagnose our model.ArviZ will thus include `floor_ind` as a variable in the `constant_data` group of the resulting {ref}`InferenceData <xarray_for_arviz>` object. Moreover, including `floor_ind` in the `InferenceData` object makes sharing and reproducing analysis much easier, all the data needed to analyze or rerun the model is stored there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the model let's do some **prior predictive checks**. \n",
    "\n",
    "Indeed, having sensible priors is not only a way to incorporate scientific knowledge into the model, it can also help and make the MCMC machinery faster -- here we are dealing with a simple linear regression, so no link function comes and distorts the outcome space; but one day this will happen to you and you'll need to think hard about your priors to help your MCMC sampler. So, better to train ourselves when it's quite easy than having to learn when it's very hard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pooled_model:\n",
    "    prior_checks = pm.sample_prior_predictive(random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ArviZ `InferenceData` uses `xarray.Dataset`s under the hood, which give access to several common plotting functions with `.plot`. In this case, we want scatter plot of the mean log radon level (which is stored in variable `a`) for each of the two levels we are considering. If our desired plot is supported by xarray plotting capabilities, we can take advantage of xarray to automatically generate both plot and labels for us. Notice how everything is directly plotted and annotated, the only change we need to do is renaming the y axis label from `a` to `Mean log radon level`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = prior_checks.prior.squeeze(drop=True)\n",
    "\n",
    "xr.concat((prior[\"alpha\"], prior[\"alpha\"] + prior[\"beta\"]), dim=\"location\").rename(\n",
    "    \"log_radon\"\n",
    ").assign_coords(location=[\"basement\", \"floor\"]).plot.scatter(\n",
    "    x=\"location\", y=\"log_radon\", edgecolors=\"none\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm no radon expert, but before seeing the data, these priors seem to allow for quite a wide range of the mean log radon level, both as measured either in a basement or on a floor. But don't worry, we can always change these priors if sampling gives us hints that they might not be appropriate -- after all, priors are assumptions, not oaths; and as with most assumptions, they can be tested.\n",
    "\n",
    "However, we can already think of an improvement: Remember that we stated radon levels tend to be higher in basements, so we could incorporate this prior scientific knowledge into our model by forcing the floor effect (`beta`) to be negative. For now, we will leave the model as is, and trust that the information in the data will be sufficient.\n",
    "\n",
    "Speaking of sampling, let's fire up the Bayesian machinery!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pooled_model:\n",
    "    pooled_trace = pm.sample(random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No divergences and a sampling that only took seconds! Here the chains look very good (good R hat, good effective sample size, small sd). The model also estimated a negative floor effect, as we expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(pooled_trace, round_to=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the expected radon levels in basements (`alpha`) and on floors (`alpha + beta`) in relation to the data used to fit the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_mean = pooled_trace.posterior.mean(dim=(\"chain\", \"draw\"))\n",
    "xvals = np.linspace(-0.2, 1.2, 100)\n",
    "\n",
    "px.scatter(\n",
    "    x=srrs_mn[\"floor\"].to_numpy(),\n",
    "    y=np.log(srrs_mn[\"activity\"].to_numpy() + 0.1),\n",
    "    labels={\"x\": \"floor\", \"y\": \"log(activity + 0.1)\"}\n",
    ").add_scatter(\n",
    "    x=xvals,\n",
    "    y=post_mean[\"beta\"].item() * xvals + post_mean[\"alpha\"].item(),\n",
    "    mode=\"lines\",\n",
    "    line=dict(dash=\"dash\", color=\"red\"),\n",
    "    name=\"Posterior Mean\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks reasonable, though notice that there is a great deal of residual variability in the data. \n",
    "\n",
    "Let's now turn our attention to the unpooled model, and see how it fares in comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = {\"county\": mn_counties}\n",
    "\n",
    "with pm.Model(coords=coords) as unpooled_model:\n",
    "    floor_ind = pm.Data(\"floor_ind\", floor_measure, dims=\"obs_id\")\n",
    "\n",
    "    alpha = pm.Normal(\"alpha\", 0, sigma=10, dims=\"county\")\n",
    "    beta = pm.Normal(\"beta\", 0, sigma=10)\n",
    "    sigma = pm.Exponential(\"sigma\", 1)\n",
    "\n",
    "    theta = alpha[county] + beta * floor_ind\n",
    "\n",
    "    y = pm.Normal(\"y\", theta, sigma=sigma, observed=log_radon, dims=\"obs_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(unpooled_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with unpooled_model:\n",
    "    unpooled_trace = pm.sample(random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sampling was clean here too; Let's look at the expected values for both basement (dimension 0) and floor (dimension 1) in each county:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = az.plot_forest(\n",
    "    unpooled_trace,\n",
    "    var_names=[\"alpha\"],\n",
    "    r_hat=True,\n",
    "    combined=True,\n",
    "    figsize=(6, 18),\n",
    "    labeller=az.labels.NoVarLabeller(),\n",
    ")\n",
    "ax[0].set_ylabel(\"alpha\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify counties with high radon levels, we can plot the ordered mean estimates, as well as their 94% HPD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpooled_means = unpooled_trace.posterior.mean(dim=(\"chain\", \"draw\"))\n",
    "unpooled_hdi = az.hdi(unpooled_trace)\n",
    "\n",
    "unpooled_means_iter = unpooled_means.sortby(\"alpha\")\n",
    "unpooled_hdi_iter = unpooled_hdi.sortby(unpooled_means_iter.alpha)\n",
    "\n",
    "_, ax = plt.subplots(figsize=(10,6))\n",
    "xticks = np.arange(0, 86, 6)\n",
    "unpooled_means_iter.plot.scatter(x=\"county\", y=\"alpha\", ax=ax, alpha=0.8)\n",
    "ax.vlines(\n",
    "    np.arange(mn_counties.shape[0]),\n",
    "    unpooled_hdi_iter.alpha.sel(hdi=\"lower\"),\n",
    "    unpooled_hdi_iter.alpha.sel(hdi=\"higher\"),\n",
    "    color=\"orange\",\n",
    "    alpha=0.6,\n",
    ")\n",
    "ax.set(ylabel=\"Radon estimate\", ylim=(-2, 4.5))\n",
    "ax.set_xticks(xticks)\n",
    "ax.set_xticklabels(unpooled_means_iter.county.values[xticks])\n",
    "ax.tick_params(rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have fit both conventional (*i.e.* non-hierarchcial) models, let's see how their inferences differ. Here are visual comparisons between the pooled and unpooled estimates for a subset of counties representing a range of sample sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_counties = (\n",
    "    \"LAC QUI PARLE\",\n",
    "    \"AITKIN\",\n",
    "    \"KOOCHICHING\",\n",
    "    \"DOUGLAS\",\n",
    "    \"CLAY\",\n",
    "    \"STEARNS\",\n",
    "    \"RAMSEY\",\n",
    "    \"ST LOUIS\",\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6), sharey=True, sharex=True)\n",
    "axes = axes.ravel()\n",
    "m = unpooled_means[\"beta\"]\n",
    "for i, c in enumerate(sample_counties):\n",
    "    y = srrs_mn.filter(pl.col(\"county\") == c)[\"log_radon\"].to_numpy()\n",
    "    x = srrs_mn.filter(pl.col(\"county\") == c)[\"floor\"].to_numpy()\n",
    "    axes[i].scatter(x + np.random.randn(len(x)) * 0.01, y, alpha=0.4)\n",
    "\n",
    "    # No pooling model\n",
    "    b = unpooled_means[\"alpha\"].sel(county=c)\n",
    "\n",
    "    # Plot both models and data\n",
    "    xvals = xr.DataArray(np.linspace(0, 1))\n",
    "    axes[i].plot(xvals, m * xvals + b)\n",
    "    axes[i].plot(xvals, post_mean[\"beta\"] * xvals + post_mean[\"alpha\"], \"r--\")\n",
    "    axes[i].set_xticks([0, 1])\n",
    "    axes[i].set_xticklabels([\"basement\", \"floor\"])\n",
    "    axes[i].set_ylim(-1, 3)\n",
    "    axes[i].set_title(c)\n",
    "    if not i % 2:\n",
    "        axes[i].set_ylabel(\"log radon level\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neither of these models are satisfactory:\n",
    "\n",
    "* If we are trying to identify high-radon counties, pooling is useless -- because, by definition, the pooled model estimates radon at the state-level. In other words, pooling leads to maximal *underfitting*: the variation across counties is not taken into account and only the overall population is estimated.\n",
    "* We do not trust extreme unpooled estimates produced by models using few observations. This leads to maximal *overfitting*: only the within-county variations are taken into account and the overall population (i.e the state-level, which tells us about similarities across counties) is not estimated. \n",
    "\n",
    "This issue is acute for small sample sizes, as seen above: in counties where we have few floor measurements, if radon levels are higher for those data points than for basement ones (Aitkin, Koochiching, Ramsey), the model will estimate that radon levels are higher in floors than basements for these counties. But we shouldn't trust this conclusion, because both scientific knowledge and the situation in other counties tell us that it is usually the reverse (basement radon > floor radon). So unless we have a lot of observations telling us otherwise for a given county, we should be skeptical and shrink our county-estimates to the state-estimates -- in other words, we should balance between cluster-level and population-level information, and the amount of shrinkage will depend on how extreme and how numerous the data in each cluster are. \n",
    "\n",
    "Here is where hierarchical models come into play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilevel and hierarchical models\n",
    "\n",
    "When we pool our data, we imply that they are sampled from the same model. This ignores any variation among sampling units (other than sampling variance) -- we assume that counties are all the same:\n",
    "\n",
    "![pooled](images/pooled_model.png)\n",
    "\n",
    "When we analyze data unpooled, we imply that they are sampled independently from separate models. At the opposite extreme from the pooled case, this approach claims that differences between sampling units are too large to combine them -- we assume that counties have no similarity whatsoever:\n",
    "\n",
    "![unpooled](images/unpooled_model.png)\n",
    "\n",
    "In a hierarchical model, parameters are viewed as a sample from a population distribution of parameters. Thus, we view them as being neither entirely different or exactly the same. This is ***partial pooling***:\n",
    "\n",
    "![hierarchical](images/partial_pooled_model.png)\n",
    "\n",
    "We can use PyMC to easily specify multilevel models, and fit them using Markov chain Monte Carlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial pooling model\n",
    "\n",
    "The simplest partial pooling model for the household radon dataset is one which simply estimates radon levels, without any predictors at any level. A partial pooling model represents a compromise between the pooled and unpooled extremes, essentially a weighted average (based on sample size) of the unpooled county estimates and the pooled estimates.\n",
    "\n",
    "$$\\hat{\\alpha} \\approx \\frac{(n_j/\\sigma_y^2)\\bar{y}_j + (1/\\sigma_{\\alpha}^2)\\bar{y}}{(n_j/\\sigma_y^2) + (1/\\sigma_{\\alpha}^2)}$$\n",
    "\n",
    "Estimates for counties with smaller sample sizes will shrink towards the state-wide average, while those for counties with larger sample sizes will be closer to the unpooled county estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a very simple partial pooling model, which ignores the effect of floor vs. basement measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords=coords) as partial_pooling:\n",
    "    county_idx = pm.Data(\"county_idx\", county, dims=\"obs_id\")\n",
    "\n",
    "    # Priors\n",
    "    mu_a = pm.Normal(\"mu_a\", mu=0.0, sigma=10)\n",
    "    sigma_a = pm.Exponential(\"sigma_a\", 1)\n",
    "\n",
    "    # Random intercepts\n",
    "    alpha = pm.Normal(\"alpha\", mu=mu_a, sigma=sigma_a, dims=\"county\")\n",
    "\n",
    "    # Model error\n",
    "    sigma_y = pm.Exponential(\"sigma_y\", 1)\n",
    "\n",
    "    # Expected value\n",
    "    y_hat = alpha[county_idx]\n",
    "\n",
    "    # Data likelihood\n",
    "    y_like = pm.Normal(\"y_like\", mu=y_hat, sigma=sigma_y, observed=log_radon, dims=\"obs_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(partial_pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with partial_pooling:\n",
    "    partial_pooling_trace = pm.sample(tune=2000, random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_county = srrs_mn.group_by(\"county\").agg(pl.count(\"idnum\")).sort(\"county\")[\"idnum\"].to_numpy()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharex=True, sharey=True)\n",
    "for ax, trace, level in zip(\n",
    "    axes,\n",
    "    (unpooled_trace, partial_pooling_trace),\n",
    "    (\"no pooling\", \"partial pooling\"),\n",
    "):\n",
    "    # add variable with x values to xarray dataset\n",
    "    trace.posterior = trace.posterior.assign_coords({\"N_county\": (\"county\", N_county)})\n",
    "    # plot means\n",
    "    trace.posterior.mean(dim=(\"chain\", \"draw\")).plot.scatter(\n",
    "        x=\"N_county\", y=\"alpha\", ax=ax, alpha=0.9\n",
    "    )\n",
    "    ax.hlines(\n",
    "        partial_pooling_trace.posterior.alpha.mean(),\n",
    "        0.9,\n",
    "        max(N_county) + 1,\n",
    "        alpha=0.4,\n",
    "        ls=\"--\",\n",
    "        label=\"Est. population mean\",\n",
    "    )\n",
    "\n",
    "    # plot hdi\n",
    "    hdi = az.hdi(trace).alpha\n",
    "    ax.vlines(N_county, hdi.sel(hdi=\"lower\"), hdi.sel(hdi=\"higher\"), color=\"orange\", alpha=0.5)\n",
    "\n",
    "    ax.set(\n",
    "        title=f\"{level.title()} Estimates\",\n",
    "        xlabel=\"Nbr obs in county (log scale)\",\n",
    "        xscale=\"log\",\n",
    "        ylabel=\"Log radon\",\n",
    "    )\n",
    "    ax.legend(fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the difference between the unpooled and partially-pooled estimates, particularly at smaller sample sizes: As expected, the former are both more extreme and more imprecise. Indeed, in the partially-pooled model, estimates in small-sample-size counties are informed by the population parameters -- hence more precise estimates. Moreover, the smaller the sample size, the more regression towards the overall mean (the dashed gray line) -- hence less extreme estimates. In other words, the model is skeptical of extreme deviations from the population mean in counties where data is sparse. This is known as **shrinkage**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go back and integrate the `floor` predictor, but allowing the intercept to vary by county.\n",
    "\n",
    "## Varying intercept model\n",
    "\n",
    "This model allows intercepts to vary across county, according to a random effect.\n",
    "\n",
    "$$y_i = \\alpha_{j[i]} + \\beta x_{i} + \\epsilon_i$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\epsilon_i \\sim N(0, \\sigma_y^2)$$\n",
    "\n",
    "and the intercept random effect:\n",
    "\n",
    "$$\\alpha_{j[i]} \\sim N(\\mu_{\\alpha}, \\sigma_{\\alpha}^2)$$\n",
    "\n",
    "As with the the no-pooling model, we set a separate intercept for each county, but rather than fitting separate least squares regression models for each county, multilevel modeling **shares strength** among counties, allowing for more reasonable inference in counties with little data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords=coords) as varying_intercept:\n",
    "    floor_idx = pm.Data(\"floor_idx\", floor_measure, dims=\"obs_id\")\n",
    "    county_idx = pm.Data(\"county_idx\", county, dims=\"obs_id\")\n",
    "\n",
    "    # Priors\n",
    "    mu_a = pm.Normal(\"mu_a\", mu=0.0, sigma=10.0)\n",
    "    sigma_a = pm.Exponential(\"sigma_a\", 1)\n",
    "\n",
    "    # Random intercepts\n",
    "    alpha = pm.Normal(\"alpha\", mu=mu_a, sigma=sigma_a, dims=\"county\")\n",
    "    # Common slope\n",
    "    beta = pm.Normal(\"beta\", mu=0.0, sigma=10.0)\n",
    "\n",
    "    # Model error\n",
    "    sd_y = pm.Exponential(\"sd_y\", 1)\n",
    "\n",
    "    # Expected value\n",
    "    y_hat = alpha[county_idx] + beta * floor_idx\n",
    "\n",
    "    # Data likelihood\n",
    "    y_like = pm.Normal(\"y_like\", mu=y_hat, sigma=sd_y, observed=log_radon, dims=\"obs_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(varying_intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with varying_intercept:\n",
    "    varying_intercept_trace = pm.sample(tune=2000, random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = az.plot_forest(\n",
    "    varying_intercept_trace,\n",
    "    var_names=[\"alpha\"],\n",
    "    figsize=(6, 18),\n",
    "    combined=True,\n",
    "    r_hat=True,\n",
    "    labeller=az.labels.NoVarLabeller(),\n",
    ")\n",
    "ax[0].set_ylabel(\"alpha\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(varying_intercept_trace, var_names=[\"sigma_a\", \"beta\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimate for the `floor` coefficient is approximately -0.66, which can be interpreted as houses without basements having about half ($\\exp(-0.66) = 0.52$) the radon levels of those with basements, after accounting for county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(varying_intercept_trace, var_names=[\"beta\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = xr.DataArray([0, 1], dims=\"Level\", coords={\"Level\": [\"Basement\", \"Floor\"]})\n",
    "post = varying_intercept_trace.posterior  # alias for readability\n",
    "theta = (\n",
    "    (post.alpha + post.beta * xvals).mean(dim=(\"chain\", \"draw\")).to_dataset(name=\"Mean log radon\")\n",
    ")\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "theta.plot.scatter(x=\"Level\", y=\"Mean log radon\", alpha=0.2, color=\"k\", ax=ax)  # scatter\n",
    "ax.plot(xvals, theta[\"Mean log radon\"].T, \"k-\", alpha=0.2)\n",
    "# add lines too\n",
    "ax.set_title(\"MEAN LOG RADON BY COUNTY\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to show that the partial pooling model provides more objectively reasonable estimates than either the pooled or unpooled models, at least for counties with small sample sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_counties = (\n",
    "    \"LAC QUI PARLE\",\n",
    "    \"AITKIN\",\n",
    "    \"KOOCHICHING\",\n",
    "    \"DOUGLAS\",\n",
    "    \"CLAY\",\n",
    "    \"STEARNS\",\n",
    "    \"RAMSEY\",\n",
    "    \"ST LOUIS\",\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6), sharey=True, sharex=True)\n",
    "axes = axes.ravel()\n",
    "m = unpooled_means[\"beta\"]\n",
    "for i, c in enumerate(sample_counties):\n",
    "    y = srrs_mn.filter(pl.col(\"county\") == c)[\"log_radon\"].to_numpy()\n",
    "    x = srrs_mn.filter(pl.col(\"county\") == c)[\"floor\"].to_numpy()\n",
    "    axes[i].scatter(x + np.random.randn(len(x)) * 0.01, y, alpha=0.4)\n",
    "\n",
    "    # No pooling model\n",
    "    b = unpooled_means[\"alpha\"].sel(county=c)\n",
    "\n",
    "    # Plot both models and data\n",
    "    xvals = xr.DataArray(np.linspace(0, 1))\n",
    "    axes[i].plot(xvals, m.values * xvals + b.values)\n",
    "    axes[i].plot(xvals, post_mean[\"beta\"] * xvals + post_mean[\"alpha\"], \"r--\")\n",
    "\n",
    "    varying_intercept_trace.posterior.sel(county=c).beta\n",
    "    post = varying_intercept_trace.posterior.sel(county=c).mean(dim=(\"chain\", \"draw\"))\n",
    "    theta = post.alpha.values + post.beta.values * xvals\n",
    "    axes[i].plot(xvals, theta, \"k:\")\n",
    "    axes[i].set_xticks([0, 1])\n",
    "    axes[i].set_xticklabels([\"basement\", \"floor\"])\n",
    "    axes[i].set_ylim(-1, 3)\n",
    "    axes[i].set_title(c)\n",
    "    if not i % 2:\n",
    "        axes[i].set_ylabel(\"log radon level\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Varying intercept and slope model\n",
    "\n",
    "The most general model allows both the intercept and slope to vary by county:\n",
    "\n",
    "$$y_i = \\alpha_{j[i]} + \\beta_{j[i]} x_{i} + \\epsilon_i$$\n",
    "\n",
    "Construct a model called `varying_intercept_slope` that implements this alternative model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords=coords) as varying_intercept_slope:\n",
    "    \n",
    "    # Write your model here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the model DAG just to check your model is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(varying_intercept_slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with varying_intercept_slope:\n",
    "    varying_intercept_slope_trace = pm.sample(tune=2000, random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the trace of this model includes divergences, which can be problematic depending on where and how frequently they occur. These can occur is some hierararchical models, and they can be avoided by using the **non-centered parametrization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-centered Parameterization\n",
    "\n",
    "The partial pooling models specified above uses a **centered** parameterization of the slope random effect. That is, the individual county effects are distributed around a county mean, with a spread controlled by the hierarchical standard deviation parameter. As the preceding plot reveals, this constraint serves to **shrink** county estimates toward the overall mean, to a degree proportional to the county sample size. This is exactly what we want, and the model appears to fit well--the Gelman-Rubin statistics are exactly 1.\n",
    "\n",
    "But, on closer inspection, there are signs of trouble. Specifically, let's look at the trace of the random effects, and their corresponding standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract posterior samples for chain 0 using polars\n",
    "sigma_b_df = varying_intercept_slope_trace.posterior[\"sigma_b\"].sel(chain=0).to_dataframe().reset_index()\n",
    "beta_df = varying_intercept_slope_trace.posterior[\"beta\"].sel(chain=0).to_dataframe().reset_index()\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2)\n",
    "axs[0].plot(sigma_b_df[\"sigma_b\"].to_numpy(), alpha=0.5)\n",
    "axs[0].set(ylabel=\"sigma_b\")\n",
    "axs[1].plot(beta_df[\"beta\"].to_numpy(), alpha=0.5)\n",
    "axs[1].set(ylabel=\"beta\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when the chain reaches the lower end of the parameter space for $\\sigma_b$, it appears to get \"stuck\" and the entire sampler, including the random slopes `beta`, mixes poorly. \n",
    "\n",
    "Jointly plotting the random effect variance and one of the individual random slopes demonstrates what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = varying_intercept_slope_trace.posterior[\"beta\"].sel(county=\"AITKIN\").values.flatten()\n",
    "y = varying_intercept_slope_trace.posterior[\"sigma_b\"].values.flatten()\n",
    "\n",
    "diverging_mask = varying_intercept_slope_trace.sample_stats['diverging'].values.flatten().astype(bool)\n",
    "\n",
    "go.Figure().add_trace(go.Scatter(\n",
    "    x=x[~diverging_mask],\n",
    "    y=y[~diverging_mask],\n",
    "    mode='markers',\n",
    "    name='Non-diverging',\n",
    "    marker=dict(color='blue', size=8, opacity=0.2)\n",
    ")).add_trace(go.Scatter(\n",
    "    x=x[diverging_mask],\n",
    "    y=y[diverging_mask],\n",
    "    mode='markers',\n",
    "    name='Diverging',\n",
    "    marker=dict(color='orange', size=8, opacity=0.7)\n",
    ")).update_layout(\n",
    "    title=\"Neal's Funnel\",\n",
    "    xaxis_title='Beta (AITKIN county)',\n",
    "    yaxis_title='Sigma_b',\n",
    "    showlegend=True,\n",
    "    yaxis=dict(range=[0, None])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the group variance is small, this implies that the individual random slopes are themselves close to the group mean. This results in a *funnel*-shaped relationship between the samples of group variance and any of the slopes (particularly those with a smaller sample size). \n",
    "\n",
    "In itself, this is not a problem, since this is the behavior we expect. However, if the sampler is tuned for the wider (unconstrained) part of the parameter space, it has trouble in the areas of higher curvature. The consequence of this is that the neighborhood close to the lower bound of $\\sigma_b$ is sampled poorly; indeed, in our chain it is not sampled at all below 0.1. In addtion, the sampler generates a lot of divergent samples. The result of this will be biased inference.\n",
    "\n",
    "Now that we've spotted the problem, what can we do about it? The best way to deal with this issue is to reparameterize our model. Notice the random slopes in this version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords=coords) as varying_intercept_slope_noncentered:\n",
    "    floor_idx = pm.Data(\"floor_idx\", floor_measure, dims=\"obs_id\")\n",
    "    county_idx = pm.Data(\"county_idx\", county, dims=\"obs_id\")\n",
    "\n",
    "    # Priors\n",
    "    mu_a = pm.Normal(\"mu_a\", mu=0.0, sigma=10.0)\n",
    "    sigma_a = pm.Exponential(\"sigma_a\", 5)\n",
    "\n",
    "    # Non-centered random intercepts\n",
    "    # Centered: a = pm.Normal('a', mu_a, sigma=sigma_a, shape=counties)\n",
    "    z_a = pm.Normal(\"z_a\", mu=0, sigma=1, dims=\"county\")\n",
    "    alpha = pm.Deterministic(\"alpha\", mu_a + z_a * sigma_a, dims=\"county\")\n",
    "\n",
    "    mu_b = pm.Normal(\"mu_b\", mu=0.0, sigma=10.0)\n",
    "    sigma_b = pm.Exponential(\"sigma_b\", 5)\n",
    "\n",
    "    # Non-centered random slopes\n",
    "    z_b = pm.Normal(\"z_b\", mu=0, sigma=1, dims=\"county\")\n",
    "    beta = pm.Deterministic(\"beta\", mu_b + z_b * sigma_b, dims=\"county\")\n",
    "\n",
    "    # Model error\n",
    "    sigma_y = pm.Exponential(\"sigma_y\", 5)\n",
    "\n",
    "    # Expected value\n",
    "    y_hat = alpha[county_idx] + beta[county_idx] * floor_idx\n",
    "\n",
    "    # Data likelihood\n",
    "    y_like = pm.Normal(\"y_like\", mu=y_hat, sigma=sigma_y, observed=log_radon, dims=\"obs_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(varying_intercept_slope_noncentered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a [**non-centered** parameterization](https://twiecki.io/blog/2017/02/08/bayesian-hierchical-non-centered/). By this, we mean that the random deviates are no longer explicitly modeled as being centered on $\\mu_b$. Instead, they are independent standard normals $\\upsilon$, which are then scaled by the appropriate value of $\\sigma_b$, before being location-transformed by the mean.\n",
    "\n",
    "This model samples much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with varying_intercept_slope_noncentered:\n",
    "    noncentered_trace = pm.sample(tune=3000, target_accept=0.95, random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the bottlenecks in the traces are gone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract posterior samples for chain 0 using polars\n",
    "sigma_b_df = noncentered_trace.posterior[\"sigma_b\"].sel(chain=0).to_dataframe().reset_index()\n",
    "beta_df = noncentered_trace.posterior[\"beta\"].sel(chain=0).to_dataframe().reset_index()\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2)\n",
    "axs[0].plot(sigma_b_df[\"sigma_b\"].to_numpy(), alpha=0.5)\n",
    "axs[0].set(ylabel=\"sigma_b\")\n",
    "axs[1].plot(beta_df[\"beta\"].to_numpy(), alpha=0.5)\n",
    "axs[1].set(ylabel=\"beta\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And correspondingly, the low end of the posterior distribution of the slope random effect variance can now be sampled efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = noncentered_trace.posterior[\"beta\"].sel(county=\"AITKIN\").values.flatten()\n",
    "y = noncentered_trace.posterior[\"sigma_b\"].values.flatten()\n",
    "\n",
    "diverging_mask = noncentered_trace.sample_stats['diverging'].values.flatten().astype(bool)\n",
    "\n",
    "go.Figure().add_trace(go.Scatter(\n",
    "    x=x[~diverging_mask],\n",
    "    y=y[~diverging_mask],\n",
    "    mode='markers',\n",
    "    name='Non-diverging',\n",
    "    marker=dict(color='blue', size=8, opacity=0.2)\n",
    ")).add_trace(go.Scatter(\n",
    "    x=x[diverging_mask],\n",
    "    y=y[diverging_mask],\n",
    "    mode='markers',\n",
    "    name='Diverging',\n",
    "    marker=dict(color='orange', size=8, opacity=0.7)\n",
    ")).update_layout(\n",
    "    title=\"Neal's Funnel\",\n",
    "    xaxis_title='Beta (AITKIN county)',\n",
    "    yaxis_title='Sigma_b',\n",
    "    showlegend=True,\n",
    "    yaxis=dict(range=[0, None])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, we are now fully exploring the support of the posterior. This results in less bias in these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, constrained_layout=True)\n",
    "az.plot_posterior(varying_intercept_slope_trace, var_names=[\"sigma_b\"], ax=ax1)\n",
    "az.plot_posterior(noncentered_trace, var_names=[\"sigma_b\"], ax=ax2)\n",
    "ax1.set_title(\"Centered (top) and non-centered (bottom)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `sigma_b` now has a lot of density near zero, which would indicate that counties don't vary that much in their answer to the `floor` \"treatment\". \n",
    "\n",
    "This was the problem with the original parameterization: the sampler has difficulty with the geometry of the posterior distribution when the values of the slope random effects are so different for standard deviations very close to zero compared to when they are positive. However, even with the non-centered model the sampler is not that comfortable with `sigma_b`: in fact if you look at the estimates with `az.summary` you'll see that the number of effective samples is quite low for `sigma_b`.\n",
    "\n",
    "Also note that `sigma_a` is not that big either -- i.e counties do differ in their baseline radon levels, but not by a lot. However we don't have that much of a problem to sample from this distribution because it's much narrower than `sigma_b` and doesn't get dangerously close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(varying_intercept_slope_trace, var_names=[\"sigma_a\", \"sigma_b\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To wrap up this model, let's plot the relationship between radon and floor for each county:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = xr.DataArray([0, 1], dims=\"Level\", coords={\"Level\": [\"Basement\", \"Floor\"]})\n",
    "post = noncentered_trace.posterior  # alias for readability\n",
    "theta = (\n",
    "    (post.alpha + post.beta * xvals).mean(dim=(\"chain\", \"draw\")).to_dataset(name=\"Mean log radon\")\n",
    ")\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "theta.plot.scatter(x=\"Level\", y=\"Mean log radon\", alpha=0.2, color=\"k\", ax=ax)  # scatter\n",
    "ax.plot(xvals, theta[\"Mean log radon\"].T, \"k-\", alpha=0.2)\n",
    "# add lines too\n",
    "ax.set_title(\"MEAN LOG RADON BY COUNTY\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This, while both the intercept and the slope vary by county, there is far less variation in the slope. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding group-level predictors\n",
    "\n",
    "A primary strength of multilevel models is the ability to handle predictors on multiple levels simultaneously. If we consider the varying-intercepts model above:\n",
    "\n",
    "$$y_i = \\alpha_{j[i]} + \\beta x_{i} + \\epsilon_i$$\n",
    "\n",
    "we may, instead of a simple random effect to describe variation in the expected radon value, specify another regression model with a county-level covariate. Here, we use the county uranium reading $u_j$, which is thought to be related to radon levels:\n",
    "\n",
    "$$\\alpha_j = \\gamma_0 + \\gamma_1 u_j + \\zeta_j$$\n",
    "\n",
    "$$\\zeta_j \\sim N(0, \\sigma_{\\alpha}^2)$$\n",
    "\n",
    "Thus, we are now incorporating a house-level predictor (floor or basement) as well as a county-level predictor (uranium).\n",
    "\n",
    "Note that the model has both indicator variables for each county, plus a county-level covariate. In classical regression, this would result in collinearity. In a multilevel model, the partial pooling of the intercepts towards the expected value of the group-level linear model avoids this.\n",
    "\n",
    "Group-level predictors also serve to reduce group-level variation, $\\sigma_{\\alpha}$ (here it would be the variation across counties, `sigma_a`). An important implication of this is that the group-level estimate induces stronger pooling -- by definition, a smaller $\\sigma_{\\alpha}$ means a stronger shrinkage of counties parameters towards the overall state mean. \n",
    "\n",
    "This is fairly straightforward to implement in PyMC -- we just add another level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords=coords) as hierarchical_intercept:\n",
    "    # Priors\n",
    "    sigma_a = pm.HalfCauchy(\"sigma_a\", 5)\n",
    "\n",
    "    # County uranium model\n",
    "    gamma_0 = pm.Normal(\"gamma_0\", mu=0.0, sigma=10.0)\n",
    "    gamma_1 = pm.Normal(\"gamma_1\", mu=0.0, sigma=10.0)\n",
    "\n",
    "    # Uranium model for intercept\n",
    "    mu_a = pm.Deterministic(\"mu_a\", gamma_0 + gamma_1 * u)\n",
    "    # County variation not explained by uranium\n",
    "    epsilon_a = pm.Normal(\"epsilon_a\", mu=0, sigma=1, dims=\"county\")\n",
    "    alpha = pm.Deterministic(\"alpha\", mu_a + sigma_a * epsilon_a, dims=\"county\")\n",
    "\n",
    "    # Common slope\n",
    "    beta = pm.Normal(\"beta\", mu=0.0, sigma=10.0)\n",
    "\n",
    "    # Model error\n",
    "    sigma_y = pm.Uniform(\"sigma_y\", lower=0, upper=100)\n",
    "\n",
    "    # Expected value\n",
    "    y_hat = alpha[county] + beta * floor_measure\n",
    "\n",
    "    # Data likelihood\n",
    "    y_like = pm.Normal(\"y_like\", mu=y_hat, sigma=sigma_y, observed=log_radon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(hierarchical_intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see the new level, with `sigma_a` and `gamma`, which is two-dimensional because it contains the linear model for `a_county`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with hierarchical_intercept:\n",
    "    hierarchical_intercept_trace = pm.sample(tune=2000, random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uranium = u\n",
    "post = hierarchical_intercept_trace.posterior.assign_coords(uranium=uranium)\n",
    "avg_a = post[\"mu_a\"].mean(dim=(\"chain\", \"draw\")).values[np.argsort(uranium)]\n",
    "avg_a_county = post[\"alpha\"].mean(dim=(\"chain\", \"draw\"))\n",
    "avg_a_county_hdi = az.hdi(post, var_names=\"alpha\")[\"alpha\"]\n",
    "\n",
    "# Calculate HDI for the trend line (mu_a)\n",
    "mu_a_hdi = az.hdi(post, var_names=\"mu_a\")[\"mu_a\"]\n",
    "sorted_indices = np.argsort(uranium)\n",
    "\n",
    "fig = go.Figure().add_trace(\n",
    "    go.Scatter(\n",
    "        x=np.concatenate([uranium[sorted_indices], uranium[sorted_indices][::-1]]),\n",
    "        y=np.concatenate([mu_a_hdi.sel(hdi=\"lower\").values[sorted_indices],\n",
    "\n",
    "    mu_a_hdi.sel(hdi=\"higher\").values[sorted_indices][::-1]]),\n",
    "        fill='toself',\n",
    "        fillcolor='rgba(128,128,128,0.2)',\n",
    "        line=dict(color='rgba(255,255,255,0)'),\n",
    "        showlegend=True,\n",
    "        name='Mean intercept HPD',\n",
    "        hoverinfo='skip'\n",
    "    )\n",
    ").add_trace(go.Scatter(\n",
    "    x=uranium[sorted_indices],\n",
    "    y=avg_a,\n",
    "    mode='lines',\n",
    "    line=dict(dash='dash', color='black', width=2),\n",
    "    opacity=0.6,\n",
    "    name='Mean intercept'\n",
    ")).add_trace(go.Scatter(\n",
    "    x=uranium,\n",
    "    y=avg_a_county,\n",
    "    mode='markers',\n",
    "    marker=dict(color='teal', size=6, opacity=0.8),\n",
    "    name='Mean county-intercept'\n",
    "))\n",
    "\n",
    "for i, u_val in enumerate(uranium):\n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=u_val, x1=u_val,\n",
    "        y0=avg_a_county_hdi.sel(hdi=\"lower\").values[i],\n",
    "        y1=avg_a_county_hdi.sel(hdi=\"higher\").values[i],\n",
    "        line=dict(color=\"orange\", width=1.5),\n",
    "        opacity=0.7\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"County-level uranium\",\n",
    "    yaxis_title=\"Intercept estimate\",\n",
    "    showlegend=True,\n",
    "    plot_bgcolor='white',\n",
    "    width=800,\n",
    "    height=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uranium is indeed strongly associated with baseline radon levels in each county. The graph above shows the average relationship and its uncertainty: the baseline radon level in an average county as a function of uranium, as well as the 94% HPD of this radon level (dashed line and envelope). The blue points and orange bars represent the relationship between baseline radon and uranium, but now for each county. As you see, the uncertainty is bigger now, because it adds on top of the average uncertainty -- each county has its idyosyncracies after all.\n",
    "\n",
    "If we compare the county-intercepts for this model with those of the partial-pooling model without a county-level covariate:The standard errors on the intercepts are narrower than for the partial-pooling model without a county-level covariate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = az.plot_forest(\n",
    "    [varying_intercept_trace, hierarchical_intercept_trace],\n",
    "    model_names=[\"W/t. county pred.\", \"With county pred.\"],\n",
    "    var_names=[\"alpha\"],\n",
    "    combined=True,\n",
    "    figsize=(6, 40),\n",
    "    textsize=9,\n",
    "    legend=False\n",
    ")\n",
    "\n",
    "# Clean up y-axis labels to show only county names (remove duplicates)\n",
    "y_labels = [label.get_text() for label in ax[0].get_yticklabels()]\n",
    "clean_labels = []\n",
    "seen_counties = set()\n",
    "\n",
    "for label in y_labels:\n",
    "    if ':' in label:\n",
    "        # Extract just the county name part (after the colon)\n",
    "        county_name = label.split(':')[1].strip().replace('[', '').replace(']', '')\n",
    "        if county_name not in seen_counties:\n",
    "            clean_labels.append(county_name)\n",
    "            seen_counties.add(county_name)\n",
    "        else:\n",
    "            clean_labels.append('')  # Empty string for duplicate\n",
    "    else:\n",
    "        clean_labels.append(label)\n",
    "\n",
    "ax[0].set_yticklabels(clean_labels)\n",
    "ax[0].set_ylabel(\"County\")\n",
    "ax[0].legend([\"W/t. county pred.\", \"With county pred.\"], loc='upper center', bbox_to_anchor=(0.5, 1.02), ncol=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the compatibility intervals are narrower for the model including the county-level covariate. This is expected, as the effect of a covariate is to reduce the variation in the outcome variable -- provided the covariate is of predictive value. More importantly, with this model we were able to squeeze even more information out of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations among levels\n",
    "\n",
    "In some instances, having predictors at multiple levels can reveal correlation between individual-level variables and group residuals. We can account for this by including the average of the individual predictors as a covariate in the model for the group intercept.\n",
    "\n",
    "$$\\alpha_j = \\gamma_0 + \\gamma_1 u_j + \\gamma_2 \\bar{x} + \\zeta_j$$\n",
    "\n",
    "These are broadly referred to as ***contextual effects***.\n",
    "\n",
    "To add these effects to our model, let's create a new variable containing the mean of `floor` in each county and add that to our previous model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new variable for mean of floor across counties\n",
    "avg_floor_data = srrs_mn.group_by(\"county\").agg(pl.col(\"floor\").mean()).select(\"floor\").to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords=coords) as contextual_effect:\n",
    "    floor_idx = pm.Data(\"floor_idx\", floor_measure)\n",
    "    county_idx = pm.Data(\"county_idx\", county)\n",
    "    y = pm.Data(\"y\", log_radon)\n",
    "\n",
    "    # Priors\n",
    "    sigma_a = pm.HalfCauchy(\"sigma_a\", 5)\n",
    "\n",
    "    # County uranium model for slope\n",
    "    gamma = pm.Normal(\"gamma\", mu=0.0, sigma=10, shape=3)\n",
    "\n",
    "    # Uranium model for intercept\n",
    "    mu_a = pm.Deterministic(\"mu_a\", gamma[0] + gamma[1] * u + gamma[2] * avg_floor_data)\n",
    "\n",
    "    # County variation not explained by uranium\n",
    "    epsilon_a = pm.Normal(\"epsilon_a\", mu=0, sigma=1, dims=\"county\")\n",
    "    alpha = pm.Deterministic(\"alpha\", mu_a + sigma_a * epsilon_a)\n",
    "\n",
    "    # Common slope\n",
    "    beta = pm.Normal(\"beta\", mu=0.0, sigma=10)\n",
    "\n",
    "    # Model error\n",
    "    sigma_y = pm.Uniform(\"sigma_y\", lower=0, upper=100)\n",
    "\n",
    "    # Expected value\n",
    "    y_hat = alpha[county_idx] + beta * floor_idx\n",
    "\n",
    "    # Data likelihood\n",
    "    y_like = pm.Normal(\"y_like\", mu=y_hat, sigma=sigma_y, observed=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with contextual_effect:\n",
    "    contextual_effect_trace = pm.sample(tune=2000, random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(contextual_effect_trace, var_names=\"gamma\", round_to=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we might infer from this that counties with higher proportions of houses without basements tend to have higher baseline levels of radon. This seems to be new, as up to this point we saw that `floor` was *negatively* associated with radon levels. But remember this was at the household-level: radon tends to be higher in houses with basements. But at the county-level it seems that the less basements on average in the county, the more radon. So it's not that contradictory. What's more, the estimate for $\\gamma_2$ is quite uncertain and overlaps with zero, so it's possible that the relationship is not that strong. And finally, let's note that $\\gamma_2$ estimates something else than uranium's effect, as this is already taken into account by $\\gamma_1$ -- it answers the question \"once we know uranium level in the county, is there any value in learning about the proportion of houses without basements?\".\n",
    "\n",
    "All of this is to say that we shouldn't interpret this causally: there is no credible mechanism by which a basement (or absence thereof) *causes* radon emissions. More probably, our causal graph is missing something: a confounding variable, one that influences both basement construction and radon levels, is lurking somewhere in the dark... Perhaps is it the type of soil, which might influence what type of structures are built *and* the level of radon? Maybe adding this to our model would help with causal inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "Gelman (2006) used cross-validation tests to check the prediction error of the unpooled, pooled, and partially-pooled models\n",
    "\n",
    "**root mean squared cross-validation prediction errors**:\n",
    "\n",
    "* unpooled = 0.86\n",
    "* pooled = 0.84\n",
    "* multilevel = 0.79\n",
    "\n",
    "There are two types of prediction that can be made in a multilevel model:\n",
    "\n",
    "1. a new individual within an existing group\n",
    "2. a new individual within a new group\n",
    "\n",
    "For example, if we wanted to make a prediction for a new house with no basement in St. Louis and Kanabec counties, we just need to sample from the radon model with the appropriate intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, \n",
    "\n",
    "$$\\tilde{y}_i \\sim N(\\alpha_{69} + \\beta (x_i=1), \\sigma_y^2)$$\n",
    "\n",
    "Because we judiciously set the county index and floor values as shared variables earlier, we can modify them directly to the desired values (69 and 1 respectively) and sample corresponding posterior predictions, without having to redefine and recompile our model. Using the model just above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_coords = {\"obs_id\": [\"ST LOUIS\", \"KANABEC\"]}\n",
    "with contextual_effect:\n",
    "    pm.set_data({\"county_idx\": np.array([69, 31]), \"floor_idx\": np.array([1, 1]), \"y\": np.ones(2)})\n",
    "    stl_pred = pm.sample_posterior_predictive(contextual_effect_trace.posterior)\n",
    "\n",
    "contextual_effect_trace.extend(stl_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(contextual_effect_trace, group=\"posterior_predictive\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction for a house within a new county is a little trickier. It is actually easier to create a new model to work with, **but use the trace from the original model for posterior predictive sampling**. \n",
    "\n",
    "How can this work?\n",
    "\n",
    "First, consider how posterior predictive sampling works in PyMC: samples are drawn not from the distributions themselves, but from the set of samples in the trace. Therefore, we can take the trace from the original model, and use it to sample posterior predictions from a new model that has the same variables.\n",
    "\n",
    "The variables in the new model need only have the same name as the original -- to reinforce this, I will use `pm.Flat` variables as placeholders in this example. The only variables we actually need are the ones that need to be resampled for a new county.\n",
    "\n",
    "We don't even need `Data` here; we can use raw data, since we are just creating this model to get posterior predictions for houses in this notional new county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as new_county_house:\n",
    "\n",
    "    # New data\n",
    "    u_new = np.array([-0.2, 0.3])\n",
    "    xbar = np.array([0.5, 0.8])\n",
    "    floor_idx = np.array([1, 0])\n",
    "    \n",
    "    # Placeholders for variables already in the trace\n",
    "    sigma_a = pm.Flat('sigma_a')\n",
    "    gamma = pm.Flat('gamma', shape=3)\n",
    "    beta = pm.Flat('beta')\n",
    "    sigma_y = pm.Flat('sigma_y')\n",
    "\n",
    "    # Calculate new county expected value\n",
    "    mu_a_new = pm.Deterministic('mu_a_new', gamma[0] + gamma[1]*u_new + gamma[2]*xbar)\n",
    "\n",
    "    # Sample from the county intercept distribution\n",
    "    mu_new = pm.Normal('mu_new', mu_a_new, sigma_a)\n",
    "\n",
    "    # Expected value for houses in new county\n",
    "    y_hat_new = mu_new + beta * floor_idx\n",
    "    \n",
    "    y_new = pm.Normal('y_new', mu=y_hat_new, sigma=sigma_y)\n",
    "\n",
    "    pp_new = pm.sample_posterior_predictive(contextual_effect_trace, var_names=[\"y_new\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(pp_new, group='posterior_predictive');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits of Multilevel Models\n",
    "\n",
    "- Accounting for natural hierarchical structure of observational data.\n",
    "\n",
    "- Estimation of coefficients for (under-represented) groups.\n",
    "\n",
    "- Incorporating individual- and group-level information when estimating group-level coefficients.\n",
    "\n",
    "- Allowing for variation among individual-level coefficients across groups.\n",
    "\n",
    "As an alternative approach to hierarchical modeling for this problem, check out a [geospatial approach](https://www.pymc-labs.io/blog-posts/spatial-gaussian-process-01/) to modeling radon levels.\n",
    "\n",
    "---\n",
    "## References\n",
    "\n",
    "Gelman, A., & Hill, J. (2006). Data Analysis Using Regression and Multilevel/Hierarchical Models (1st ed.). Cambridge University Press.\n",
    "\n",
    "Betancourt, M. J., & Girolami, M. (2013). Hamiltonian Monte Carlo for Hierarchical Models.\n",
    "\n",
    "Gelman, A. (2006). Multilevel (Hierarchical) modeling: what it can and cannot do. Technometrics, 48(3), 432435."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
